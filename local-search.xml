<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>cs224n《深度学习自然语言处理》2021</title>
    <link href="/2021/11/05/cs224n/"/>
    <url>/2021/11/05/cs224n/</url>
    
    <content type="html"><![CDATA[<h1 id="第一课-简介"><a class="markdownIt-Anchor" href="#第一课-简介"></a> 第一课 简介</h1>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一些计算机基础知识读书笔记</title>
    <link href="/2021/08/13/cs-base/"/>
    <url>/2021/08/13/cs-base/</url>
    
    <content type="html"><![CDATA[<h1 id="计算机是怎样跑起来的"><a class="markdownIt-Anchor" href="#计算机是怎样跑起来的"></a> 《计算机是怎样跑起来的》</h1><blockquote><p>究其根源，是因为计算机对他们来说，并没有有意思到可以令他们废寝忘食的地步。为什么他们会觉得计算机没意思呢？通过和多名培训对象的交流，我渐渐找到了答案。因为他们不了解计算机。然而，又是什么造成了他们的“不了解”呢？</p></blockquote><h2 id="一-背景"><a class="markdownIt-Anchor" href="#一-背景"></a> 一. 背景</h2><p>计算机三大原则：计算机是执行输入、运算、输出的机器；程序是指令和数据的集合；计算机的处理方式有时与人们的思维习惯不同。</p><p>微软公司率先提出了作为新一代互联网平台的．NET技术。作为.NET核心的XMLWeb服务使用通用技术SOAP（关于调用指令的规范）、XML（定义数据格式的规范），促使企业间的计算机协同工作。</p><h1 id="图解网络硬件"><a class="markdownIt-Anchor" href="#图解网络硬件"></a> 《图解网络硬件》</h1><h1 id="计算机网络安全原理"><a class="markdownIt-Anchor" href="#计算机网络安全原理"></a> 《计算机网络安全原理》</h1><h2 id="前言与绪论"><a class="markdownIt-Anchor" href="#前言与绪论"></a> 前言与绪论</h2><p>网络安全需求可大致分为<strong>两类</strong>，一类是对网络中的消息传输进行保护，确保其机密性、完整性、真实性、不可否认性等不被攻击者破坏；另一类是保护访问网络中的信息资源或系统，保障合法用户正常访问，阻止非法用户访问。第一类需求一般通过对消息进行安全变换（消息加密、附加编码等）和安全通信协议实现（包含安全变换算法）。对于第二类需求，主要通过访问这身份验证、访问请求过滤（WAF、流量清洗等）、恶意活动检测等方法实现。</p><p>本书不涉及端系统安全以及网络安全风险评估与安全测评方面内容。</p><p>随着技术发展，传统的电信网络和有线电视网络逐渐融入计算机网络技术，即“三网合一”，每一个网络都可为用户提供话音、视频、数据业务。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>联邦图模型研究工作梳理</title>
    <link href="/2021/08/02/FGL/"/>
    <url>/2021/08/02/FGL/</url>
    
    <content type="html"><![CDATA[<p>当前联邦学习方向风头正胜，GNN模型也在各种应用场景（如风控系统、欺诈检测、推荐系统、生物制药等）中取得良好表现。而由于隐私保护和商业竞争，GNN模型也受数据孤岛问题的限制。但目前只有少数工作和图模型的联邦学习（Federated Graph Learning，FGL）相关。也许是因为图模型训练过程中更加凸显了联邦学习中的技术缺陷，感觉这个方向有更多可研究的问题和挑战。</p><p>本文列举出当前现有的相关研究工作。</p><h2 id="基础概念"><a class="markdownIt-Anchor" href="#基础概念"></a> 基础概念</h2><h3 id="federated-graph-learning-a-position-paper-arxiv-2021"><a class="markdownIt-Anchor" href="#federated-graph-learning-a-position-paper-arxiv-2021"></a> 《Federated Graph Learning - A Position Paper》—— arxiv 2021</h3><p>本文对联邦图学习方法进行定义与分类。依据图数据在客户端上的分布情况，分为图间联邦、图内联邦、图结构联邦三类。其中，图内联邦又可以区分为横向联邦和纵向联邦。</p><p>基础定义沿用GCN [Kipf and Welling, 2016] 和 FedAvg [McMahan, 2017]中的概念。</p><h4 id="一-inter-graph-federated-learning"><a class="markdownIt-Anchor" href="#一-inter-graph-federated-learning"></a> 一. Inter-graph federated learning</h4><p>适用于图级别任务，客户端的每一个样本都是一个图。比如生物药性研究中，每个分子可以表示为由原子（节点）和化学键（边）组成的图模型。应用FedAvg时候全局模型为$$\hat{y_i}^{(k)} = H(X_i<sup>{(k)},A_i</sup>{(k)}, W)$$​，目标函数如下所示：</p><div align="center">  <img src="/2021/08/02/FGL/inter.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="二-intra-graph-federated-learning"><a class="markdownIt-Anchor" href="#二-intra-graph-federated-learning"></a> 二. Intra-graph federated learning</h4><p>每个客户端都拥有（潜在）整体大图的一部分。</p><h5 id="1-横向联邦"><a class="markdownIt-Anchor" href="#1-横向联邦"></a> 1. 横向联邦</h5><p>不同客户端的子图间有同样的属性和标签空间，但节点集合间由于缺少一些连接而隔离，<strong>也可以存在一些重复</strong>。一般支持图级别或链路级别的任务，如某社交APP中各用户设备保留其局部网络结构。应用FedAvg后全局模型为$$\hat{Y}^{(k)} = H(X<sup>{(k)},A</sup>{(k)}, W)$$​，优化目标为：</p><div align="center">  <img src="/2021/08/02/FGL/hintra.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h5 id="2-纵向联邦"><a class="markdownIt-Anchor" href="#2-纵向联邦"></a> 2. 纵向联邦</h5><p>不同客户端在节点空间上是重合的，但拥有不同的属性和标签（该场景支持多任务学习）。此种联邦的目的在于组合不同客户端的节点属性并共享各方面的标签。常见于多机构间合作，如图所示，比如文章《Towards federated graph learning for collaborative ﬁnancial crimes detection》和《Fede: Embedding knowledge graphs in federated setting》。</p><div align="center">  <img src="/2021/08/02/FGL/vintra.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="三-graph-structured-federated-learning"><a class="markdownIt-Anchor" href="#三-graph-structured-federated-learning"></a> 三. Graph-structured federated learning</h4><p>除了数据呈图结构外，图结构还可能呈现在参加联邦的客户端中。此类联邦中，服务器端使用GNN模型来聚合来自不同客户端的本地模型信息。本质上算是一种特殊的联邦学习模型聚合方式。最经典的应用是基于不同地理位置监控设备的数据进行交通流量预测，使用GNN处理设备间的空间信息《Cross-node federated graph neural network for spatio-temporal data modeling》。</p><p>三种类型图联邦方法特点总结如下：</p><div align="center">  <img src="/2021/08/02/FGL/sum.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><h4 id="一些挑战"><a class="markdownIt-Anchor" href="#一些挑战"></a> 一些挑战</h4><h5 id="1-non-iid-graph-structure"><a class="markdownIt-Anchor" href="#1-non-iid-graph-structure"></a> 1. Non-IID graph structure</h5><p>任何一种联邦图模型学习方法都避免不了数据非独立同分布问题，这个问题严重影响着联邦模型的收敛速度和准确性。</p><p>而相对于传统数据，图数据中除了属性和标签分布外，还有边（即结构信息）带来的影响。</p><p>研究工作《Asfgnn: Automated separated-federated graph neural network》和《Graphﬂ: A federated learning framework for semi-supervised node classiﬁcation on graphs》尝试改善这一问题，但目前还没有可以完全解决的方法。</p><p>图结构信息特性包括如：度分布、平均路径长度、平均聚类系数等。<strong>研究一下这些特性</strong>也许有助于解决non-IID问题。</p><p>另外，也许可以采纳传统FL中一些个性化模型学习的思路，如FedNAS（Fednas: Federated deep learning via neural architecture search）等。</p><h5 id="2-横向图内联邦的孤岛问题"><a class="markdownIt-Anchor" href="#2-横向图内联邦的孤岛问题"></a> 2. 横向图内联邦的孤岛问题</h5><p>通常图表示学习方法依赖于在多跳邻居间进行游走和信息聚合，但现在这个大图被分割在不同客户间，而用户本地数据无法支持多跳信息聚合。</p><p>也许可以通过本地<strong>子图间潜在边发现</strong>来解决这个问题。研究工作《Fedgnn: Federated graph neural network for privacy-preserving recommendation》中也有提及这一点，并提出了一种基于同态加密的子图扩充方法。</p><h5 id="3-纵向图内联邦的实体匹配和安全数据共享"><a class="markdownIt-Anchor" href="#3-纵向图内联邦的实体匹配和安全数据共享"></a> 3. 纵向图内联邦的实体匹配和安全数据共享</h5><p>VFL需要做到同时保证准确性、隐私性和通信效率，但目前这方面的研究工作很少。</p><p>《Fede: Embedding knowledge graphs in federated setting》尝试基于一个由服务器维护的匹配表完成聚合，但这在一定程度上违背了隐私保护的目的。</p><p>在传统VFL领域，《Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption》使用同态加密方法联邦训练逻辑斯特回归模型，而《Multiparticipant multi-class vertical federated learning》将其推广到多参与者与多分类任务。</p><h5 id="4-图内联邦数据集问题"><a class="markdownIt-Anchor" href="#4-图内联邦数据集问题"></a> 4. 图内联邦数据集问题</h5><p>目前做实验基本是通过模拟方法随机拆分数据，而没有比较贴近现实的数据集。</p><h5 id="5-通信及内存消耗"><a class="markdownIt-Anchor" href="#5-通信及内存消耗"></a> 5. 通信及内存消耗</h5><p>联邦学习中一般局部模型会放到端侧，这对其计算能力有较高要求，而端侧与服务器间的通信也带来了额外的带宽消耗。</p><p>也许针对GNN模型的压缩技术（包括模型量化、剪枝、蒸馏等）可以缓解这一问题。</p><p>《Degree-quant: Quantization-aware training for graph neural networks》涉及GNN的模型量化方法。《Distilling knowledge from graph convolutional networks》是GNN蒸馏方法。《Lightrec: A memory and search-efﬁcient recommender system》提出模型量化方法。</p><h5 id="6-联邦学习安全性问题"><a class="markdownIt-Anchor" href="#6-联邦学习安全性问题"></a> 6. 联邦学习安全性问题</h5><p>这个部分可以再专门写一篇文章，目前人智领域以及网络安全领域顶会上都有很多相关工作。</p><h3 id="fedgraphnna-federated-learning-system-and-benchmark-for-graph-neural-networks"><a class="markdownIt-Anchor" href="#fedgraphnna-federated-learning-system-and-benchmark-for-graph-neural-networks"></a> 《FedGraphNN：A Federated Learning System and Benchmark for Graph Neural Networks》</h3><p>文章没有相关算法创新，偏重于工程（<a href="https://github.com/FedML-AI/FedGraphNN%EF%BC%89%EF%BC%8C%E6%98%AF%E5%9F%BA%E4%BA%8EFedML%E6%A1%86%E6%9E%B6%E7%9A%84%E9%92%88%E5%AF%B9GNN%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%8C%E9%92%88%E5%AF%B9Graph-level" target="_blank" rel="noopener">https://github.com/FedML-AI/FedGraphNN），是基于FedML框架的针对GNN算法的实现，针对Graph-level</a> FL，即每个客户端有自己的图模型。同时提供了一个生物分子数据集hERG。这项工作<strong>主打graph-level任务</strong>，用到的数据集基本都是生物基因、化学分子数据集。</p><p>其中的模型阐述部分（Formulation）值得写作参考，实验中列出的超参可以参考。</p><p>实验部分采用LDA（latent dirichlet allocation）方法划分不均衡的数据集。</p><p>实验结果表明：</p><ul><li>小型数据集上，FGL可以达到和集中式学习相媲美的效果；但大数据的non-IID特性更为明显，FGL效果有所下降；</li><li>直接将现有方法应用于GNN无法达到良好效果，基本相对于集中式学习都有所折扣ROC-AUC下降大概0.05~0.1。</li><li>可能由于non-IID影响，集中式表现更好的模型在联邦模式中不一定表现最好，比如文中实验里GAT效果下降明显</li><li>底端通信采用RPC（如果设备只能依靠公网IP找打的话需要使用）或者MPI效率都差不多；</li></ul><p>主要是看看里面的代码实现。有关代码的学习实践笔记见。</p><h2 id="面向non-iid问题"><a class="markdownIt-Anchor" href="#面向non-iid问题"></a> 面向non-IID问题</h2><h3 id="fedglfederated-graph-learning-framework-with-global-self-supervision"><a class="markdownIt-Anchor" href="#fedglfederated-graph-learning-framework-with-global-self-supervision"></a> 《FedGL：Federated Graph Learning Framework with Global Self-Supervision》</h3><p>定义了<strong>在图数据上</strong>进行联邦学习会面临的两大问题：<strong>heterogeneity和complementarity</strong>。如下图所示，<strong>异构性</strong>即每个客户端的图数据上节点数、边数和标签情况分布不同；<strong>互补性</strong>即每个客户端上的节点以及边的情况可能有重合/补充，比如client A这里节点5和节点7之间没有边，但client B这边是有边的。</p><div align="center">  <img src="/2021/08/02/FGL/ques.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>针对这两个问题，作者在传统FL步骤基础上加入了对于<strong>client端预测结果和节点表征结果</strong>传送和处理的步骤，在server端形成<strong>全局伪标签（non-IID问题）<strong>和</strong>全局伪图（互补性问题）</strong>。</p><div align="center">  <img src="/2021/08/02/FGL/fedgl.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="1-全局伪标签"><a class="markdownIt-Anchor" href="#1-全局伪标签"></a> 1. 全局伪标签</h4><p>处理各client端的预测结果，加权组合形成全部节点的预测结果<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>1</mn></msub><mi mathvariant="normal">，</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>P</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">P_1，...,P_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">，</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>​。在每个预测结果中选取信度较高的样本的标签作为标签。</p><p>Server端将这些伪标签数据下放，提升client端模型训练效果。</p><h4 id="2全局伪图"><a class="markdownIt-Anchor" href="#2全局伪图"></a> 2.全局伪图</h4><p>处理各client端的节点表征结果，加权组合形成全局节点表征<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>H</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">H_1,...,H_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, 即矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>H</mi><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9201900000000001em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9201900000000001em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.08333000000000002em;">˙</span></span></span></span></span></span></span></span></span>, 将此矩阵乘以其转置得到整个图的伪邻接矩阵，下放至client端提升模型效果。</p><p>client接收到server端的信息由三种：模型信息、伪标签信息和伪全图信息。伪全图信息被直接用来完善当前client上的图结构（补全边但是不增加节点），伪标签则被用来构建自监督学习（在损失函数中加入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>S</mi><mi>S</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{SSL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>项）。</p><p><strong>【问题就在于，这些也把client端的信息暴露的差不多了啊…】</strong></p><p>以上操作需要server知道每个client上到底有哪些节点且每个节点有单独索引的基础上，这样才能处理client上传的预测值和表征向量。这样联邦学习只保护了节点的属性、图结构信息以及本身的标签，但是暴露了节点信息（可能某些情况下不重要？）</p><p>实验安排、考虑到的各个方面以及描述方法值得借鉴。</p><h3 id="fedgnnfederated-graph-neural-network-for-privacy-preserving-recommendationkdd-2021"><a class="markdownIt-Anchor" href="#fedgnnfederated-graph-neural-network-for-privacy-preserving-recommendationkdd-2021"></a> 《FedGNN：Federated Graph Neural Network for Privacy-Preserving Recommendation》KDD 2021</h3><p>传统的基于GNN的推荐系统依赖于集中式的数据存储，但实际中用户数据极具私密性。但如果简单地使用分布式学习技术会有如下问题：</p><ul><li>本地数据量过小，不支持训练GNN模型；</li><li>当与全局模型同步时，本地GNN模型会泄露本地数据；</li><li>本地数据只包含user-item的一阶交互信息，没有办法处理高阶信息</li></ul><p>文章提出一种基于联邦学习方法FedGNN的<strong>推荐系统</strong>，而且重点针对训练过程中的隐私性提出了几点创新方法。</p><p>但这个系统的实现前提是：当前user节点与其交互的item节点相连，同时还与其邻居节点相连。这就<strong>要求用户知道自己的邻居是谁</strong>。过程中使用同态加密手段处理，保证中心服务器可以依据item完成user节点匹配但同时无法知道每个user到底查了什么item。</p><blockquote><p>The local subgraph on each user clients is constructed from the user-item interaction data and the nieghboring users that have interacted items with this user.</p></blockquote><div align="center">  <img src="/2021/08/02/FGL/fedgnn.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>在模型的联邦计算方面没有太多创新，但关注了联邦中参数传递过程中的隐私泄露问题。通过构造伪交互item和本地差分隐私方法分别保护embedding层和GNN模型的梯度信息。</p><p>文章思路很灵巧，对比的Baseline包括集中式的协同过滤、矩阵分解和GNN方法（该方法相比它们安全性更高），以及联邦的协同过滤和矩阵分解方法（该方法相比它们的推荐效果要好，但感觉这主要是GNN的功劳，而文章的主要贡献就是使FedGNN变得可实现，虽然我觉得这个通过上传item同态加密结果，查找邻居的方法不太实际）。</p><h3 id="subgraph-federated-learning-with-missing-neighbor-generation"><a class="markdownIt-Anchor" href="#subgraph-federated-learning-with-missing-neighbor-generation"></a> 《Subgraph Federated Learning with Missing Neighbor Generation》</h3><p>直接使用FedAvg训练一个图模型（GraphSAGE）命名为FedSage，在此基础上提出一个链路生成器形成FedSage+模型。</p><p>假设不同客户端中没有重复的节点数据，而面临一个问题即，<strong>跨子图的边不会被任何一个客户端捕捉到</strong>。</p><p>通过设计一个遗失邻居节点生成器NeighGen来完成子图补全，如下图所示，包括编码器和生成器两个部分。编码器是一个GNN模型，以待修复图为输入，输出计算后的节点表征向量。生成器包括节点数量预测器dGen和特征生成器fGen，二者都是全连接网络模型。模型均由现有数据隐藏部分数据后训练而得。</p><div align="center">  <img src="/2021/08/02/FGL/generate.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>通过组合损失函数（加权），将每个客户端本地的NeighGen以及其本地节点分类器GraphSAGE共同训练。</p><p>但这样的训练框架下，每个客户端的NeighGen依然是仅仅基于本地数据，会存在偏差，所以还要设计其联邦学习方法。然而，直接将FedAvg应用到上述损失函数上会有负面效果。因为原本不同客户端就应当生成不同的“遗失邻居”。因此我们将fGen的损失函数由公式5转为公式6的部分，代表希望生成的邻居节点与其它客户端中遗失的节点类似。（这…，我为啥有点…，而且<strong>你是怎么知道其它客户端中隐藏的节点信息的呢？？？？</strong>）</p><div align="center">  <img src="/2021/08/02/FGL/e5.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2021/08/02/FGL/e6.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="federated-graph-classification-over-non-iid-graphs"><a class="markdownIt-Anchor" href="#federated-graph-classification-over-non-iid-graphs"></a> 《Federated Graph Classification over Non-IID Graphs》</h3><p>从题目可以看出，这篇文章<strong>也是主打图级别</strong>任务。（作者竟然说对于节点分类/链路预测任务的研究工作已经有很多了…）</p><p>首先观察不同领域的图数据间是否存在一些共享特性（比如节点度分布、最短路径长度、最大组成大小、聚类系数等）。经过和相同节点和边数量随机生成的模型相比发现确实如此。但与此同时不同客户端间的图数据也在结构和特征信息上存在non-IID特性。</p><p>这篇文章分析了同一数据集、同一领域数据、跨领域数据三种不同层次中存在的异构性造成的影响。</p><p>考虑使用基于聚类的联邦学习框架，根据梯度信息，将较为相似的客户端聚为一类，而为不同的簇维护不同的全局模型。而且这个聚类的方式是动态的。</p><p>另外作者发现在每轮迭代中，梯度范数波动很大，而且不同客户端的梯度范数的范围很不同。如果基于这样的信息聚类，结果会不准确。为了解决这种问题提出进阶版本模型GCFL+，此方法<strong>不仅基于当前梯度信息聚类</strong>，而是维护一个$$d$$长度的梯度窗口。</p><h3 id="vertically-federated-graph-neural-network-for-privacy-preserving-node-classification"><a class="markdownIt-Anchor" href="#vertically-federated-graph-neural-network-for-privacy-preserving-node-classification"></a> 《Vertically federated graph neural network for privacy-preserving node classification》</h3><p>蚂蚁金服团队与浙大、北大合作的作品。</p><h3 id="asfgnn-automated-separated-federated-graph-neural-network"><a class="markdownIt-Anchor" href="#asfgnn-automated-separated-federated-graph-neural-network"></a> 《Asfgnn: Automated separated-federated graph neural network》</h3><p>支持自动化超参优化。</p><h3 id="fl-agcns-federated-learning-framework-for-automatic-graph-convolutional-network-search"><a class="markdownIt-Anchor" href="#fl-agcns-federated-learning-framework-for-automatic-graph-convolutional-network-search"></a> 《FL-AGCNS: Federated learning framework for automatic graph convolutional network search》</h3><h3 id="spreadgnnserverless-multi-task-federated-learning-for-graph-neural-networks"><a class="markdownIt-Anchor" href="#spreadgnnserverless-multi-task-federated-learning-for-graph-neural-networks"></a> 《SpreadGNN：Serverless Multi-task Federated Learning for Graph Neural Networks》</h3><p>从题目可以看出，这篇文章主打<strong>无服务器</strong>联邦，而且每个用户可能拥有其样本数据中的部分标签。该方法可以处理不同客户端间数据集大小和标签分布的non-IID问题。而且即便在用户只能和部分邻居通信时也可以保持比较好的检测效果。</p><p>文章重点强调了联邦多任务学习（Federated Multitask Learning, FMTL），在损失函数中包括了任务之间的关系。</p><p>而处理“没有一个中央服务器”的挑战：</p><h3 id="graphfl-a-federated-learning-framework-for-semi-supervised-node-classification-on-graphs"><a class="markdownIt-Anchor" href="#graphfl-a-federated-learning-framework-for-semi-supervised-node-classification-on-graphs"></a> 《Graphﬂ: A federated learning framework for semi-supervised node classiﬁcation on graphs》</h3><p>现有联邦学习方法存在以下问题：1）在Non-IID数据下表现不好；2）无法处理带有新标签的数据；3）无法使用未标记数据。而这些问题在图数据上会表现得非常明显。</p><p>本文使用元学习方法（使用MAML）解决前两个问题，并利用自训练（self-training）技术利用未标记的图数据。</p><p>作者不再致力于得到一个在所有数据集上都表现特别好的模型，而是将不同客户端的局部模型训练视为元学习中的一个任务，使用MAML学习到task-independent初始化参数。</p><h3 id="federated-dynamic-gnn-with-secure-aggregation"><a class="markdownIt-Anchor" href="#federated-dynamic-gnn-with-secure-aggregation"></a> 《Federated dynamic gnn with secure aggregation》</h3><p>在多用户图序列数据中，学习目标的动态表征。</p><h3 id="distributed-graph-convolutional-networks"><a class="markdownIt-Anchor" href="#distributed-graph-convolutional-networks"></a> 《Distributed graph convolutional networks》</h3><p>分布式GNN训练方法，保留了子图间的边连接。</p><h3 id="fede-embedding-knowledge-graphs-in-federated-setting"><a class="markdownIt-Anchor" href="#fede-embedding-knowledge-graphs-in-federated-setting"></a> 《Fede: Embedding knowledge graphs in federated setting》</h3><h3 id="towards-federated-graph-learning-for-collaborative-financial-crimes-detection"><a class="markdownIt-Anchor" href="#towards-federated-graph-learning-for-collaborative-financial-crimes-detection"></a> 《Towards federated graph learning for collaborative ﬁnancial crimes detection》</h3><h3 id="cross-node-federated-graph-neural-network-for-spatio-temporal-data-modeling"><a class="markdownIt-Anchor" href="#cross-node-federated-graph-neural-network-for-spatio-temporal-data-modeling"></a> 《Cross-node federated graph neural network for spatio-temporal data modeling》</h3><h3 id="sgnn-a-graph-neural-network-based-federated-learning-approach-by-hiding-structure"><a class="markdownIt-Anchor" href="#sgnn-a-graph-neural-network-based-federated-learning-approach-by-hiding-structure"></a> 《Sgnn: A graph neural network based federated learning approach by hiding structure》</h3><h3 id="locally-private-graph-neural-networks"><a class="markdownIt-Anchor" href="#locally-private-graph-neural-networks"></a> 《Locally private graph neural networks》</h3><h3 id="peer-to-peer-federated-learning-on-graphs"><a class="markdownIt-Anchor" href="#peer-to-peer-federated-learning-on-graphs"></a> 《Peer-to-peer federated learning on graphs》</h3><h3 id="cluster-driven-graph-federated-learning-over-multiple-domains"><a class="markdownIt-Anchor" href="#cluster-driven-graph-federated-learning-over-multiple-domains"></a> 《Cluster-driven graph federated learning over multiple domains》</h3>]]></content>
    
    
    <categories>
      
      <category>知识梳理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>联邦学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>杜克大学《怪诞行为学》</title>
    <link href="/2021/07/14/weird/"/>
    <url>/2021/07/14/weird/</url>
    
    <content type="html"><![CDATA[<p>几年前看过那套丛书，感觉很有意思。而相比书籍，教授的课程内容丰富了许多。</p><p>教授为了“政治正确”而选择了不同于自己来源地区意识形态的地区的着装，太可爱了。</p><p>这个课程主要涉及：1）各种非理性行为（irrationality）；2）关于钱的心理学；3）不诚实的行为；4）动机与劳动；5）自控力；6）情绪，六大部分。</p><p>拖延是人类的正常行为（<strong>不用过分谴责自己</strong>），只是在制定计划和做决策时考虑到这个因素就好了。具体的做法是，将自己原本认为可以完成的时间乘以2。另外，DDL真的是第一生产力，要明确目标后，人为自行设定<strong>合理的</strong>DDL。</p><p><strong>合理的DDL机制</strong>和<strong>测试方法</strong>将极大程度上帮助学习和工作。</p><h2 id="非理性行为-irrationality"><a class="markdownIt-Anchor" href="#非理性行为-irrationality"></a> 非理性行为 Irrationality</h2><p>本节将介绍各种影响行为的因素，了解选择架构（周围的世界和架构如何影响了我们的观点与决策），了解初始决策如何影响了长期行为，进一步会思考如何改进处事方法。</p><p>教授针基于自己烧伤期间在医院里换绷带的经历，对人们对痛苦的感知进行实验，结论如下：</p><ul><li>疼痛的时长变为两倍，并不会让疼痛感增加两倍；短促而剧烈的疼痛不比长期而温和的疼痛更好；</li><li>最好是从最疼的地方开始，慢慢减轻；</li><li>最好在疼痛中间有间歇，可以让人休息一下。</li></ul><p>而由于这一件事，教授开始研究我们的直觉很多时候是怎样误导我们的。比如视错觉、大猩猩实验，即便有理性加持，也是不可避免的，因为我们其实是通过大脑在看，有一定的目标与期望，而可能与现实不符。而且视觉还是我们发展的很好的功能机制。</p><p>在遗体捐赠项目中，将勾选内容设定为“选择加入”和“选择退出&quot;会有非常不同的统计结果，大多数人都不会勾选。这就是选择架构的影响。我们身处的环境、默认选项、复杂性等都会影响我们的选择。选择架构其实包含着一种暗示。比如，被要求列举出伴侣的三个优点的人会比被要求列举出十个的人，感受到更多对对方的爱意。再比如，苹果应用商店中的标价实际上只有几种，这大大降低了决策难度。</p><p>人们时候解释自己行为的时候往往是合理化之后的结果，而也许并不是真实的原因。【所以需要更多理解他人的行为，而非关注他们的解释和说辞。】</p><p>很多时候人们并不是厌恶某个选项，只是讨厌变动而已。所以有一些设计会包含**“强迫选择”**，首先让人们进入一个变动的情境。</p><p>平常要警惕“默认行为”。</p><p>很多策略可以对我们当下立即要做出的决策产生影响。虽然这个影响会随着时间推移而淡去，但长此以往也会影响后续的很多决定。</p><p>商家经常用到一个策略，那就是加入一个“低端选项”，而促使人们去购买<strong>中间项</strong>，因为它突然变得更吸引人。这个策略也可以应用在，如何选择合适的“僚机”。。。。</p><h3 id="首个决策的重要性"><a class="markdownIt-Anchor" href="#首个决策的重要性"></a> 首个决策的重要性</h3><p>“羊群效应”不仅仅出现在我们和他人之间，也存在于我们自身的各种决策之间。我们对糟糕情绪的记忆并没有我们对行为习惯的记忆那么深刻，比如我形成了点外卖的习惯，即使它们现在贵的要死。</p><p>“锚点效应”也彰显着我们最初的决策发挥着的巨大影响。</p><p>星巴克应用了很多策略让自己和比较廉价的咖啡店处于不同的比较层次。而当消费者形成习惯之后，就不用再可以维持这样的差异，比如星巴克最开始的时候为了保持“逼格”，不会卖小甜点。对比现在的华为和小米手机的品牌定位也可以看出这个策略。</p><p>也许你在完成自己的事业的时候，也要学会如何利用这些效应，包装自己。当然，只是作为辅助手段。</p><h3 id="我们学到什么"><a class="markdownIt-Anchor" href="#我们学到什么"></a> 我们学到什么</h3><p>首先，我们总是会存在很多的决策偏差。我们的直觉经常是不准确的，而我们并没有意识到。</p><p>所以，我们需要一些实际的经验数据来评估事情的真相，而非依赖<strong>无意识的</strong>直觉或习惯。</p><p>另外，要注意，即便是权威专家也会犯上述类似的错误。</p><p>记住，我们并不是万能的，我们是非理性的，这是人类的局限性，保持谦逊。（也是生活的魅力所在）</p><h3 id="p1无意识消费者"><a class="markdownIt-Anchor" href="#p1无意识消费者"></a> P1：无意识消费者</h3><p>心理抗拒现象，比如非要走好几个街区买到2升装的可乐，但拒绝店员提出用相同的价格买下2瓶1升装的可乐。</p><p>很多时候我们对我们爱的人/重要的人提出的很明智的建议，也会是这种抗拒的态度。</p><p>所以，我们最好在不出发这种抗拒行为的前提下，<strong>间接性地</strong>抛出建议。对自己、对他人都需要技巧，避免这些无意识的不利行为。</p><p>周遭的环境确实会<strong>深深地</strong>影响一个人，潜意识地暴露。</p><p>如果你想在游泳的时候更快，也许买菲尔普斯的战衣，<strong>真的会有（辅助）效果</strong>，一种无意识的刺激。</p><p>我们要做了解这些现象，并有战略性地使用它们。</p><h3 id="p2自我认识错觉"><a class="markdownIt-Anchor" href="#p2自我认识错觉"></a> P2：自我认识错觉</h3><p>在相亲网站上填写资料时，男性更倾向于展示收入，而女性更倾向于展示外貌。而双方对于异性特质的关注程度也是这样的。</p><p>而在经过面对面的闪电约会之后，对外貌、收入等方面的关注程度就<strong>不再与性别有相关性</strong>了。</p><p>人们择偶的时候真的知道自己到底重视什么吗？人们的实际行为和“声称”的部分有很大差异。也许就是根据进化而来的本性，好外貌、好性格、高收入是通用的加分项。（但是这个“好”的标准，具体下来会有所差异）</p><h2 id="金钱心理学"><a class="markdownIt-Anchor" href="#金钱心理学"></a> 金钱心理学</h2><p>金钱总是涉及到<strong>机会成本</strong>。每花一笔钱的时候，想想自己放弃了什么。而当前的金融环境，可能让我们很难这么做。</p><p>货币创造了一个使所有物品都能进行交易的中央机制。不再需要以物换物，使得市场交换高效而美妙。</p><p>【例子】人们愿意花1000美元买先锋音箱而非花700美元买索尼音箱，而当索尼音箱买1000美元但附赠价值300美元的DVD时，选择会变化。因为这个时候300美元变得<strong>具体化</strong>了，吸引力增大了，而钱只代表了广义的机会成本，比较模糊。</p><p>面对大额开销时，我们会更加“大方”，比如花2000元给价值几十万的车换真皮座椅。</p><h3 id="支付的痛苦"><a class="markdownIt-Anchor" href="#支付的痛苦"></a> 支付的痛苦</h3><p>现金支付会让我们对消费行为更加谨慎。</p><p>旅行前支付完全部的费用，会比旅行结束的一天支付让我们更加享受旅行。虽然后一种方式是更理性的消费模式。</p><p>多次小额支付往往会比一次性支付更经济，但会带来更多痛苦。</p><p>【例子】教授给他的学生们按“每吃一口5元”的方式给午餐计费，这虽然在总价上优惠了很多，但吃饭的过程很痛苦。</p><p>这可能是年卡或终身VIP的魅力，虽然并不划算。</p><p>心理账户</p><div align="center">  <img src="/2021/07/14/weird/1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>免费版本有巨大的吸引力，即便付费版只需要0.99元。</p><h3 id="公平"><a class="markdownIt-Anchor" href="#公平"></a> 公平</h3><p>我们不愿意为专业付钱，而是付辛苦费，不在意结果，而是工作人员到底在上面花费了多少时间和精力。因为人们喜欢**“公平”**。所以我们付款的意愿也取决于环境，而非我们自身认定的当前产品的价值。</p><h3 id="厌恶损失"><a class="markdownIt-Anchor" href="#厌恶损失"></a> 厌恶损失</h3><p>获得让我们快乐，但损失让我们<strong>尤其</strong>痛苦。这个叫做“禀赋效应”。</p><p>人们得到一件东西，然后要出手的时候，会赋予它更高的价值。</p><p>比如闲鱼卖家、比如一些无理由退换、试用期策略。</p><h3 id="定位"><a class="markdownIt-Anchor" href="#定位"></a> 定位</h3><p>有的时候支付一些报酬反而会取得不好的效果，但如果支付的报酬足够多的话效果会回升。因为存在金钱规则和社交规则的差异。</p><p>比如<strong>罚款会让人抵消罪恶感</strong>，即便之后把罚款取消了，也不会回复原来的效果。所以在交往中<strong>最好明确自己的定位</strong>，是属于市场规范还是社交规范。</p><p>【例子】两家银行，一家与客户有社交关系，一家完全按照规章制度办事，那么当客户因迟交还款而被罚钱，而且银行拒绝接受他的理由和宽限请求时，他会对第一家银行比较愤怒。</p><p><strong>尽量不要将社交因素商业化</strong>。</p><h3 id="p1金钱-时间与幸福感"><a class="markdownIt-Anchor" href="#p1金钱-时间与幸福感"></a> P1：金钱、时间与幸福感</h3><p>在超过一定阈值之后，金钱能带来的幸福感就会有所降低。</p><p>自愿地把一部分钱花在别人身上，会给我们自己带来幸福，感觉更富足。即便你自己并不是很富裕。</p><p>对于时间的花销也是如此。</p><h3 id="p2性经济学"><a class="markdownIt-Anchor" href="#p2性经济学"></a> P2：性经济学</h3><p>有很多政策都暗示着，性是女性的所有物，而男性愿意用一些其它的资源来换取。</p><p>在男女双方的博弈中，存在最小兴趣原则。对性这件事上，女性拥有更多的权利。</p><p>男女双方立场不同，所“抱怨”的角度也不同。</p><p>可以通过提示资源转移让女性更接受性图像。而通过激发不同的情感可以改变女性对资源转移的想法。</p><h2 id="不诚实-dishonesty"><a class="markdownIt-Anchor" href="#不诚实-dishonesty"></a> 不诚实 Dishonesty</h2><p>很多时候，大多数时候，我们说谎，而且经常说谎，但并不觉得自己不道德。</p><p>但“回答答对题目数获取以奖励”的实验中，人们大多会就答对题目数目<strong>轻微撒谎</strong>，而且撒谎的<strong>程度竟然和它所带来的收益和风险无关</strong>。</p><h3 id="敷衍因子"><a class="markdownIt-Anchor" href="#敷衍因子"></a> 敷衍因子</h3><p>作为个人，我们如何在希望营造（给别人也给自己）道德完美形象和获取更多利益间进行取舍呢？教授提出<strong>敷衍因子（fuge factor）</strong>，在这个微小范围内撒谎，可以让我们平衡上述两个方面，达到两全。</p><p>如何缩小敷衍因子：</p><ul><li>如果实验前先抄写了十诫，结果是，没有人谎报。所以只要<strong>想到一些道德戒律</strong>都会缩小我们的敷衍因子。但日常的一些道德教育貌似并没有起到很大的改善作用。</li><li>如果在行为之前签名会起到效果，但在这之后签名就不会了。<strong>签名位置的重要性。</strong></li></ul><p>如何增大敷衍因子：</p><ul><li>如果事物不和金钱直接挂钩，那么我们就更能为自己合理化这件事，做出更多不诚实的行为；实验中将金钱换为游戏币（<strong>增加人们与金钱之间的距离</strong>）会催生更大程度的谎言；</li><li>提供他人作弊的例子，会催生更多欺骗。因为这可能让人们发掘欺骗的成本并不高；另外也可能是为了<strong>寻求社会认同</strong>。而如果只是口头说说作弊的方法并不会催生更多欺骗，另外，如果带头作弊的人来自一个很不同的群体，也不会催生更多欺骗，甚至说谎率会降低。</li></ul><p>哪类人更容易说谎呢？可能是那些<strong>创造性强</strong>的人，因为他们合理化能力也比较强。</p><p>除了关注那些撒下弥天大谎的少数“害群之马”，我们应该更需要关注于撒小谎并进行合理化的大多数人。</p><h3 id="利益冲突"><a class="markdownIt-Anchor" href="#利益冲突"></a> 利益冲突</h3><p>很多时候阳光政策和公开化并不能解决问题。</p><p>当我们全心贯注与对自己有好处的事物上，我们向别人索取它时，会觉得很自然，并没有考虑到这里边的利益冲突。</p><h3 id="持续欺骗"><a class="markdownIt-Anchor" href="#持续欺骗"></a> 持续欺骗</h3><p>人们会习惯于自己撒的小谎，而在积累了一定时间和数量之后，撒大谎，<strong>对欺骗习以为常</strong>。</p><p><strong>中途的告解/忏悔可以阻挡这一过程</strong>，可能是因为人们不想破坏掉刚刚忏悔后的那种内心的宁静。也可能是因为这个动作/行为，为你的生活开启了新篇章，让我们可以重新开始。</p><p>欺骗是否存在文化差异？教授的实验证明不会，因为他的实验只是在验证敷衍因子。但如果涉及到特定领域，就会多少有些差异了。</p><p>教授的一些实验发现，银行家的欺诈程度是政客的2倍，但他也说，这些接受实验的政客年龄比较小，不诚实的“能力”还有很大提升空间哈哈哈。</p><h3 id="p1医疗决策"><a class="markdownIt-Anchor" href="#p1医疗决策"></a> P1：医疗决策</h3><p>癌症晚期病人是选择化疗（冒着缩短寿命和承受痛苦的风险）还是选择舒缓治疗？</p><p>很多时候人们面对癌症会做出非理性决策。更多是感性的，比如，只要有一线希望、万一呢。。。。</p><p>医护人员也可能无意识地提供不实信息引导决策。</p><h3 id="p2道德钟摆效应"><a class="markdownIt-Anchor" href="#p2道德钟摆效应"></a> P2：道德钟摆效应</h3><p>人们为了重新积极地看待自己的道德，而在做出自认为自私的行为后，产生补偿行为。这个补偿行为可以是针对完全不相关的其它事情的。</p><p>但钟摆效应也是双向的，在我们做完一件自我道德感觉良好的事情之后，而产生相对自私的行为。比如在环保商店中买过东西的被试，会倾向于在后续的“独裁者”实验中表现得更自私。</p><p>所以，注意不要自我感觉良好哦~</p><h2 id="劳作与动机"><a class="markdownIt-Anchor" href="#劳作与动机"></a> 劳作与动机</h2><p>工作的积极性在多大程度上纯粹是由于报酬驱动的呢？</p><p>意义情境、西西弗斯情境</p><h3 id="意义"><a class="markdownIt-Anchor" href="#意义"></a> 意义</h3><p>比如监狱里用“把一堆土从一端运送到另一端，再返回来”来惩罚犯人。</p><p>如何取消一个投入很大的项目，而不让人们感到消沉？<strong>加入一些其它的目的和意义</strong>，比如：</p><ul><li>在全公司人面前展示这个项目；</li><li>为这个项目构建出模型；</li><li>探索将其中技术应用到其它项目的方法；</li></ul><h3 id="认同"><a class="markdownIt-Anchor" href="#认同"></a> 认同</h3><p>做一个实验，对实验人员的成果的反应分为三种情境：承认情境、忽视情境和碎纸机情境。</p><p>一个小的动作（比如说一句“aha”）就能让人们对自己的工作产生更良好的感觉，有更强的动力。而相反的，忽略（仅仅是忽略的态度）其工作效果，也会毁了他们的动力。【这一点在研究生阶段深有体会】</p><h3 id="宜家效应"><a class="markdownIt-Anchor" href="#宜家效应"></a> 宜家效应</h3><p>Labor lead to love。</p><p>【例子】最开始生产一种非常简单的蛋糕粉，只需要导入模具中就可以烘焙了，但销量并不好，不是因为味道，而是因为相对于传统方法，它实在太简单了。于是商家去除了其中的鸡蛋、奶油等成分，使制作过程重新变得复杂，销量提升了。。。</p><p>所以我们对于一件事物的喜爱，并不只在于它本身是什么样子，<strong>也取决于我们到底对它付出了多少努力</strong>。</p><p>在抽象的想法和观念上也是有这个效应的。有一种**“创造的骄傲”**在里面。</p><p>爱迪生和特斯拉之争。</p><p>对自己理论/点子的热爱，也是我们动力的源泉。但同样的，注意不要故步自封，被这种喜爱蒙蔽了双眼。</p><h3 id="激励的效果"><a class="markdownIt-Anchor" href="#激励的效果"></a> 激励的效果</h3><p>对工作的影响效果并不是随着激励的增加而一直增加的。</p><div align="center">  <img src="/2021/07/14/weird/2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>对简单的<strong>机械性任务</strong>的提升效果是随着激励的提升而上升的，而当涉及到<strong>脑力任务</strong>（包括注意力、思考、记忆等）时就无法达到这样的效果。在这些任务中，我们更需要一个“心无旁骛”的心流状态。</p><p>Bonuses can be distracting, and many actually decrease performance.</p><p>比赛的最后五分钟，<strong>关键球员</strong>的命中率并没有提升，但他们得到了更多的传球，并且有了更多投球的尝试。（但会不会其它球员的命中率是下降的啊）</p><p>亚当斯密的流水线作业 vs 卡尔马克思的工作意义</p><h3 id="p1的激励形式"><a class="markdownIt-Anchor" href="#p1的激励形式"></a> P1：的激励形式</h3><p>天生的群体倾向性，利用这种跟随的倾向促使人们“做出选择”。</p><p>他人在场可能会导致我们表现更差。</p><p>创新是最难以被金钱激励的行为。</p><blockquote><p>工作不是对人的惩罚，而是奖励，是力量和快乐之源。</p></blockquote><h2 id="自我控制"><a class="markdownIt-Anchor" href="#自我控制"></a> 自我控制</h2><p>对现在与未来的权衡取舍。但我们只能活在当下，所以总是做出一些不理性的决策。</p><p>给自己设计一些小花招。</p><h3 id="替代奖励"><a class="markdownIt-Anchor" href="#替代奖励"></a> 替代奖励</h3><p>比如让自己后悔，比如把注射药物的痛苦和欣赏电影的欢乐联系在一起。</p><h3 id="自我契约"><a class="markdownIt-Anchor" href="#自我契约"></a> 自我契约</h3><p>尤利西斯让水手们把他绑在桅杆上。</p><p>因为你<strong>提前就知道</strong>你会受到诱惑，并且掌握了正确的应对方式。比如我们不选择直接在宿舍里复习功课，而是去图书馆或其他的公共场合。</p><p>即便是老鼠或鸽子也知道这个道理。</p><h3 id="分散注意力"><a class="markdownIt-Anchor" href="#分散注意力"></a> 分散注意力</h3><p>自我损耗（ego depletion）。</p><p><strong>更多的自由就会带来更多的诱惑</strong>，意味着在无意识状态下更多的失败，关键问题是如何平衡。而随着科技的发展，我们忽略了自身认知决策能力上的局限。</p><h3 id="p1-利用非理性决策促进自我控制"><a class="markdownIt-Anchor" href="#p1-利用非理性决策促进自我控制"></a> P1: 利用非理性决策促进自我控制</h3><p>自控失误，比如健身卡、按时吃药等等。</p><p>小概率偏好，比如买彩票。可以改成为自己的减重加注。</p><p>损失厌恶偏好</p><h3 id="p2-训练自控力"><a class="markdownIt-Anchor" href="#p2-训练自控力"></a> P2: 训练自控力</h3><p>棉花糖实验</p><p>首先，自控力可以锻炼，就像健身练肌肉一样，只要你找到了正确的方法。比如刚刚提到的替代奖励、自我契约等等。</p><p>另外，自控力也是有限的，所以最好是有计划地使用。</p><p>但最后一点发现是，坚信使用自制力不会导致“耗尽状态”的人，更不容易出现耗尽状态。</p><p>具体的锻炼方法：使用非惯用手、定期健身、戒甜食，甚至是握手柄等。</p><p><strong>当然，改变的动力是重中之重</strong>。</p><h3 id="p3理财"><a class="markdownIt-Anchor" href="#p3理财"></a> P3：理财</h3><p>随着科技、商业等领域的发展，当今一代的人面临着更多的诱惑。</p><p>教导孩子理财和自控是非常重要的。</p><p>但自控力确实是<strong>说起来容易做起来难</strong>。</p><p>我们当然不用循规蹈矩，而拥有了更多的自由，但需要注意的是，<strong>你将会成为你正在成为的那个人</strong>。</p><h2 id="情绪"><a class="markdownIt-Anchor" href="#情绪"></a> 情绪</h2><p>一般情况下，情绪是自然而来的，持续时间并不长，但它们一旦出现我们就会被它们掌控，几乎是全盘接管。</p><p>情绪的这种特性有两面性。所以，要照顾好自己呦~</p><p>我们很容易因为一个具体的人而引发情绪，而忽略一群人，因为一群人仅仅是一个数字，一个概念。</p><p>远方的哭声，具体化的图像与案例。</p><h3 id="p1恶乌及乌"><a class="markdownIt-Anchor" href="#p1恶乌及乌"></a> P1：恶乌及乌</h3><p>比如纳粹对犹太人生活的宣传影片。</p><p>保守主义的人更倾向于有厌恶情绪。</p><h3 id="p2幽默"><a class="markdownIt-Anchor" href="#p2幽默"></a> P2：幽默</h3><p>幽默感产生于冲突性当中，这个场景中是需要存在消极因素的。</p><p>与灾难的距离感，或者轻度的灾难会带来一些幽默。</p><p>需要一些<strong>良性的</strong>冲突。</p><p>笑点和一个人的价值观、个人经历及信仰有关，有的人认为是幽默，有的人就觉得是冒犯。</p><div align="center">  <img src="/2021/07/14/weird/3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>龙虾教授《Maps of Meaning》</title>
    <link href="/2021/07/14/maps-of-meaning/"/>
    <url>/2021/07/14/maps-of-meaning/</url>
    
    <content type="html"><![CDATA[<p>2017年的课程，也有配套的书《Maps of Meaning》。</p><p>这个课程和《人格及其转变》有比较多重复的内容，可以重新理解吸收。</p><h2 id="语境和背景"><a class="markdownIt-Anchor" href="#语境和背景"></a> 语境和背景</h2><p>财富的帕累托分布、智商问题、马克思主义、极权主义、1989年苏联解体、乔治威尔的书</p><p>人类为什么发展处两大阵营？在冷战时期的军备竞赛</p><p>人们不仅仅是为了资源而战争，首先要明确人们为什么觉得某些资源是有价值的。</p><p>当看到有人做出极端行为时，想象自己身处那样的场景到底会怎么做，想象自己是犯罪者，是集中营中的卡波，是731部队的一员…其它人做过的事情我可能也会做，而我不能对这些东西/可能性一无所知。</p><p>不同的信仰体系，以及不同信仰体系下行为的匹配。理论加上实践结果构成了我们的生活。我们不能脱离<strong>认知结构</strong>去生活，它指导了我们的价值体系和行为。<strong>混乱</strong>是世界不再与你的认知匹配的时候的状态，你的“故事”分崩离析。如果我们一直停留在刚刚面对混乱时的“僵住”的应急状态，会很糟糕，<strong>那种紧急状态非常耗能</strong>。有时候非常剧烈的积极事件（比如中了乐透）也会导致不稳定与混乱。经历混乱之后，人们会重塑他们的认知/人格或者选择重塑世界。人们共享的道德体系带来已知和安全感。</p><p><strong>我们如何定义真实</strong>？达尔文主义也许和科学唯物主义是相对的，因此教授认为实用主义的真实比科学中的真实有更深切的意义。后现代主义认为人类有一个内置的中心叙事，作为人类价值体系的戏剧化表达。教授认为<strong>伟大的戏剧比现实更加真实</strong>，因为其中包含了对人类现实的提取与抽象，提炼加工，可以指导我们的行动。比如《达芬奇密码》大卖、电影《匹诺曹》、《阿凡达》、《星际迷航》、《哈利波特》等、各种音乐、流传千年的神话寓言等。</p><p><strong>终极的真实是痛苦</strong>，最本质的现实是混乱，这在很多宗教体系中也有所展现。生活是苦难的，是有限的。<strong>生活的意义在于承担适当的责任</strong>，比如家人、亲朋、社会事业等。当人们脱离外部系统的约束时，会是一种漂浮的状态，由于过分原子化而抑郁。</p><blockquote><p>He who has a why to live can bear almost any how.</p></blockquote><div align="center">  <img src="/2021/07/14/maps-of-meaning/1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>mother nature，father culture.</strong> 自然同时体现着毁灭与创造，原始的美丽与危险，文化也同时意味着专制与安全，它们都具有两面性。作为个体也是有两面性，英雄与恶棍，这些都是古老故事中的重要角色。</p><p>为什么你会变得邪恶？首先，因为你可以，这是移情的一个分支，我们有了自我意识之后，马上就产生了好的和邪恶的区分。我们首先需要意识到<strong>我们可以变得邪恶</strong>，我们都有阴暗面，我们的一部分会阻碍我们的存在，让我们变得更糟。<strong>黑暗的部分会是你人格力量的一大组成部分</strong>，不要放弃它（想到了张灵玉的水脏雷），否则你会逐渐输给更具有攻击性的人。</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在已知/秩序与未知/混乱之间的穿梭构成了我们的生活，这也是神话人物的形象。</p><p>生活是有终极意义的，但为了体验那份意义你必须对自己的工作<strong>承担最终的责任</strong>。</p><h2 id="匹诺曹电影隐喻"><a class="markdownIt-Anchor" href="#匹诺曹电影隐喻"></a> 《匹诺曹》电影隐喻</h2><h4 id="电影背景音乐"><a class="markdownIt-Anchor" href="#电影背景音乐"></a> 电影背景音乐</h4><div align="center">  <img src="/2021/07/14/maps-of-meaning/3.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>人们对着星星许愿，为什么？布满星星的夜空令人<strong>敬畏</strong>，这是那些超越人类自身平凡的东西所带来的感受。</p><p>明星存在的意义是什么？他们具有我们期望的品质，是一种投射。你期望自己变为什么样子？</p><p><strong>未来写作计划</strong>，可以先写自传，你是谁，从哪里来（<strong>过去写作计划</strong>），之后关注你要到哪里去，<strong>你到底想要什么</strong>。如果大脑对超过18个月的事情依然有情绪反应，尤其是负面的情绪反应，说明你有一部分灵魂卡在了那里，那里还有问题需要面对和解决。可以每天写15分钟，关于你未来3-5年希望自己是什么样子，不想自己是什么样子。如果目标足够具体，其大体的流程与“耶鲁幸福课”中的WOOP方法一致。</p><p>你需要认识到，问题是一个接着一个的，永远不会有“等所有问题都解决了，我就快乐了”的状态，而这个过程是很有意思的。</p><p>“如果你的心在你的梦想中”，意味着你的理性与感性融为一体，共同去争取那个你真正想要的目标。<strong>Put yourself together.</strong> 做一个融入自我的人！一个整合的人蕴含着惊人的力量。</p><p>如果你<strong>足够诚实</strong>，那么现实就在你这一边，会支持你。没有人可以始终逃避真相，你必须摆正现实。</p><p><strong>相信/信念意味着爱</strong>，你永远都不会确切地知道/弄清楚任何事，但你选择去做，是因为一种信念和热爱。</p><h4 id="等级层次"><a class="markdownIt-Anchor" href="#等级层次"></a> 等级层次</h4><p>短期记忆、程序记忆、长期记忆</p><p>生物界的层次分化系统、社会等级安排，比如鸟鸣、龙虾，人类也构建了很多优势等级。动物之间的模拟战斗。</p><p>并不是我们的内在信念在调节我们的情绪，而是对于内在信念的执行和反馈结果（与外界交互）起到这个作用。<strong>我们的“世界地图”与他人行为的匹配程度</strong>影响着我们的情绪。</p><p>下丘脑的巨大作用，其实我们并不能完全控制自己，比如遇到意外时的瞬时反应。</p><p>地域性和领土捍卫的本能深深地扎根在我们身上。而当今文明/文化基于了我们最根本的保障，大部分时间我们都可以处于一种安静的、满足的状态。</p><p>情绪和动机必须纳入人与人之间的关系。</p><p>让两岁的发怒的孩子独自冷静下来，控制住自己，而非被他“统治”。适当的奖励，培养出其强大的自我。一定要注意<strong>准确地、真诚地赞美别人</strong>，尤其是当他们做了你觉得非常棒的事情的时候。在孩子两到四岁的时候培养他的社交，尤其是与同龄人的社交能力。比如“过家家”游戏有很重要的意义。母乳喂养也是一种社交互动。</p><p>不要剥夺孩子参与游戏的机会。</p><p>学会<strong>合作与竞争</strong>。天生的道德系统。训练自己擅长<strong>玩“元游戏”</strong>，所以别人是愿意跟你玩游戏的，这样才是赢家。生活也是由各种各样的游戏组成。我们需要有一个目标，任何目标都比没有目标好，因为没有目标就没有积极情绪。</p><p>与人相处的策略：信任他并且承担责任，也就是首先信任，但如果他违反了规则，<strong>你必须惩罚他们，但在这之后不要怀恨在心</strong>，互动的大门依然敞开。</p><h4 id="好的目标"><a class="markdownIt-Anchor" href="#好的目标"></a> 好的目标</h4><p>**你面对它，面对它，而变得强大！**逃避是无济于事的，永远都不会是好的策略。</p><p>事物同样是它自己的对立面，你也必须做到<strong>自相矛盾</strong>，才能面对恶龙，最终拿到金币。<strong>你需要善良，但与此同时也是一个怪兽</strong>，而不只是一个无辜的小兔子。<strong>整合好你的攻击性</strong>，容忍冲突，捍卫自己，而非永远落败。</p><p>确定适当的目标，内部混乱/冲突会消失，自然会站在你这边。</p><p>阐明你为什么想要达成这样的目标，大概举出5个原因，为什么这对你好，对你的家人好，对社会好？</p><h4 id="木偶的隐喻"><a class="markdownIt-Anchor" href="#木偶的隐喻"></a> 木偶的隐喻</h4><p>木偶拥有半个灵魂，被背后更高级、更无意识的力量操作，符合弗洛伊德对人类无意识的解释。但我们不必只是这样，而变成<strong>一个强大而自治的人</strong>。幕后到底发生了什么？</p><p>我们还没有关于意识的科学模型，存在主义的基础。</p><p>自然、文化，而个人是自然和文化互动下的产物，而人也具有<strong>自主性/自由意志</strong>，影响着自然和文化的互动过程。</p><blockquote><p>The divine of the individual is to use language to call the world into being.</p></blockquote><p>有研究表明，孩子在家庭中的共享环境对其成长影响并不大，有影响的是大人与孩子形成的个人关系（微环境），这类研究还在进行。</p><p>即便当下<strong>只有两个糟糕的解决方案</strong>可供选择，你也要自主地选择一个，否则恶龙会追逐你。</p><blockquote><p>说实话的一大好处是你不需要记住自己到底说了什么。</p><p>而谎言的问题在于它会失控般的生长。</p></blockquote><h4 id="小虫子的隐喻"><a class="markdownIt-Anchor" href="#小虫子的隐喻"></a> 小虫子的隐喻</h4><p>是什么困扰着你？良心是一个很神奇的东西。要学会倾听并遵从你的良心。良心会点明我们的愚蠢和不足。但良心不是全知全能的上帝，它只是一份潜在的指南，所以我们需要与它对话，共同成熟。生活很复杂是不可能简化抽象为明确的指导规则的。</p><blockquote><p>Always let your conscience be your guide.</p></blockquote><p>没有真正领悟/体验/经验到的道理是空洞的，只是一些概念和辞藻。道理需要加入自我理解与洞察。在这个过程中，知识以隐性形式存在，然后突然显露。所以，要敏感地观察自己，观察你的那些幻想（冥想是有所帮助的）。</p><p>恋母癖的孩子的母亲总是有这样的倾向，她们会隐含的与孩子做交易，即你可以不用做任何困难的事，代价就是你不要离开，只要你不离开，我会照顾你的。</p><h4 id="反派角色隐喻"><a class="markdownIt-Anchor" href="#反派角色隐喻"></a> 反派角色隐喻</h4><p><strong>狐狸和狸猫</strong></p><p>一些精神变态者将利用他的能力欺骗你，以证明自己拥有无所不能的全能、全知和自恋，他可以利用你。他们非常擅长操纵人。比如Youtube上可以看到Paul Bernardo。</p><p>骗子的诱惑，你可以得到你想要的东西，而不需要付出任何代价，有一条轻而易举的成功之路。而且真正的邪恶往往很强大，很具有操纵性。</p><p>你要学会面对冲突，你学习如何玩游戏，但是你不会失去想赢的动力，而是把它<strong>整合</strong>到游戏中。</p><p>明星在荣誉与金钱背后其实付出了很多东西，要慎重，那些付出是否正在榨干你。</p><p><strong>反派木偶师</strong></p><p>暴君，专制，完美主义，睚眦必报，取悦周围人，不允许规则之外的东西存在</p><p>我们需要有真正的朋友，一些可以和我们讲真话的朋友，<strong>真诚地批评我们</strong>。想象一下希特勒和个人崇拜。</p><p>《古拉格群岛》你需要成为自己的暴君，压抑生活中的苦难，否则你的生活就是那些已经被确定为“正确”的制度的反例。所以每个人都通过说谎参与到整个制度之中。</p><p>有的时候我们不能怪罪孩子会说谎，大多时候更聪明的孩子更早学会说谎。</p><p>背叛是最严重的罪恶，因为相信（不是单纯的轻信）别人是需要勇气的。</p><blockquote><p>Do not do anything that other can do themselves, or you just steal it.</p></blockquote><p>力量、远见、明智、智能、仁慈、机敏、表达等这些好的品质会整合在一起。</p><h4 id="极乐岛"><a class="markdownIt-Anchor" href="#极乐岛"></a> 极乐岛</h4><p>不要简单地把世界想象为受害者和加害者两个阵营，而你作为一个“无害”的人，站在一边。这样的世界观很容易受到煽动。而且受害者视角会给我们一个逃避责任的<strong>借口</strong>，而不需要承受道德压力，不用在乎任何事。但<strong>生活可以提供给你的意义感和你可以承担的责任数量成正比。</strong></p><p>就像之前我发现的那样，<strong>很多时候事情变得糟糕并不是你的问题，但你要负责从糟糕的境地中走出来</strong>。不要出卖自己。</p><p>享乐主义（追求当下让你满足的东西）的问题在于在中长期来看你很难应付自己不断升高的阈值，而且让你满足的东西不一定能契合他人的需要。而你没有让自己的人格充分展开，会沦为暴君的奴隶。人们会沦为意识形态的奴隶。</p><p>小孩子过早的愤世嫉俗是一个很不好的兆头。虚假信心。他们不相信努力和牺牲会产生任何有价值的东西。他不会承认自己不完美的地方。</p><p>在你批判现实的结构之前，你需要看向自己不足的地方，</p><p>极乐岛，庆祝脱离现实。很多影视作品会将恐怖的元素和游乐园结合起来。</p><h4 id="鲸鱼的肚子"><a class="markdownIt-Anchor" href="#鲸鱼的肚子"></a> 鲸鱼的肚子</h4><p>成长中你必须以破坏性的方式与自己的某一部分切断联系，穿越混乱，最终获得新的稳定。而且我们需要完成<strong>象征意义上的“弑父杀母”</strong>，之后以自然和文化作为我们<strong>真正的父母</strong>，找到自己在这个世界中的位置。</p><blockquote><p>你最想找的东西在你最不愿意看见的地方</p><p><em>In sterquiliniis invenitur</em></p></blockquote><p><strong>承认自己的不足，承认自己是傻瓜</strong>，是继续追求的开始。</p><p>仁慈的旧制度会本能得阻碍创新性变革，这个过程会很痛苦，正如在鲸鱼的肚子里匹诺曹要通过生火逃出去，而他的父亲一直很担忧，而希望专注于钓鱼吃鱼。<strong>变革的背后蕴含着稳定的因素。</strong></p><p>仅仅保存自己是不够的，我们要<strong>整合起来，save both</strong>。</p><p>旧的人格必须消亡才会诞生出新的生命。将自己推向极限而后复活，<strong>重生</strong>。</p><h2 id="故事与元故事"><a class="markdownIt-Anchor" href="#故事与元故事"></a> 故事与元故事</h2><p>我们很擅长发现变化和异常，察觉到和我们认知模型预想的不同的地方。</p><p>大猩猩实验与盲目性。当我们改变自己的目标之后，世界会变成另一个世界。</p><p>我们需要了解自己的<strong>盲目性和局限性</strong>，知道我们<strong>永远都无法做到全知全能</strong>，没有一种可以涵盖/解释一切的理论/模型体系，至少现在不存在，所以我们需要<strong>保持开放，认真倾听</strong>。</p><p>人与人之间的政治差异。大五模型。</p><p>了解故事，弄清楚如何与世界相处。</p><p>生活十分复杂。有时候人们选择自杀来终止这种复杂性。</p><p>下丘脑的强大功能。避免陷入下丘脑的自动控制中。</p><p><strong>你看到的世界很大程度上取决于你设定的目标</strong>。你必须设定一个目标。</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/4.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>出现问题的时候，或者打破现状的时候，你在迫使自己或他人考虑相关性远远超过他们甚至难以适应的程度，就好像周围突然同时出现了800万只蛇。我们对待事物的方式也是和我们的目标/价值框架有关的，对于阻碍目标的事物呈现消极态度，反之是积极态度，而忽略掉那些无关的。</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/5.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="达尔文进化论模型"><a class="markdownIt-Anchor" href="#达尔文进化论模型"></a> 达尔文进化论模型</h4><p>突变也许是随机进行的，但是选择并不是随机的。文化结构存在得长久了就会变成环境的一部分。有一点像历史唯物主义？</p><p>突变很有意义，但并不是所有的方向/策略的价值都是相对的，在这些随机性当中我们也发展出了相对稳定的秩序/文化。</p><p>训练我们的大脑神经元，有意识与无意识。Pay attention to your errors.</p><p>认知框架、故事、参考系、微观人格。</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/6.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="皮亚杰认知模型"><a class="markdownIt-Anchor" href="#皮亚杰认知模型"></a> 皮亚杰认知模型</h4><p>最小化你对“不匹配”的事物的批判范围，<strong>限定混乱的范围</strong>，比如只是一件事没有做好，不要上升到对整个人的批判/评价上。比如回家之后发现同伴没有收拾屋子，就评价这件事情就好，而不是说，“你总是这样，你就是个懒惰的人”。</p><p>同一时间仅修正如下图所示的小的分支，而非将整个价值观连根拔起。<strong>自下而上而非自顶向下，而且控制住混乱蔓延的范围</strong>，否则你会总是处于应急的状态，没有一个可以依靠的认知框架。</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/7.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>选择你的事业的时候也是这样，只要保证现实的和预想的有百分之八十的元素重合就好，<strong>微调而非轻易向自己发起革命</strong>。（你完全知道革命过程中的痛苦与之后带来的收益，所以，慎重！）</p><p>你是希望在一件事上做到极致，还是让五件事都达到百分之八十以上？你真的想成为一个优秀的科学家么？你知道那意味着什么吗？</p><p><strong>不要为了保持潜力而不去成为！</strong>（You know what I mean）</p><p>阅读、思考与写作，尤其是写作，写作是锻炼思维的最好方式。（我自己还要加一个实践，与外界互动）进一步的，如果你可以present，可以演讲，可以以各种方式表达、传递信息。</p><p>问题/糟糕的事情发生的时候，我们该如何解释，如何划分责任？压抑/沮丧的人会过分自责。其实我们可以更积极一些，可以从这些失败中学习到很多。<strong>建议的策略是，首先分析当下状况，之后再分析自身的过错和责任，而不是反过来。</strong></p><h4 id="凤凰重生模式"><a class="markdownIt-Anchor" href="#凤凰重生模式"></a> 凤凰重生模式</h4><p>旧有的部分被“杀死”了，确实会很痛苦，而且过程中你可能无法重生。</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/8.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>## 符号表征的神经心理学<p>相对于唯物主义，教授更倾向于一点点神秘学观点，存在即是潜能，我们可以让世界按我们真心期望的那样展开。</p><p>人类是自然和文化的产物，而人类的意识<strong>同时又能作用于</strong>自然与文化。</p><p>生活就是不断处理超出你的理解范围的不断发生的问题，并在这个世界上彰显出自我。</p><p>流水线在很大程度上改变了我们整个人类社会。</p><h4 id="元规则"><a class="markdownIt-Anchor" href="#元规则"></a> 元规则</h4><p>《圣经》中即便是上帝也无法摆脱花园（秩序与已知）里的蛇（混乱与潜在）。</p><p>人类的生理局限性也决定了我们需要设立目标，需要有一个价值框架，不然面对的都是混沌与虚无。</p><p>小孩子可以在讲不清游戏规则的时候，很好地玩游戏。大猩猩也在践行这一种它们自己并不能表达清楚的等级制度。</p><p>世界上是由<strong>元规则</strong>的，并不是相对主义的那一套。 <strong>面对未知与混乱，我们解决它，试图重新建模</strong>，这是我们思考的方式以及人类道德体系的构建基础。这是这个世界的<strong>元规则、元事实</strong>。神话中的勇斗恶龙的英雄形象，位于所有文化结构的顶部，是那个<strong>眼睛</strong>的意向。Pay attention！</p><p>以社交焦虑为例子，参与一个聚会会感到紧张，因为外界在评价你、判断你，你面对着很多混沌。这个时候过多的自我意识是不好的，不要持续想着“人们会怎么看我？我是不是看起来得体？我好尴尬”等等。最简单的办法就是将注意力向外移，<strong>真切地关注他人</strong>，这样就启动了你的<strong>自然机制</strong>（哪怕是非常内向且神经质高的人，也是可以，甚至是擅长，进行一对一社交的）。而且更好的是，他人总是能带来一些你不知道的信息。</p><h4 id="模块与集中"><a class="markdownIt-Anchor" href="#模块与集中"></a> 模块与集中</h4><p>为什么我们的大脑是模块化的？<strong>模块化的优势</strong>。</p><ul><li>一个模块失效不会带来“中心节点失效”问题，不会全体瘫痪</li><li>每个模块自治可以独自做一些有创意的事情，而且它们之间还可以交流</li></ul><p>所以有人反对日益一体化的欧盟。</p><p>完全模块化是有问题的，完全集中也不可取，<strong>事情的决策会在这两极摇摆</strong>，这是个很复杂的问题。</p><p>大脑分左右半球，右半球更多的负责一些比较未知的部分（想象力与情感），左半球更倾向于掌握秩序（语言、逻辑等）。左脑与积极情绪相关。正面情绪和负面情绪之间也是需要<strong>平衡</strong>的。仅仅有积极情绪是很累人的。海马体主要负责检测现实的发展（左脑模型）与预想中（右脑模型）不匹配的地方，<strong>是两个模型之间的对决</strong>而非完全的现实与头脑的比较。</p><p>梦境的作用是什么呢？一些未知的、新的信息/模型从右脑传输至左脑？这也是学习的过程。梦境并不是随机的，它们只是比较难以理解罢了。</p><p>部落间交易的故事，协商的过程。</p><h4 id="美索不达米亚神话"><a class="markdownIt-Anchor" href="#美索不达米亚神话"></a> 美索不达米亚神话</h4><p>美索不达米亚文明，咸水神和淡水神，Apsu（男神），Tiamat（女神）紧紧拥抱在一起（缠绕在一起，和DNA类似，与中国阴阳的思想也吻合）。</p><p>他们的第一世后代，形成了原始力量（长老之神，远古神明），而他们做的第一件事就是杀死了Apsu（我们生活在文化的尸体上）。这象征着上帝之死，信念体系的崩塌。诺亚方舟，洪水神话也类似。</p><p>Timat恼怒准备清除这些神明，她聚集了一群怪兽，这些怪兽由很多种恐怖的元素组合而成（比如蛇神、鸟嘴等），Timat将怪兽头目命名为秦谷（类似撒旦）。</p><p>什么是众神之王？**众神之王可以面对混沌而重生秩序。**众神派出不同的神明去应对Timat，有一天他们催生了Marduk。他的天赋是口才，他可以讲魔法，比如将黑夜变为白天等。（这象征着人类交流能力的重要性）。可以看到（眼睛象征）又可以说话（交流）的东西应该是众神之王，是可以引领命运的。</p><p>Marduk使用风在Timat周围形成了网（制服混沌的概念之网，建立边界），他出去之后面对Timat，战胜了怪兽并杀死了秦谷。人类的构造中包含着恶魔之血。</p><p>摩西的故事中，人们深受毒蛇困扰，而上帝给予的方法是，将毒蛇钉在铜架上/做个铜像，让大家看着。所有去看蛇的人都不会被咬了。</p><p>苦难是超越的关键，心理治疗的理念也是如此，你必须自愿接受，直接面对困扰你的事情，观察它，冥想，超越它。</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/9.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="如何生活"><a class="markdownIt-Anchor" href="#如何生活"></a> 如何生活</h4><p>首先，选择玩**“元游戏”**而非仅仅应对一个一个不停涌来的问题。第二，放弃现有的，主动面对混沌，重建更广阔的秩序。</p><p>How can we tell stories that have meanings deeper than we know？</p><h2 id="符号表征模式"><a class="markdownIt-Anchor" href="#符号表征模式"></a> 符号表征模式</h2><p>在不同的情景下，代表元规则的符号表征有所不同，常见的表征方式如下图所示：</p><div align="center">  <img src="/2021/07/14/maps-of-meaning/10.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>当你觉察到最终的结果是值得的，你会主动进入“地下城”，进行部分人格的更新，经历冒险之后再回到地面。</p><blockquote><p>What you know might be your enemy. What you don’t know might be your best friend.</p></blockquote><p>约翰与鲸鱼的圣经故事，展示了如果一个人背叛了他的人生使命，会给自己、给他人、给世界带来这样的混乱与灾难。</p><h2 id="神话故事"><a class="markdownIt-Anchor" href="#神话故事"></a> 神话故事</h2><h3 id="创世纪与佛"><a class="markdownIt-Anchor" href="#创世纪与佛"></a> 创世纪与佛</h3><p>如果一群人有两个不同的信念系统该如何处理？比如冷战的时候，可以简化为两种思想形态。</p><p>尼采与陀神的思想有共同之处，包括马克思、弗洛伊德等人。</p><p>教授认为，个人无法创造自己的价值观，无法成为“超人”。没有谦虚，事情很容易失控。</p><p>我们需要在细节知识和模糊的全景图中间得到平衡。</p><p>创世纪故事，裸体，自我意识，善与恶的知识。</p><p>亚当的世界坍塌了，反复暴露在死亡中。</p><h3 id="洪水和通天塔"><a class="markdownIt-Anchor" href="#洪水和通天塔"></a> 洪水和通天塔</h3><p>真正的成年，不再依赖一个“父亲”的权威，明确意识到对于你自己的人生，没有人比你懂得更多。并没有一个绝对安全的地方，而且你也并不真的需要这样的安全，没有那么脆弱。</p><p>从小型的新鲜事物开始，锻炼适应能力。</p><p>“帮助”（代替）别人解决，只会使TA愈加虚弱。对别人、对自己都是这样。</p><p>生活确实充满了痛苦和挫折，但大多数时候我们只是盲目而无助地折磨自己。</p><p>注意灾难即将发生的小迹象，不要逃避，等着恶龙一点点长大。最终都是要面对的。很多时候大的灾难发生也是由于当事人最开始不够机警。</p><h2 id="个体的神性"><a class="markdownIt-Anchor" href="#个体的神性"></a> 个体的神性</h2><p>如果你拥有很多但总是内心苦闷，那么你应该放弃某些已经拥有的东西，不论是财富还是某些想法、观念、人设等等。Just let it go. 完成转化。</p><p>确认目标，调整定位，之后再“活在当下”。</p><p>如果现实令人不满/失望，那么我们唯一能做的就是首先接收事实，然后不断尝试，主动承担痛苦，最好的结果是解决了问题，给自己、给他人、给世界带来更好的结果。</p><p>如果你不再浪费时间，不再说谎，并且投入到真心想做的事情中，会创造多少好的成绩呢？会消除多少不够好的事情呢？</p><p>你不必立即修复所有的问题，但至少你自愿地、用心地尝试就可以了。</p><p>而<strong>自愿的意思，就是真心实意，全神贯注，all in</strong>。相信我，这非常有趣，有效，而且是唯一途径。</p><p><strong>So, what’s the meaning of life?</strong></p><p>[ 后面三节课听得非常不专心，没有什么收获，不知道是怎么了。可能是机翻的字幕干扰太大了。]</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>联邦学习基础知识与论文笔记</title>
    <link href="/2021/07/01/fl/"/>
    <url>/2021/07/01/fl/</url>
    
    <content type="html"><![CDATA[<p>科研中需要用到联邦学习知识，而且这也是目前的一个热点，所以系统学习整理下。</p><p>本文选择了杨强教授2020年和2021年出版的两本书《联邦学习》和《联邦学习实战》（偏向FATE框架），Google的综述和何朝阳博士的FedML、以及一些实际应用案例进行介绍。</p><h1 id="联邦学习"><a class="markdownIt-Anchor" href="#联邦学习"></a> 《联邦学习》</h1><p>在发展过程中，联邦学习的一些曾用名及相关领域有，面向隐私保护的机器学习（Privacy-Preserving Machine Learning）、面向隐私保护的深度学习（Privacy-Preserving Deep Learning）、协作式机器学习（Collaborative Machine Learning）、协作式深度学习（Collaborative Deep Learning）、分布式机器学习（Distributed MachineLearning）、分布式深度学习（Distributed Deep Learning） 、联邦优化（Federated Optimization）和面向隐私保护的数据分析（Privacy-Preserving Data Analytics）等。</p><p>联邦学习强调的核心理念是：<strong>数据不动模型动，数据可用不可见</strong>。联邦学习是利用分散在各参与方的数据集，通过隐私保护技术融合多方数据信息，协同构建全局模型的一种分布式训练方式。在模型训练过程中，模型的相关信息（如模型参数、模型结构、参数梯度等）能够在各参与方之间交换（交换方式可以是明文、数据加密、添加噪声等），但本地训练数据不会离开本地。</p><p>**可能存在联邦模型比集中式模型（理想模型）效果好的情况。**比如以下场景，部分客户端的数据质量非常差，集中式训练方法由于存在低质量的训练数据而质量糟糕；反而进行联邦训练时，会对本地情况进行考察，将异常客户端剔除，总体性能有所提升。</p><p>当前联邦学习的研究主要集中于提升安全性以及处理统计学难题、通信开销、滞留问题、容错性等方面，考虑安全框架、客户端污染等问题。目前常用平台有：</p><ul><li>FATE，微众银行，实现了一种基于同态加密和多方计算的安全计算协议，支持一系列的联邦学习架构和安全计算算法。</li><li>TFF，包含一个单机的实验运行模拟器。TFF接口由两层构成，联邦学习应用程序接口（Application ProgrammingInterface，API）和联邦学习核心API。</li><li>coMind关键组件是联邦平均算法的实现，搭建在TensorFlow的顶层并且提供实现联邦学习的高层API；</li><li>Horovod，由Uber创立，基于开放的消息传输接口（Message Passing Interface，MPI）支持联邦，工作在Tensorflow和Pytorch顶层，快速易用，目前还不支持加密方式。</li><li>OpenMined/PySyft提供了联邦学习和差分隐私。PySyft是PyTorch的一个简单外挂扩展。</li></ul><p><strong>根据样本和特征重叠程度不同</strong>，联邦学习可以分为，横向联邦学习（Horizontal FederatedLearning，HFL），纵向联邦学习（Vertical Federated Learning，VFL），联邦迁移学习（Federated Transfer Learning，FTL）。横向联邦学习也叫作基于样本划分的联邦学习，纵向联邦学习称为基于特征划分的联邦学习。联邦迁移学习适用于参与方的数据样本和数据特征都很少重叠的情况，特别适合处理<strong>异构数据的联邦问题</strong>。</p><ul><li>横向联邦，研究最多，常用于跨设备端的场景，当前线性模型、GBDT、RNN、CNN、横向矩阵分解等方法已经实现。基本上使用梯度下降等最优化算法的模型都能使用横向框架学习；</li><li>纵向联邦，常用于跨机构场景，当前线性模型、SecureBoost、神经网络、纵向矩阵分解、纵向因子分解机等都已经实现；</li><li>联邦迁移学习，强调在异构特征分布的多方场景下协同训练，当前研究较少，但应该是个热点。</li></ul><p>FL最常见的应用案例是在商业银行检测多方借贷活动，通过联邦学习，不再需要建立一个中央数据库。联邦迁移学习适用于金融场景中的风控建模，比如小微企业成立时间短，在信贷业务应用中存在数据稀缺、不全面、历史信息沉淀不足等问题，可以依据金融机构在中大型企业的信贷模型，将知识迁移到当前的小微企业中。</p><p><strong>根据各模型协调方式不同</strong>，可以分为集中式拓扑和对等网络拓扑。前者存在一个中心计算方负责收集模型信息并整合，易于设计和实现；后者则各方平等。</p><h2 id="一-隐私安全与机器学习"><a class="markdownIt-Anchor" href="#一-隐私安全与机器学习"></a> 一. 隐私安全与机器学习</h2><p><strong>面向隐私保护的机器学习（PPML</strong>），指使用了保护用户隐私和数据安全的防御技术的机器学习。2018年是其技术重大突破的一年。</p><p>与安全机器学习（Secure ML）中攻击者违反ML系统的完整性和可用性不同，PPML中攻击者违反了系统的隐私性和机密性。</p><ul><li>完整性，攻击导致ML系统检测错误，比如入侵检测为正常（假阴性）；</li><li>可用性，攻击导致出现分类错误（假阴性和假阳性），即系统不可用；</li><li>机密性，ML系统中的一些敏感信息（训练集或训练模型）泄露；</li></ul><div align="center">  <img src="/2021/07/01/fl/sec.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>一个ML系统可以分成数据发布、模型训练和模型推理阶段。数据发布阶段可能发生特征推理攻击（Attribute-Inference Attacks）。模型训练阶段可能发生<strong>重构攻击（Reconstruction Attacks）</strong>，目的是重构数据提供者的原始数据，或者学习关于数据的更多信息，而不是最终模型所提供的信息。模型推理阶段可以实施<strong>模型反演攻击（Model Inversion Attacks）<strong>或</strong>成员推理攻击（Membership-InferenceAttacks）</strong>。<strong>重构攻击是联邦学习的主要隐私关注点。</strong></p><ul><li>重构攻击，如果数据结构是已知的，梯度信息可能也会被利用，从而泄露关于训练数据的额外信息。为了抵御重构攻击，应当避免使用存储显式特征值的机器学习模型。模型训练过程中，安全多方计算和同态加密可以被用来通过保护计算中间结果来抵御重构攻击。</li><li>模型反演攻击，目的是从模型中抽取训练数据或训练数据的特征向量。应当向敌手暴露尽可能少的关于模型的信息，比如仅仅返回舍入后的预测值，同态加密的贝叶斯神经网络等；</li><li>成员推理攻击，敌手对模型至少有黑盒访问权限，同时拥有一个特定的样本作为其先验知识。敌手的目标是判断模型的训练集中是否包含特定的样本。</li><li>特征推理攻击，敌手出于恶意目的，将数据去匿名化或锁定记录的拥有者；有文献提出群组匿名化技术，通过泛化（generalization）和抑制（repression）机制抵抗这种攻击。</li></ul><p>对于攻击者我们通常认为他是<strong>半诚实的</strong>，即遵守协议，但也会试图从接收到的信息中学习更多除输出以外的信息。在密码学中，通常会首先建立一个针对半诚实的敌手的安全协议，然后通过零知识证明（zero-knowledge proof）对其进行加强，进而防御恶意的敌手的攻击。</p><blockquote><p>半诚实，诚实但好奇的</p></blockquote><p>对于每个安全模型，敌手会攻击一部分参与方使之腐败，而腐败的参与方可能相互勾结。参与方的腐败可以<strong>是静态的（static）</strong>，也可以是<strong>自适应的（adaptive）</strong>。敌手的复杂度可以是<strong>多项式时间（polynomial-time）的或无计算界限（computational unbounded）<strong>的，分别对应</strong>计算安全和信息理论安全</strong>。</p><h3 id="1-安全多方计算"><a class="markdownIt-Anchor" href="#1-安全多方计算"></a> 1. 安全多方计算</h3><p>对于任何功能需求，我们都可以在不必显示除了输出以外的前提下计算它。</p><p>通常情况下，安全多方计算能够通过三种不同的框架来实现：不经意传输（Oblivious Transfer，OT）、秘密共享（Secret Sharing，SS）和阈值同态加密（Threshold Homomorphic Encryption，THE）。<strong>秘密共享</strong>被广泛认为是安全多方计算的核心。</p><ul><li>不经意传输常见构造方法有，Bellare-Micali构造、Naor-Pinka构造以及Hazay-Lindell构造；姚氏混淆电路；不经意传输扩展。</li><li>秘密共享是指通过将秘密值分割为随机多份，并将这些份（或称共享内容）分发给不同方来隐藏秘密值；Shamir秘密共享基于多项式构建，其它还有算数秘密共享（安全乘法三元组）、二进制秘密共享。</li></ul><p>在PPML中的应用包括DeepSecure、SecureML、Chameleon和ID3。</p><h3 id="2-同态加密"><a class="markdownIt-Anchor" href="#2-同态加密"></a> 2. 同态加密</h3><p>同态加密（HE）是一种允许对密文进行计算操作并生成加密结果的加密技术。在密文上获得的计算结果被解密后与在明文上的计算结果相匹配，就如同对明文执行了一样的计算操作。</p><p>同态加密方案H由一个四元组组成，包括KeyGen（密钥生成函数），Enc（加密函数），Dec（解密函数）和Eval（评估函数）。</p><p>加法和乘法。</p><p>同态加密分为部分同态加密（Partially Homomorphic Encryption，进行加法或乘法），些许同态加密（Somewhat Homomorphic Encryption，进行有限次）和全同态加密（Fully Homomorphic Encryption，进行无限次）。许多研究人员目前正着眼于发现满足特定需求的、更有效的SHE方法，而非去发掘FHE方法。</p><p>在PPML中的应用包括CryptoNets、CryptoDL、GAZELLE、FedMF等。</p><h3 id="3-差分隐私"><a class="markdownIt-Anchor" href="#3-差分隐私"></a> 3. 差分隐私</h3><p>函数的输出结果对数据集中的任何特定记录都不敏感。因此，差分隐私能被用于抵抗成员推理攻击。中心思想是混淆数据，使得敌手无法从查询结果中辨别个体级别的敏感性。</p><p>差分隐私在向数据引入噪声的同时，权衡了实用性和隐私性。现有的机器学习差分隐私机制很少能达到较好的平衡。</p><p>加入噪声有两种方法，一种是根据函数的敏感性（可表示添加或删除单个样本，函数值可能发生变化的最大程度），一种是根据离散值的指数分布（指数机制，质量函数q）。</p><p>根据噪声添加的位置，可以分为输入扰动、目标扰动、算法扰动和输出扰动。</p><p>联邦学习中可以使用本地差分隐私（LDP），每一个输入方扰乱自己的数据，然后将已混淆的数据发布至不受信任的服务器。中心思想是<strong>随机回应（Randomized Response，RR）</strong>。</p><p>Moments accountant算法，Papernot等人的工作，基于差分隐私的LSTM，使用GAN生成差分隐私数据集。</p><h2 id="二-分布式机器学习dml"><a class="markdownIt-Anchor" href="#二-分布式机器学习dml"></a> 二. 分布式机器学习（DML）</h2><p>分布式机器学习也称为分布式学习，是指利用多个计算节点（也称为工作者，Worker）进行机器学习或者深度学习的算法和系统，旨在提高性能、保护隐私，并可扩展至更大规模的训练数据和更大的模型。</p><p>DML可以分为两类：面向扩展性的DML（Scalability-MotivatedDML）和面向隐私保护的DML（Privacy-Motivated DML）。</p><p><strong>面向扩展性的DML</strong>被广泛用于解决大规模机器学习问题中的计算资源和内存空间限制。<strong>并行技术</strong>（例如数据并行、模型并行和混合并行）是实现面向扩展性的DML系统的主要选择。</p><p><strong>面向隐私保护的DML</strong>主要用于保护用户隐私，并通过分散的数据存储来确保数据安全。安全多方计算、同态加密和差分隐私是面向隐私保护的DML系统里的常用隐私保护技术。</p><p>使用最广的DML数据处理系统之一是Apache Spark MLlib。基于图的并行处理算法是DML最近一个比较新的方法，GraphLab平台，另一个平台是Apache Spark GraphX。微软发布DMTK。其他DL框架，如TensorFlow、PyTorch也都支持DNN的分布式训练和部署。</p><h3 id="1-面向扩展性的dml"><a class="markdownIt-Anchor" href="#1-面向扩展性的dml"></a> 1. 面向扩展性的DML</h3><p>常用的DML方案包括数据并行、模型并行、图并行、任务并行、混合并行和交叉并行。</p><p>**图并行（Graph Parallelism）方法，**也称为以图为中心的方法（Graph-CentricApproach），是一种用于划分和分配训练数据和执行ML算法的新技术，其执行速度比基于数据并行的方法要快几个数量级。</p><p>使用了任务并行的大数据计算框架有Apache Storm和Apache YARN。</p><h3 id="2-面向隐私的dml"><a class="markdownIt-Anchor" href="#2-面向隐私的dml"></a> 2. 面向隐私的DML</h3><p>一般被列为ML系统隐私的信息有，训练数据输入、预测标签输出、模型信息（包括模型参数、结构和损失函数）和身份识别信息（如记录的数据来源站点、出处或拥有者）。</p><p>不同于其他ML算法，数据的划分对于<strong>决策树</strong>来说是至关重要的，因为决策树的学习需要决定特征集合的划分，这取决于特征属性的类别以及在某一特定属性下的具有类标签的样本数量。</p><p>常用的用于保护数据隐私的方法大致分为：模糊处理（随机化、添加噪声或修改数据使其拥有某一级别的隐私，如差分隐私方法）和密码学方法（不以明文传输或不传输入值，比如安全多方计算）。但是，随机扰动影响了数据精度和模型性能。与基于扰动的方法相比，密码学方法并不需要牺牲数据精度和模型性能，但是需要更多的额外计算。</p><p>当前研究成果：一种基于差分隐私的隐私保护逻辑回归；面向隐私保护的支持向量机；深度学习模型中也有所使用；无监督学习；在FedAvg基础上进行秘密共享、不经意传输，并考虑了在一个沟通成本高昂、客户加入退出频繁的复杂移动环境下的使用；基于MPC的方法；</p><h4 id="面向隐私保护的梯度下降法"><a class="markdownIt-Anchor" href="#面向隐私保护的梯度下降法"></a> 面向隐私保护的梯度下降法</h4><p>包括朴素联邦学习（Naive FederatedLearning或者Vanilla Federated Learning）、代数方法、稀疏梯度更新方法、模糊处理方法和密码学方法（如同态加密和安全多方计算）。</p><ul><li>前三种，每一方发送给协调方明文形式的梯度信息以更新模型，而这只能保护数据的原始形式，即低隐私保护等级和非常高的效率；稀疏梯度更新方法还能通过更新梯度中的一个实体子集，用精度来换取效率和隐私；</li><li>模糊处理方法，基于随机化（加入噪声）、泛化或抑制机制（如梯度分层化、差分隐私、k-匿名方法）；</li></ul><p>朴素联邦学习法中，如果是纵向联邦，模型在各方间分配。梯度下降方法里，<strong>目标函数能被分解为一个可微函数和一个线性可分函数</strong>。每一方将自己的数据用于各自的局部模型，从而获得中间结果，并将其正常发送给协调方。协调方将所有中间结果积累起来，并评估可微函数以计算损失和梯度。最后，协调方更新整个模型，并将更新后的局部模型发送给每个相关方。</p><p><strong>联邦学习是DML的一种特殊类型。</strong></p><h3 id="三-横向联邦"><a class="markdownIt-Anchor" href="#三-横向联邦"></a> 三. 横向联邦</h3><p>当前有研究关注：在联邦学习框架下对用户模型更新或者对梯度信息进行安全聚合；适用于模型参数聚合的加法同态加密，防御系统里的中央服务器窃取模型信息或者数据隐私；多任务形式的系统，多个参与方通过分享知识和保护隐私的方式完成不同的机器学习任务，同时进一步解决通信开销大、网络延迟以及系统容错等问题；深度梯度压缩减少通信带宽；考虑恶意用户。</p><p>两种架构：客户-服务器架构和对等网络架构。如果联邦平均算法使用了安全多方计算或加法同态加密技术，则能防范半诚实的（semi-honest）服务器的攻击。</p><div align="center">  <img src="/2021/07/01/fl/average.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>对等架构中，训练方们必须提前商定发送和接收模型参数信息的顺序，通常采用循环传输或随机传输（gossip学习）。</p><p>模型评估中，<strong>本地模型性能</strong>表示某一参与方在本地测试数据集上检验得出的横向联邦学习模型的性能，<strong>全局模型性能</strong>表示所有参与方在测试数据集上对横向联邦学习模型进行测试得出的模型性能。对等网络架构要得到全局模型性能将会更为复杂。一种可能的方式是选取某一个参与方来充当一个临时的协调方。</p><h4 id="1-fedavg介绍"><a class="markdownIt-Anchor" href="#1-fedavg介绍"></a> 1. FedAvg介绍</h4><p>为了区别于并行小批量随机梯度下降算法（parallel mini-batch SGD），联邦平均算法也被称为并行重启的随机梯度下降算法（parallel restarted SGD）或者local SGD。</p><p>相对于分布式优化问题，联邦优化有一些特性：</p><ul><li>数据集非独立同分布，不同参与方拥有的数据可能有着完全不同的分布；分布式学习中基本没有这个问题。</li><li>数据量不平衡，联邦学习的不同参与方通常拥有不同规模的训练数据集；</li><li>数量很大的参与方，分布式优化中并行工作机器的数量是可以轻易控制的，联邦学习则不然；</li><li>慢速且不稳定的通信连接，在数据中心里，人们期望计算节点彼此间能够快速通信，并且丢包率很低。然而，在联邦学习中，客户和服务器间的通信依赖于现有的网络连接。在FL中计算代价相比通信代价是微乎其微的。</li></ul><p>人们之前担心FedAvg不收敛，但最近的研究表明，充分参数化的DNN的损失函数表现得很好，特别是出现不好的局部极小值的可能性比以前认为的要小。<strong>Dropout训练方法的成功经验为联邦模型平均方法提供了一些直观的经验解释</strong>。</p><p>在此基础上，可以使用加法同态加密（AHE）如Paillier算法、基于带错误学习（Learning With Errors，LWE）的加密方法，来加强联邦平均算法的安全属性。使用AHE后，模型信息不会以明文形式传输，但加密操作和解密操作将会提高计算的复杂度，并且密文的传输也会增加额外的通信开销。而且为了评估非线性函数，需要使用多项式近似，引入精度和隐私性之间的权衡。</p><h4 id="2-fedavg改进"><a class="markdownIt-Anchor" href="#2-fedavg改进"></a> 2. FedAvg改进</h4><h5 id="提升通信效率"><a class="markdownIt-Anchor" href="#提升通信效率"></a> 提升通信效率</h5><p>压缩更新后的模型参数（通常是真正更新的无偏估计值），比如使用概率分层。</p><p>结构化参数更新，比如去除冗余、量化权重、哈夫曼编码以利用有效权重的偏倚分布等。</p><p>一种知名的梯度压缩方法是深度梯度压缩方法（DGC），动量修正、本地梯度截断、动量因子隐藏和预热训练。</p><p>如果仍然可以保证训练的收敛性，客户端也可以避免将不相关的模型更新上传到服务器，以降低通信开销。每个客户都检查其本地模型更新是否符合全局趋势，以及是否与全局模型改进足够相关。这样，每个客户端可以决定是否将其本地模型更新上传到服务器。</p><h5 id="参与方选择"><a class="markdownIt-Anchor" href="#参与方选择"></a> 参与方选择</h5><p>随机筛选出来的参与方发送资源查询消息，询问它们的本地资源以及与训练任务相关的数据规模。</p><p>协调方使用这些信息估计每一个参与方计算本地模型更新所需的时间，以及上传更新所需的时间，决定参与方。在给定一个全局迭代轮次所需的具体时间预算的情况下，协调方希望选择尽可能多的参与方。</p><h4 id="3-研究工作"><a class="markdownIt-Anchor" href="#3-研究工作"></a> 3. 研究工作</h4><p>通信开销问题是联邦学习系统面临的主要挑战之一，相关研究包括自适应通信策略AdaComm</p><p>解决IID问题的异步更新方法（《Asynchronous federatedoptimization》）。</p><p>一些研究工作希望去除可能会造成隐私泄露的协调方（中央服务器）。</p><p>具体应用中，首先是谷歌提出的移动终端应用的横向联邦学习进行输入预测，Gboard。</p><p>当前挑战如下：</p><ul><li>无法查看或者检查分布式的训练数据，很难选择机器学习模型的超参数以及设定优化器；</li><li>如何有效地激励公司和机构参与到横向联邦学习系统中来；</li><li>如何防止参与方的欺骗行为</li><li>由于模型的训练和评估在每一个参与方上都是本地进行的，我们需要发掘新的方法以避免过拟合以及触发提前停止训练</li><li>如何管理拥有不同可靠度的参与方</li></ul><h3 id="四-纵向联邦"><a class="markdownIt-Anchor" href="#四-纵向联邦"></a> 四. 纵向联邦</h3><p>数据集上具有相同的样本空间、不同的特征空间的参与方所组成的联邦学习归类为纵向联邦学习（Vertical Federated Learning，VFL）。横向联邦更多应用于B2C场景，纵向更多偏向B2B。</p><p>VFL系统的训练过程一般由两部分组成：1）对齐具有相同ID，但分布于不同参与方的实体；2）基于这些已对齐的实体执行加密的（或隐私保护的）模型训练。</p><p>架构中是否该保留一个受信任的第三方？</p><h4 id="1-安全联邦线性回归模型"><a class="markdownIt-Anchor" href="#1-安全联邦线性回归模型"></a> 1. 安全联邦线性回归模型</h4><div align="center">  <img src="/2021/07/01/fl/char.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2021/07/01/fl/proce.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2021/07/01/fl/pred.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-安全联邦提升树-secureboost"><a class="markdownIt-Anchor" href="#2-安全联邦提升树-secureboost"></a> 2. 安全联邦提升树 SecureBoost</h4><p>可以通过基于加密的数据库交集算法对样本进行对齐。</p><p>一项研究工作提出了角色区分，主动方（activeparty）不仅是数据提供方，同时拥有样本特征和样本标签，此外还扮演着协调者的角色，计算每个树节点的最佳分割；被动方（passive party）只是数据提供者，只提供样本特征，没有样本标签。</p><p>为了保证gi和hi的隐私性，主动方在将gi和hi发送给被动方之前，对梯度进行了加法同态加密。分割的评估将由主动方执行。</p><p>每一个被动方首先要对其所有的特征进行分桶，然后将每个特征的特征值映射至每个桶（buckets）中。基于分桶后的特征值，被动方将聚合相应的加密梯度统计信息。通过这种方法，主动方只需要从所有被动方处收集聚合的加密梯度统计信息。从而主动方可以更高效地确定全局最优分割。</p><p>在主动方得到全局最优分割之后，将特征id（kopt）和阈值id（vopt）返回给相应的被动方i。</p><div align="center">  <img src="/2021/07/01/fl/securebost.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>预测过程中，新样本的特征也分散于各个参与方中，并且不能对外公开。分类过程从主动方的root节点开始。</p><ul><li>主动方查询与当前节点相关联的[参与方id，记录id]记录。基于该记录向相应参与方发送待标注样本的id和记录id，并且询问下一步的树搜索方向（即向左子节点或右子节点）</li><li>被动方接收到待标注样本的id和记录id后，将待标注样本中相应特征的值与本地查找表中的记录[记录id，特征，阈值]中的阈值进行比较，得出下一步的树搜索方向。然后，该被动方将搜索决定发往主动方。</li><li>主动方接收到被动方传来的搜索决定，前往相应的子节点。</li><li>迭代步骤1～3，直至到达一个叶节点得到分类标签以及该标签的权值。</li></ul><p>纵向联邦学习中各参与方彼此间有更紧密的共生关系。训练很容易受到通信故障的影响，从而需要可靠并且高效的通信机制。</p><p>目前，大部分防止信息泄露或者对抗恶意攻击的研究都是针对横向联邦学习的场景。由于纵向联邦学习通常需要参与方之间进行更紧密和直接的交互，因此需要灵活高效的安全协议，以满足每一方的安全需求。</p><h3 id="五-联邦迁移学习"><a class="markdownIt-Anchor" href="#五-联邦迁移学习"></a> 五. 联邦迁移学习</h3><p>异构联邦学习，参与方的数据集之间可能只有少量的重叠样本和特征，数据集的分布和数据量差异较大，而某些参与方可能只有数据，没有或只有很少的标注数据。</p><p>联邦迁移学习可以帮助只有少量数据（较少重叠的样本和特征）和弱监督（较少标记）的应用建立有效且精确的机器学习模型。</p><p>迁移学习主要分为三类：<strong>基于实例的迁移、基于特征的迁移和基于模型的迁移</strong>。联邦迁移学习将传统的迁移学习扩展到了面向隐私保护的分布式机器学习范式中。</p><ul><li>基于实例的联邦迁移学习，横向联邦中参与方可以<strong>有选择地挑选或者加权训练样本</strong>，<strong>以减小分布差异</strong>，从而可以将目标损失函数最小化；纵向联邦中，参与方可能具有非常不同的业务目标。对齐的样本及其某些特征可能对联邦迁移学习产生负面影响（<strong>负迁移</strong>），所以参与方可以<strong>有选择地挑选用于训练的特征和样本</strong>，以避免产生负迁移。</li><li>基于特征的联邦迁移学习，参与方协同学习一个**共同的表征（representation）空间，**缓解从原始数据转换而来的表征之间的分布和语义差异。对于横向联邦，可以最小化参与方样本之间的最大平均差异学习表征空间；对于纵向联邦，可以通过最小化对齐样本中属于不同参与方的表征之间的距离达到。</li><li>基于模型的联邦迁移学习，参与方利用<strong>预训练模型</strong>作为联邦学习任务的全部或者部分初始模型。横向联邦学习本身就是一种基于模型的联邦迁移学习。纵向联邦学习，可以从对齐的样本中学习预测模型或者利用半监督学习技术，以<strong>推断缺失的特征和标签</strong>。</li></ul><p>相较于传统的迁移学习，联邦迁移学习有如下特点：</p><ul><li>数据限制，基于分布在多方的数据来建立模型，并且每一方的数据不能集中到一起或公开给其他方。</li><li>要求对用户隐私和数据（甚至模型）安全进行保护</li></ul><p>一个多方的联邦迁移学习系统可以被认为是多个两方联邦迁移学习系统的结合。假设每一方都是诚实但好奇（honest-but-curious）的，即所有方都遵守联邦的协议和规则，但他们会尝试从收到的数据中推测出尽量多的信息。</p><p>联邦迁移学习系统的安全定义。</p><p>假设所有标签都在A方，可以使用基于加密（如RSA）的掩码技术，在保护隐私的同时，匹配A方和B方之间具有相同ID的样本。最终目标是双方协作地建立一个迁移学习模型，在不向对方公开数据的情况下，尽可能准确地为目标域中的B方预测标签。</p><p>有许多研究讨论了通过梯度传输而导致的间接隐私泄露的潜在风险，为了防止双方知道对方的梯度信息，A方和B方用加密的随机值进一步掩藏了各自的梯度，然后A方和B方交换加密的掩藏梯度和损失。</p><p>在安全联邦迁移学习中，性能损失的唯一来源是<strong>最终损失函数的泰勒二级近似</strong>，而不是在神经网络中的每个非线性激活层。</p><p>同态加密技术通常需要大量的计算资源和大规模的并行能力才能得以扩展，因此在许多需要实时计算的应用中，使用同态加密是不合适的。另一种安全协议是<strong>秘密共享（secret sharing scheme）</strong>，没有精度损失，计算效率大大提高，缺点是在进行线上计算之前，必须离线生成和存储许多用于乘法计算的三元组数据。</p><p>书中分别介绍了基于同态加密和秘密共享的联邦迁移学习的训练和预测过程。</p><p>当前联邦迁移学习面临的三个挑战：</p><ul><li>需要制定一种学习可迁移知识的方案。该方案能够很好地捕捉参与方之间的不变性。每一个参与方都对各自本地模型的设计和训练拥有完全的控制权。需要平衡自主性和泛化性。</li><li>需要精确地了解每一个参与方对共享表征作出的贡献，并考虑如何保护每个参与方所贡献信息的隐私安全；</li><li>设计能够部署在联邦迁移学习中的高效安全协议。</li></ul><h3 id="六-联邦强化学习"><a class="markdownIt-Anchor" href="#六-联邦强化学习"></a> 六. 联邦强化学习</h3><p>强化学习可以有如下分类：基于模型（首先尝试建立环境的虚拟模型）与无模型（通过反复迭代来修正价值函数和智能体策略）；基于价值（试图学习价值函数，找最优策略）和基于策略（从策略参数中进行搜索）；蒙特卡洛更新（通过使用整个周期内的积累奖励评估）与时间差分更新（价值函数的新估计值和旧估计值的差值）；在策略与离策略。</p><div align="center">  <img src="/2021/07/01/fl/rl.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>分布式强化学习（Distributed Reinforcement Learning，DRL）在过去几十年得到了广泛的研究，但未涉及隐私保护问题。</p><p>大多数并行强化学习的设置采用的是迁移智能体经验或梯度的操作，但在隐私保护中是行不通的。HFRL应用并行强化学习应用的基础设置，并将隐私保护任务作为一项额外约束。</p><p>VFRL的一种可能的框架–联邦DQN（Federated DQN）。</p><h3 id="七-联邦学习激励机制"><a class="markdownIt-Anchor" href="#七-联邦学习激励机制"></a> 七. 联邦学习激励机制</h3><p>关键是制定一种奖励方法，公平公正地与参与方们分享联邦产生的利润。</p><p><strong>目标为</strong>最大化联邦的可持续性经营，同时最小化参与方间的不公平性，动态地将给定的预算分配给联邦中的各个参与方，还可以扩展为一种能够帮助联邦抵御恶意的参与方的调节机制。</p><p>通常使用的收益分享方法大致分为三类：</p><ul><li>平等，任何效用都平均分配；</li><li>边际效益，参与方的效益是它加入团队时所产生的效用；</li><li>边际损失，参与方的效益是它离开团队时所产生的效用；</li></ul><p>基于收益分享博弈法，反向拍卖，发布奖励（通过输出协议、信息论分析、<strong>模型改良</strong>等），纠正高估问题等方法。</p><p>设计激励机制的挑战在于：</p><ul><li>估计参与方加入联邦的代价成本；</li><li>如何估计参与方i对联邦做出的贡献；可以运行沙盒模拟。</li></ul><h3 id="八-应用场景"><a class="markdownIt-Anchor" href="#八-应用场景"></a> 八. 应用场景</h3><h4 id="1-cv"><a class="markdownIt-Anchor" href="#1-cv"></a> 1. CV</h4><p>分为目标检测、语义分割、运动跟踪、三维重建、视觉问答和动作识别等几个方面。</p><p>比如安装在人口密集区域如公园、购物广场及大学的摄像头组成联邦。本地目标检测模型被部署并开始工作。整个模型训练和部署过程都能以持续的方式执行，因为新的标注数据可以源源不断地加进来。</p><p>在医疗领域广泛用于疾病的诊断和预防。</p><p>为了加快训练过程，通常使用预训练模型来加速模型的收敛。然而，预训练机器学习模型与现有的联邦学习场景并不兼容。所以研发一种概率联邦学习框架，可以聚合预训练神经网络模型。更具体地说，其思想是跨客户匹配已训练的本地模型参数，以此构建全局模型。</p><p>最有可能发展的应用之一是基于分散在各种设备上的异构数据而构建的CV驱动自动驾驶系统。</p><p>联邦学习也对只有有限算力的用户设备带来了巨大挑战，1）开发专门用于DNN训练的硬件；2）促进如参数修剪、低秩分解、知识蒸馏等模型压缩技术的发展。</p><h4 id="2-nlp"><a class="markdownIt-Anchor" href="#2-nlp"></a> 2. NLP</h4><p>一个典型的应用场景是基于移动设备用户频繁键入的单词来学习词库外**（Out-of-Vocabulary，OOV）单词**。（这个可能要看一下）</p><p>唤醒词检测问题，唤醒词检测应用程序必须只消耗非常有限的能源预算，并且通常运行在内存和计算资源有限的微控制器上；而且需要有强鲁棒性。《Federated learning for keywordspotting》（ICASSP 2019），受到Adam[293]的启发，这一研究通过自适应平均策略优化了联邦平均算法解决数据non-IID、不平衡且高度分散的问题。</p><p>FetAtt方法，将注意力机制引入移动键盘输入建议的联邦学习中，将服务器模型参数作为查询对象，以客户模型参数作为键值，计算每个客户端的GRU神经网络各层相对于服务器GRU神经网络各层的注意力权值。可以通过客户端模型的细粒度聚合，对服务器模型进行微调以达到更好的泛化能力。</p><p><strong>联邦学习与无监督学习、半监督学习或迁移学习的结合</strong>是解决数据稀缺问题的一个很有市场的研究方向。</p><p>联邦学习中如何有效利用未标注数据。</p><h4 id="3-推荐系统"><a class="markdownIt-Anchor" href="#3-推荐系统"></a> 3. 推荐系统</h4><p>在推荐系统中，<strong>冷启动</strong>和用户数据隐私是两个未解决的主要问题。</p><p>一般来说，推荐模型可以分为四种：协同过滤、基于内容的推荐系统、基于模型的推荐系统和混合推荐系统。</p><ul><li>协同过滤（CF），通过对用户与商品的历史互动进行建模来实现推荐；矩阵高度系数，大多采用低秩因子分解方法/矩阵因子分解。</li><li>基于内容，商品由若干个关键词进行标记，而用户画像由描述该用户喜欢的商品种类的关键词组成，通过关键词对齐方法进行推荐；</li><li>基于模型，使用机器学习和深度学习技术，对用户-商品关系进行直接建模；适用于对非线性关系；</li><li>混合推荐，一种简单的混合方法是，先分别进行基于内容过滤预测和协同过滤预测，再将二者的结果聚合在一起。</li></ul><p>联邦协同过滤，或者使用深度因子分解机（Factorziation Machine，FM）模型替代协同过滤，进一步提高性能。</p><p>线上学习场景中应用联邦学习，即联邦在线学习排名（FederatedOnline Learning to Rank，FOLtR）。</p><p>一种针对推荐模型的联邦元学习框架，相比基线，联邦元学习推荐模型具有最高的预测精准度，并且仅需几个更新步骤便可以快速适应新用户。</p><h4 id="4-实际场景"><a class="markdownIt-Anchor" href="#4-实际场景"></a> 4. 实际场景</h4><p>金融、医疗、城市计算与智慧城市、边缘计算和物联网。</p><p>区块链具有不可变性和可跟踪性，是联邦学习中防止恶意攻击的有效工具。</p><p>随着探索更多的联邦学习应用场景，该领域变得越来越具有包容性。它涵盖了分布式机器学习、统计学、信息安全、加密算法、模型压缩、博弈论和经济学原理，以及激励机制设计等方面的研究和实践。</p><h1 id="联邦学习实战"><a class="markdownIt-Anchor" href="#联邦学习实战"></a> 《联邦学习实战》</h1><h2 id="一-背景"><a class="markdownIt-Anchor" href="#一-背景"></a> 一. 背景</h2><p>基于图的并行处理算法是DML最近一个比较新的方法。</p><h3 id="安全机制"><a class="markdownIt-Anchor" href="#安全机制"></a> 安全机制</h3><h4 id="1同态加密"><a class="markdownIt-Anchor" href="#1同态加密"></a> 1.同态加密</h4><p>可以分为三类：部分同态加密（Partially Homomorphic Encryp-tion，PHE），些许同态加密（Somewhat Homomorphic Encryption，SHE），全同态加密（Fully Homomorphic Encryption，FHE）。</p><h4 id="2差分隐私"><a class="markdownIt-Anchor" href="#2差分隐私"></a> 2.差分隐私</h4><p>采用了一种随机机制，使得当输入中的单个样本改变之后，输出的分布不会有太大的改变。</p><p>函数的输出结果对数据集里的任何特定记录都不敏感。因此，能被用于抵抗成员推理攻击。</p><p>可以分为中心化差分隐私和本地化差分隐私，区别主要在于差分隐私对数据处理的阶段不同。</p><p>目前实现差分隐私保护的主流方法是添加扰动噪声数据。定义全局敏感度。</p><p>拉普拉斯机制、高斯机制、指数机制</p><h4 id="3安全多方计算"><a class="markdownIt-Anchor" href="#3安全多方计算"></a> 3.安全多方计算</h4><p>MPC最初针对的是一个安全两方计算问题（即著名的“百万富翁问题”）而被正式提出的。</p><p>秘密共享（Secret Sharing，SS），不经意传输（ObliviousTransfer，OT），混淆电路（Garbled Circuit，GC）可以用来实现安全多方计算。</p><p>以上是联邦学习中涉及到的三大安全机制（最常用的还是同态加密），它们在计算性能、通信性能和安全性方面的对比如下：</p><ul><li>计算性能，耗时在求取梯度上，同态加密，计算在密文的状态下进行，密文的计算要比明文的计算耗时更长。</li><li>通信性能，同态加密传输的是密文数据，密文数据比明文数据占用的比特数要更大，因此传输效率要比明文慢；秘密共享为了保护数据隐私，通常会将数据进行拆分并向多方传输，完成相同功能的迭代。同态加密和差分隐私需要一次，而秘密共享需要多次数据传输才能完成。</li><li>安全性，同态加密由于传输的是密文数据，因此其安全性是最可靠的；秘密共享通过将模型参数数据进行拆分，只有当恶意客户端超过一定的数目并且相互串通合谋时，才有信息泄露的风险，总体上安全性较高；差分隐私对模型参数添加噪声数据，但添加的噪声会直接影响模型的性能。</li></ul><div align="center">  <img src="/2021/07/01/fl/table.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>Python的常见安全计算库有pycrypto，Python-Paillier，differential-privacy，diffprivlib，MPyC等。</p><h2 id="二-python实现横向联邦"><a class="markdownIt-Anchor" href="#二-python实现横向联邦"></a> 二. Python实现横向联邦</h2><p>在本地以循环的方式来模拟，而没有涉及到网路通信模块开发。</p><p>一般训练时需要调整的参数有，训练的客户端数量、全局迭代次数、本地模型的迭代次数、本地训练相关的算法配置（学习率、优化算法、训练样本大小等）、模型信息、数据信息。</p><p>服务端类的主要函数包括：</p><ul><li>定义构造函数，第一，将配置信息拷贝到服务端中；第二，按照配置中的模型信息获取模型；</li><li>定义模型聚合函数</li><li>定义模型评估函数</li></ul><p>客户端主要函数是：</p><ul><li>定义构造函数</li><li>定义模型本地训练函数</li></ul><p>整体实现代码在配套的Github上，</p><h2 id="三-fate框架"><a class="markdownIt-Anchor" href="#三-fate框架"></a> 三. FATE框架</h2><p>FATE（Federated AI Technology Enabler），是微众银行AI部门发起的联邦学习开源项目，全球第一个联邦学习工业级开源框架，为联邦学习生态系统提供了可靠的安全计算框架。</p><p>FATE目前支持三种类型的单机安装，分别是：使用Docker镜像安装FATE；在主机中安装FATE；使用Docker从源代码中构建FATE。推荐使用Docker镜像安装FATE，这样可以大大降低产生问题的概率（注意docker及docker-compose版本以及请检查8080、9060和9080端口）。</p><p>为了降低开发人员的部署难度，VMware与微众银行联合开发了KubeFATE。</p><p>在金融领域的应用是信用风险管理、反洗钱应用和交通违章保险。</p><h1 id="谷歌综述"><a class="markdownIt-Anchor" href="#谷歌综述"></a> 谷歌综述</h1><p>传统学习方法为<strong>集中式学习</strong>，将数据收集至服务端进行训练和预测，结果发送至客户端，但这会造成延时高、浪费终端设备资源、数据隐私风险大等问题。而如果将模型放到<strong>端侧</strong>，终端根据本地数据完成模型训练和预测又存在数据量少且无法利用其它用户数据信息的问题。故2016年由谷歌提出联邦学习。联邦学习（FL）是一种分布式学习框架，许多客户端（如移动设备、组织）在中央服务器（如服务提供商）的协调下<strong>共同训练模型</strong>，同时保护<strong>本地数据隐私</strong>。广泛来讲，联邦学习是为了解决<strong>数据孤岛</strong>问题。</p><p>目前联邦学习已成为一个比较热门的研究方向。作为一个研究方向，“联邦学习”概念很宽泛，经过这几年的研究其边界得到极大扩大。比如学习过程中的数据不均衡、数据非独立同分布（non-I.I.D）、设备不可靠、有限通信带宽等挑战。</p><p>联邦学习和一般<strong>分布式学习</strong>的主要区别？[ 主要还是数据的问题 ]</p><table><thead><tr><th></th><th><strong>分布式训练</strong></th><th><strong>联邦学习</strong></th></tr></thead><tbody><tr><td>数据分布</td><td>集中存储，但可以任意打乱、平衡地分配给所有客户端</td><td>分布式存储，数据无法互通、可能存在数据的Non-IID</td></tr><tr><td>节点数量</td><td>1~1000</td><td>1~10^10</td></tr><tr><td>节点状态</td><td>所有节点稳定运行</td><td>节点可能不在线</td></tr></tbody></table><p>联邦学习理论研究主要集中于针对优化算法的收敛性问题。</p><p>下表为FedML作者何朝阳总结的FL领域相关研究方向：</p><div align="center">  <img src="/2021/07/01/fl/challenge.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="1-fedavg"><a class="markdownIt-Anchor" href="#1-fedavg"></a> 1. FedAvg</h4><p>Server端将client端传送过来的模型（参数）进行加权求平均，之后发送会client端，反复迭代，直到模型收敛（已经证明）。如下图划线部分所示，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">n_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>代表第K个客户端所拥有的样本数量。</p><div align="center">  <img src="/2021/07/01/fl/fedavg.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-fedopt"><a class="markdownIt-Anchor" href="#2-fedopt"></a> 2. FedOPT</h4><p>FedOPT将本地的优化器和服务器端的优化器加以区分，两个优化器互相配合。</p><p>可作为一个通用框架理解FL中的一些挑战（针对下图框架中<strong>各个参数进行更现实的优化</strong>）：</p><ul><li>根据每个client端的计算能力设置其不同的迭代次数K</li><li>在Server可以将梯度视为伪梯度进行优化</li><li>每个client端的数据是异构的如何解决（这是区分于distributed learning的最大的点）</li><li>如何在端侧训练大模型</li><li>每一轮采样client的策略，传统使用uniform sampling，没有区分用户之间信息重要程度的差异</li><li>服务端聚合算法研究</li></ul><div align="center">  <img src="/2021/07/01/fl/fedopt.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-non-iid问题"><a class="markdownIt-Anchor" href="#二-non-iid问题"></a> 二. Non-IID问题</h3><p>各客户端之间数据分布的不一致性，包括特征、标签、数量等方面的分布偏差，甚至还有<strong>概念漂移</strong>问题。</p><p>非独立同分布主要有三个方面：</p><ul><li><strong>不同客户端数据分布不同</strong><ul><li>特征分布倾斜：P(x)不同；比如不同人的笔迹不同</li><li>标签分布倾斜：P(y)不同；比如企鹅在南极、北极熊在北极</li><li>标签相同特征不同：P(x|y)不同；概念飘移</li><li>特征相同标签不同：P(y|x)不同；比如点头表示yes / no?</li><li>数量不平衡</li></ul></li><li><strong>数据偏移</strong>：训练集测试集不同分布；比如以狗在草地上奔跑的照片训练，但使用狗在海里游泳的照片测试识别能力</li><li><strong>非独立</strong>：可用节点大多在附近的时区，图数据</li></ul><p>文章<a href="https://arxiv.org/pdf/1806.00582.pdf" target="_blank" rel="noopener">《Federated Learning with Non-IID Data》</a>重点研究了这个现象，定义了Weight divergence指标观察全局模型与本地模型之间的差异，发现FedAvg聚合策略下有non-IID的数据集的Global和Local模型间差异很大。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>d</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>W</mi><mrow><mi>F</mi><mi>e</mi><mi>d</mi></mrow></msup><mo>−</mo><msup><mi>W</mi><mrow><mi>S</mi><mi>G</mi><mi>D</mi></mrow></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>W</mi><mrow><mi>S</mi><mi>G</mi><mi>D</mi></mrow></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">Weight divergence = ||W^{Fed} - W^{SGD}||/||W^{SGD}||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">e</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">h</span><span class="mord mathdefault">t</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.149108em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">F</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord">/</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord">∣</span></span></span></span></span></p><div align="center">  <img src="/2021/07/01/fl/wd.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>后续后很多方法致力于改进此类问题，一般有三种思路：1）修改现有算法；2）创建一个可全局共享的小数据集；3）为不同客户端提供不同模型。</p><p><a href="https://arxiv.org/pdf/2102.02079.pdf" target="_blank" rel="noopener">Federated Learning on Non-IID Data Silos: An Experimental Study》</a>和<a href="https://arxiv.org/abs/2102.09743" target="_blank" rel="noopener">《Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques》</a>均有所总结。</p><div align="center">  <img src="/2021/07/01/fl/non.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="1-fedprox"><a class="markdownIt-Anchor" href="#1-fedprox"></a> 1. FedProx</h4><p>FedAvg中固定时间内没有完成E轮迭代的客户端会被drop掉，而实际中有些设备的计算/通信性能较差，这会严重影响模型效果。</p><p>针对**用户间数据异构（non-IID）和系统异构（设备间通信和计算能力差异）**两大挑战。</p><p>修改Client端损失函数，通过加入<strong>proximal term修正项</strong>，确保本地更新不要太够远离初始的global模型，即如下图所示，客户端损失函数由<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_k(*)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">∗</span><span class="mclose">)</span></span></span></span>变为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_k(*)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">∗</span><span class="mclose">)</span></span></span></span>。</p><p>定义<strong>inexact solution项</strong><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>y</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">y_k^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0766639999999998em;vertical-align:-0.2831079999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4168920000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831079999999999em;"><span></span></span></span></span></span></span></span></span></span>，通过对本地模型的非精确求解，动态调整本地迭代次数，弥合系统中不同client端的性能差异性。</p><div align="center">  <img src="/2021/07/01/fl/prox.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-skewscout"><a class="markdownIt-Anchor" href="#2-skewscout"></a> 2. SkewScout</h4><h1 id="联邦学习框架"><a class="markdownIt-Anchor" href="#联邦学习框架"></a> 联邦学习框架</h1><p>当前常见的联邦学习框架包括Tendorflow Federated（Google），LEAF benchmark（CMU），FedML以及工业界提供过的PySyft和FATE。做科研一般不会选择使用工业界框架，而是直接单机模拟。</p><p>之前有博客总结了以下几个框架：</p><ul><li>TensorFlow Federated专门针对研究用例，提供大规模模拟功能以及灵活的编排来控制采样。</li><li>PySyft 是用于安全的私有深度学习Python库。 PySyft使用PyTorch中的联邦学习，差分隐私和多方计算（MPC）将私人数据与模型训练分离。</li><li>Leaf 提供了多个数据集以及模拟和评估功能。</li><li>FATE（Federated AI Technology Enabler）是一个开源项目，旨在提供安全的计算框架来支持联邦AI生态系统。</li><li>PaddleFL 是基于PaddlePaddle 的开源联邦学习框架。在PaddleFL中，通过应用程序演示提供了几种联邦学习策略和训练策略。</li><li>Clara培训框架包括基于服务器客户端方法和数据隐私保护的跨孤岛联邦学习的支持</li></ul><p>本文主要介绍<strong>FedML框架</strong></p><h3 id="一-背景介绍"><a class="markdownIt-Anchor" href="#一-背景介绍"></a> 一. 背景介绍</h3><p>一些工业界主推的框架比较偏重分布式，对科研人员的学习成本较高；而另一些框架使用单机模拟，串行模拟不同client端的训练，耗时长，FedML希望弥补这里的gap。FedML是一个<strong>以科研为主要导向</strong>的开源框架，目前包括FedML-IoT、FedNLP、FedCV等部分，完整细节介绍可以参看其官网（<a href="https://fedml.ai/%EF%BC%89%E5%92%8C%E8%AE%BA%E6%96%87%E3%80%8AFedML:" target="_blank" rel="noopener">https://fedml.ai/）和论文《FedML:</a> A Research Library and Benchmark for Federated Machine Learning》。</p><p>FedML基于Pytorch，目前支持单机、分布式和边缘计算模式，并且方便自己的代码、数据集快速嵌入框架。FedML自身实现了不同FL场景下的很多种算法，如FedAvg、FedOPT、FedNAS等，另外其中包含了多种benchmark数据集。</p><p>FedML有如下几个<strong>值得注意的重要功能</strong>：</p><ul><li>WorkerManager中使用预定义的API：<code>register_message_receive_handler</code>和<code>send_message</code>可以定义任何类型可传送的数据，比如除梯度之外的一些辅助信息（auxiliary information）。</li><li><code>TopologyManager</code>可以支持多种联邦学习架构，比如常见的集中式，分布式（无server节点）、层次式、纵向联邦（VFL，又名特征分割FL）、Split Learning、FedNAS、Turbo-Aggregate等。</li></ul><div align="center">  <img src="/2021/07/01/fl/topo.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><ul><li>在底层API即<code>FL-core</code>中包含有关FL的安全性保障机制，如secret sharing，key agreement，digital signature等。同时也包含了最新的鲁棒性聚合方法，如DP，RFA，KRUM等。</li><li>相较于使用standalone的一些框架如PySyft、LEAF、TTF等（串行模拟），支持分布式训练（多GPU，以及同一个GPU上的多进程），速度更快。比如作者在CIFAR-10上训练ResNet时，是在8块GPU上模拟了112个worker。</li></ul><p>建议<strong>从FedAvg入手</strong>本框架。</p><h3 id="二-源代码分析"><a class="markdownIt-Anchor" href="#二-源代码分析"></a> 二. 源代码分析</h3><p><strong><a href="http://FedAvgAPI.py" target="_blank" rel="noopener">FedAvgAPI.py</a></strong>文件中封装了FedAvg算法。</p><p><code>process_id</code>参数仿照MPI架构实现（对应其区分不同进程的rank_id），取值为0时表示为server端，其他值为client端。Server端创建Aggregator、ServerManager（aggregator会作为它的一个参数）；client端创建Trainer和ClientManager。</p><p>整个模型的统领过程由ServerManager完成</p><ul><li>ServerManager中提供的register_message_receive_handler用于接收client端传来的参数并callback其中handle_message_receive_model_from_client函数，其中的aggregator会记录客户端发送过来的参数以及客户端的ID，当所有客户端发送完毕后（异步调用）会启动aggregate，结束后会进行下一轮客户端采样，将合并后的模型发送回客户端。</li><li>send_message_init_config，初始化需要发送的消息</li><li>send_message_sync_model_to_client，服务器发送消息到客户端</li><li>在Aggregator中完成测试任务</li></ul><p>客户端这边要接收到服务器端的两种消息：</p><ul><li>初始化消息与合并后的模型消息</li><li>模型更新（handle_message_receive_model_from_server，update_model），round次数更新（达到设定值后销毁进程）</li><li>__train，模型更新</li></ul><p>注意其中实现<strong>采样的机制</strong>是通过换数据的方式进行的，为了尽力模拟现实中大量用户的情况。</p><p><code>Message_define.py</code>中可以自定义需要传递的消息。</p><h3 id="三-环境配置与调试"><a class="markdownIt-Anchor" href="#三-环境配置与调试"></a> 三. 环境配置与调试</h3><p>两种配置方式，一种参考CI流程（用于线上验证），另一种是参考FedML博文（<a href="http://doc.fedml.ai/#/installation-distributed-computing%EF%BC%89%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%8C%E6%8E%A8%E8%8D%90%E4%BD%BF%E7%94%A8%E5%89%8D%E4%B8%80%E7%A7%8D%E3%80%82" target="_blank" rel="noopener">http://doc.fedml.ai/#/installation-distributed-computing）中的分布式环境搭建，推荐使用前一种。</a></p><p>分布式实验环境中NFS可以方便管理。</p><p>CI-install.sh中pyflakes检查脚本是否有误，使用miniconda加快CI速度。</p><p>standalone单机版适合小数据集下的浅层模型，规模变大后推荐distributed模式。</p><p>FedML_mobile，采用Flask+Pytorch+RabbitMQ实现，对科研人员的学习成本较低。首先架设FedML服务器…这一部分没有仔细看，后续如果用到的话可以参考B站视频。</p><h1 id="相关文献"><a class="markdownIt-Anchor" href="#相关文献"></a> 相关文献</h1><ul><li>《Advances and Open Problems in Federated Learning》Google主导综述</li><li>《FedML: A Research Library and Benchmark for Federated Machine Learning》</li><li>FedAvg：《Communication-Efficient Learning of Deep Networks from Decentralized Data》</li><li>FedOPT（ICML 2020）:《Adaptive federated optimization》</li><li>FedProx：《[Federated Optimization in Heterogeneous Networks》</li><li>non-IID：《Federated Learning with Non-IID Data》</li><li>non-IID：《Federated Learning on Non-IID Data Silos: An Experimental Study》</li><li>non-IID：《Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques》</li><li>non-IID：《Survey of Personalization Techniques for Federated Learning》</li><li><a href="https://fedml.ai/" target="_blank" rel="noopener">https://fedml.ai/</a></li><li><a href="https://github.com/chaoyanghe/Awesome-Federated-Learning" target="_blank" rel="noopener">https://github.com/chaoyanghe/Awesome-Federated-Learning</a></li><li>B站视频，FedML联邦机器学习框架视频教学全集</li><li>B站视频，联邦学习综述FAQ</li><li>B站视频，联邦学习FATE课程系列</li><li>知乎专栏：《联邦学习论文分享》</li><li>杨强老师的联邦学习课程（2021年春）<a href="https://ising.cse.ust.hk/fl/index.html" target="_blank" rel="noopener">https://ising.cse.ust.hk/fl/index.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>知识梳理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>联邦学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>兜哥学安全四件套笔记</title>
    <link href="/2021/06/29/dou-brother/"/>
    <url>/2021/06/29/dou-brother/</url>
    
    <content type="html"><![CDATA[<p>为了整理AI in Cybersecurity的应用及研究场景，粗浅浏览一些这一系列书目，包括《Web安全之机器学习入门》、《Web安全之深度学习入门》、《Web安全之强化学习与GAN》、《AI安全之对抗样本入门》。在微信读书里都可以在线免费阅读。这几本书都比较粗浅，可以选择感兴趣的领域深入研究下。</p><p>最近十分痛苦地认识到如果当年认真研究手头资料，会有更加不错的科研结果，也无需度过大概5年多的黑暗时光。但一切都是最好的安排，活在当下，面向未来。</p><h1 id="web安全之机器学习入门"><a class="markdownIt-Anchor" href="#web安全之机器学习入门"></a> 《Web安全之机器学习入门》</h1><h2 id="一-环境基础"><a class="markdownIt-Anchor" href="#一-环境基础"></a> 一. 环境基础</h2><p>Python重点库：Numpy、Scipy、NTLK（NLP领域）、Scikit-learn等、Tensorflow（现在学术上基本都使用pytorch，也写了相关笔记）。</p><pre><code class="hljs python"><span class="hljs-comment">## NTLK 重点功能</span><span class="hljs-keyword">import</span> ntlk<span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> treebanksentence = <span class="hljs-string">'At eight on Thursday morning'</span>tokens = nltk.word_tokenize(sentence)tagged = nltk.pos_tag(tokens)entities = nltk.chunk.ne_chunk(tagged) <span class="hljs-comment"># 语法树</span>t = treebank.parsed_sents(<span class="hljs-string">'wsj_0001.mrg'</span>)[<span class="hljs-number">0</span>]t.draw()</code></pre><p>数据集：</p><ul><li>KDD99（DAPRA，美国国防部高级研究计划局），9周网络连接与系统审计数据。异常类型被细分为4大类共39种攻击类型，其中22种攻击类型出现在训练集中，另有17种未知攻击类型出现在测试集中。网络入侵检测领域的权威测试集。前41想特征可以分为4大类：TCP连接基本特征、TCP连接的内容特征、基于时间的网络流量统计特征、基于主机的网络流量统计特征等。</li><li>HTTP DATASET CSIC 2020，针对Web服务的正常请求和25，000个攻击请求，包括SQL注入、缓冲区溢出、信息泄露、文件包含、xss等，广泛应用于WAF类产品测评。</li><li>SEA包含70多个UNIX系统用户行为日志，命令序列表示，100个命令为一块，共150个块，前三分之一数据块用作训练该用户正常行为模型，剩余三分之二数据块随机插入了测试用的恶意数据。将连续数据块看作一个会话，只能模拟连续会话关联的攻击行为；由于缺乏用户详细个人信息（职位、权限等）、数据维度单一（仅有命令信息）以及构造性（恶意数据由人工模拟）等因素，数据集在内部威胁检测研究中作用有限。</li><li>Schonlau（<a href="http://www.schonlau.net/%EF%BC%89%E5%8F%91%E5%B8%83%E9%92%88%E5%AF%B9Linux%E6%93%8D%E4%BD%9C%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%EF%BC%8C50%E4%B8%AA%E7%94%A8%E6%88%B7%E7%9A%84%E6%93%8D%E4%BD%9C%E6%97%A5%E5%BF%97%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%97%A5%E5%BF%97%E5%8C%85%E5%90%AB15000%E6%9D%A1%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%89%8D5000%E6%9D%A1%E9%83%BD%E6%98%AF%E6%AD%A3%E5%B8%B8%E6%93%8D%E4%BD%9C%EF%BC%8C%E5%90%8E%E9%9D%A2%E7%9A%8410000%E6%9D%A1%E6%97%A5%E5%BF%97%E4%B8%AD%E9%9A%8F%E6%9C%BA%E5%8C%85%E5%90%AB%E6%9C%89%E5%BC%82%E5%B8%B8%E6%93%8D%E4%BD%9C%E3%80%82%E6%AF%8F100%E6%9D%A1%E6%93%8D%E4%BD%9C%E4%BD%9C%E4%B8%BA%E4%B8%80%E4%B8%AA%E6%93%8D%E4%BD%9C%E5%BA%8F%E5%88%97%EF%BC%8C%E5%90%8C%E6%97%B6%E8%BF%9B%E8%A1%8C%E6%A0%87%E6%B3%A8%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%93%8D%E4%BD%9C%E5%BA%8F%E5%88%97%E5%8F%AA%E8%A6%81%E6%9C%891%E6%9D%A1%E6%93%8D%E4%BD%9C%E5%BC%82%E5%B8%B8%E5%B0%B1%E8%AE%A4%E4%B8%BA%E8%BF%99%E4%B8%AA%E6%93%8D%E4%BD%9C%E5%BA%8F%E5%88%97%E5%BC%82%E5%B8%B8%E3%80%82" target="_blank" rel="noopener">http://www.schonlau.net/）发布针对Linux操作的训练数据，50个用户的操作日志，每个日志包含15000条操作命令，其中前5000条都是正常操作，后面的10000条日志中随机包含有异常操作。每100条操作作为一个操作序列，同时进行标注，每个操作序列只要有1条操作异常就认为这个操作序列异常。</a></li><li>ADFA-LD主机级入侵检测数据集，包括Linux和Windows系统，记录了系统调用数据，已完成特征化和标注。每个数据文件都独立记录了一段时间内的系统调用顺序，每个系统调用都用数字编号。</li><li>Alexa域名数据集，Alexa是当前拥有URL数量最庞大、排名信息发布最详尽的网站。Alexa排名是常被引用的用来评价某一网站访问量的指标之一。</li><li>少量多种安全相关数据，开源攻击数据网站http://www.secrepo.com/</li><li>Movie Review Data，包含1000条正面的评论和1000条负面评论，被广泛应用于文本分类，尤其是恶意评论识别方面。记录的都是原始评论数据，全部为英文。</li><li>SpamBase入门级垃圾邮件分类数据集，不是原始的邮件内容而是已经特征化的数据，58个特征。</li><li>Enron数据集，真实邮件，归档邮件来研究文档分类、词性标注、垃圾邮件识别等，广义的Enron数据集指全量真实且未被标记的Enron公司归档邮件。</li></ul><h2 id="二-处理技巧"><a class="markdownIt-Anchor" href="#二-处理技巧"></a> 二. 处理技巧</h2><h3 id="1-特征提取"><a class="markdownIt-Anchor" href="#1-特征提取"></a> 1. 特征提取</h3><p>数字型特征预处理包括标准化、归一化和正则化。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessingX_scaled = preprocessing.scale(X)X_normalizzed = preprocessing.normalize(X, norm=<span class="hljs-string">'l2'</span>)min_max_scalar = preprocessing.MinMaxScalar()X_minmax = min_max_scalar.fit_transform(X_train)</code></pre><p>文本特征切分后可以直接词典化，或选用词集/词袋（加入频率统计）模型。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_extraction <span class="hljs-keyword">import</span> DictVectorizer<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<span class="hljs-comment"># 特征取值词典化，比如city：&#123;'Beijing','London','Shanghai','Sichuan'&#125;</span>vec = DictVectorizer()vec.fit_transform(X).toarray()vec.get_feature_names()<span class="hljs-comment"># 词袋模型</span>vectorizer = CountVectorizer(min_df=<span class="hljs-number">1</span>)X = vectorizer.fit_transform(corpus).toarray()vectorizer.get_feature_names()vocabulary = vectorizer.vocabulary_new_vectorizer = CounterVectorizer(min_df=<span class="hljs-number">1</span>, vocabulary=vocabulary)</code></pre><h2 id="三-web安全场景"><a class="markdownIt-Anchor" href="#三-web安全场景"></a> 三. Web安全场景</h2><h3 id="1-xss攻击"><a class="markdownIt-Anchor" href="#1-xss攻击"></a> 1. XSS攻击</h3><p>Cross Site Scripting，跨站脚本攻击，允许恶意Web用户将代码植入到提供给其他用户使用的页面中。这种类型的漏洞由于被黑客用来编写危害性更大的网络钓鱼攻击而变得广为人知。黑客界共识是：跨站脚本攻击是新型的“缓冲区溢出攻击”，而JavaScript是新型的“ShellCode”。</p><p>XSS的攻击方式就是想办法“教唆”用户的浏览器去执行一些这个网页中原本不存在的前端代码。</p><p>XSS被用来，盗取各类用户账号（机器登录账号、用户网银账号、各类管理员账号），控制企业数据（读取、篡改、添加、删除企业敏感数据），非法转账，强制发送电子邮件，网站挂马，控制受害者机器向其他网站发起攻击。</p><p>XSS分为反射型XSS、存储型XSS、DOM型XSS。</p><p>反射型XSS，非持久性XSS，是现在最常见的一种XSS漏洞。Payload一般写在URL中，之后设法让被害者点击这个链接。</p><p>存储型XSS，持久型XSS，存储型XSS是最危险的一种跨站脚本。被服务器端接收并存储，当用户访问该网页时，这段XSS代码被读出来响应给浏览器。不需要依靠用户手动触发。</p><p>DOM型XSS，Document Object Model，即文档对象模型。不需要与服务器交互的，它只发生在客户端处理数据阶段。DOM型XSS是前端代码中存在了漏洞，而反射型XSS是后端代码中存在了漏洞。</p><p>当前存在多种XSS攻击形式，如IMG标签无分好无引号、大小写不敏感、URL编码、IP十进制、绕符号过滤等。</p><p>目前已出现利用JS特性进行特殊编码变形绕过防御的手段，如Jsfuck（针对常见的JS函数、语法进行编码转换）、Aaencode（把js转为文字表情符号）等。还有大量XSS平台，自动生成js攻击载荷，常见做法是通过钓鱼邮件，IM消息诱骗受害者点击。</p><p><strong>应用案例1</strong>，2016年雅虎邮箱XSS漏洞，首先一封带有已知HTML标签的邮件进行fuzz，发现雅虎自动过滤bool属性的值但会保留等号，黑客可以无限制地插入带bool属性的HTML标签。</p><p><strong>应用案例2</strong>，WordPress插件Jetpack存储型XSS漏洞。</p><h3 id="2-sql注入"><a class="markdownIt-Anchor" href="#2-sql注入"></a> 2. SQL注入</h3><p>通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。利用现有应用程序，将SQL命令注入到后台数据库引擎执行。</p><p>常见SQL注入方式有强制产生错误（准备步骤），非主流通道技术（E-mail、DNS以及数据库连接），使用特殊字符，使用条件语句，利用存储过程，避开输入过滤技术，推断技术。</p><p>常见工具包括sqlmap（免费开源），HAVIJ等。</p><h3 id="3-webshell"><a class="markdownIt-Anchor" href="#3-webshell"></a> 3. Webshell</h3><p>以ASP、PHP、JSP或者CGI等网页文件形式存在的一种命令执行环境，也可以将其称为一种网页后门。</p><p>黑客在入侵了一个网站后，通常会将ASP或PHP后门文件与网站服务器Web目录下正常的网页文件混在一起，然后就可以使用浏览器来访问ASP或者PHP后门，得到一个命令执行环境，从而达到控制网站服务器的目的。</p><p>在KILLCHAIN模型中，WebShell属于command&amp;control环节。</p><p>Webshell通常具有以下功能：环境探针、资源管理、文件编辑、执行OS命令、读取注册表、创建Socket、调用系统组件。</p><h3 id="4-僵尸网络"><a class="markdownIt-Anchor" href="#4-僵尸网络"></a> 4. 僵尸网络</h3><p>僵尸网络是互联网上受到黑客集中控制的一群计算机，往往被黑客用来发起大规模的网络攻击，如分布式拒绝服务攻击（DDoS）、海量垃圾邮件等，同时黑客控制的这些计算机所保存的信息，进行资源滥用或挖矿。</p><p>Mirai僵尸网络（10万计的物联网设备）2016年对美国域名解析服务提供商Dyn公司进行峰值达1.1Tbps的DDoS攻击。Mirai恶意程序通过扫描物联网设备，尝试默认通用密码进行登录操作，一旦成功即将这台物联网设备作为“肉鸡”纳入僵尸网络中，进而操控其攻击其他网络设备。</p><p>2017年，Radware研究人员发现了Brickerbot僵尸网络，和Mirai僵尸网络有诸多相似之处，区别在于它可以对配置不当的物联网设备造成永久性破坏。通过部署针对性蜜罐而被发现。Brickerbot僵尸网络通过Telnet暴力攻击物联网设备。大部分被僵尸网络攻击的设备被Shodan识别为Ubiquiti。Bricker不下载二进制文件，很难分析。恶意代码首先获得对设备的访问，然后通过rm -rf /*命令擦除设备内存，禁用TCP时间戳，并将内核线程的最大数量限制为一个。刷新所有iptables防火墙和NAT规则（见图4-22），并添加一条规则来删除所有传出的数据包。</p><p>2016年第4季度IMPERVA缓解的最大型DDoS攻击，规模为650Gbps，依托Leet僵尸网络发起。</p><p>Amnesia僵尸网络利用了未修补的远程代码执行漏洞，目标是嵌入式系统。</p><h2 id="四-机器学习算法及应用场景"><a class="markdownIt-Anchor" href="#四-机器学习算法及应用场景"></a> 四. 机器学习算法及应用场景</h2><h3 id="1-k近邻"><a class="markdownIt-Anchor" href="#1-k近邻"></a> 1. K近邻</h3><p>KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别，因此较适合类域交叉或重叠较多的待分样本集。</p><p>K值含义 - 对于一个样本X，要给它分类，首先从数据集中，在X附近找离它最近的K个数据点，将它划分为归属于类别最多的一类。</p><p>常用算法包括Brute Force、K-D Tree和Ball Tree。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> NearestNeighborsnbrs = NearestNeighbors(n_neighbors=<span class="hljs-number">2</span>, algorithm=<span class="hljs-string">'ball_tree'</span>).fit(X)distance, indices = nbrs.kneighbors(X)nbrs.kneighbors_graph(X).toarray() <span class="hljs-comment">#可视化</span><span class="hljs-comment"># 可用于监督学习</span><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifierneigh = KNeighborsClassifier(n_neighbors=<span class="hljs-number">3</span>)neigh.fix(X,y)neigh.predit(X_test)</code></pre><h4 id="应用案例1异常操作检测"><a class="markdownIt-Anchor" href="#应用案例1异常操作检测"></a> 应用案例1——异常操作检测</h4><p>黑客入侵Web服务器后会通过系统漏洞进一步提权，搜集Linux服务器的bash操作日志，识别特定用户的操作习惯，找出异常操作行为。</p><p>以去重操作命令个数、最频繁的10个命令、最不常用的10个命令（以他们与整体的最频繁50个命令和最不常用50个命令的重合程度进行特征化）为特征。</p><pre><code class="hljs python"><span class="hljs-comment"># 统计最频繁使用的10个命令</span>fdist = FreqDist(cmd_block).keys()f2 = fdist[<span class="hljs-number">0</span>:<span class="hljs-number">10</span>]f3 = fdist[<span class="hljs-number">10</span>:<span class="hljs-number">0</span>]f2 = len(set(f2) &amp; set(dist_max))f3 = len(set(f3) &amp; set(dist_min))</code></pre><p>也可以进行全量数据比较，使用词集将操作命令向量化，就是首先统计出所有出现过的命令，之后统计各命令在每个用户那里的出现此处作为向量。</p><h4 id="应用案例2检测rootkit"><a class="markdownIt-Anchor" href="#应用案例2检测rootkit"></a> 应用案例2——检测Rootkit</h4><p>一种特殊的恶意软件，它的功能是在安装目标上隐藏自身及指定的文件、进程和网络链接等信息，一般都和木马、后门等其他恶意程序结合使用。使用KDD 99数据集，识别基于telnet连接的Rootkit行为。</p><p>和Rootkit相关的主要特征是TCP连接的内容特征，可以筛选出来。</p><h4 id="应用案例3检测webshell"><a class="markdownIt-Anchor" href="#应用案例3检测webshell"></a> 应用案例3——检测Webshell</h4><p>使用ADFA-LD数据集中WebShell相关数据。使用词袋模型CountVectorizer。</p><h3 id="2-决策树"><a class="markdownIt-Anchor" href="#2-决策树"></a> 2. 决策树</h3><p>决策树和随机森林是常见的分类算法，尤其是决策树，判断的逻辑很多时候和人的思维非常接近。</p><p>随机森林指的是利用多棵树对样本进行训练并预测的一种分类器，输出的类别是由个别树输出的类别的众数而决定。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> tree<span class="hljs-keyword">import</span> pydotplusclf = tree.DecisionTreeClassifier()clf.fit(X,Y)<span class="hljs-comment"># 可视化</span>dot_data = tree.export_graphviz(clf, out_file=<span class="hljs-literal">None</span>)dot_data = pydotplus.graph_from_dot_data(dot_data)clf.predict()<span class="hljs-comment">#随机森林</span><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> ExtraTreesClassifierclf2 = RandomForestClassifier(n_estimators = <span class="hljs-number">100</span>,)</code></pre><h3 id="应用案例1检测pop3暴力破解"><a class="markdownIt-Anchor" href="#应用案例1检测pop3暴力破解"></a> 应用案例1——检测POP3暴力破解</h3><p>利用“guess-passwd”、“normal”和“POP3”标记筛选出KDD99数据集中和POP3暴力破解相关的数据，挑选与POP3密码破解相关的网络特征以及TCP协议内容特征（0，4<sub>8和22</sub>30）。</p><h4 id="应用案例2检测ftp暴力破解"><a class="markdownIt-Anchor" href="#应用案例2检测ftp暴力破解"></a> 应用案例2——检测FTP暴力破解</h4><p>使用ADFA-LD数据集中FTP暴力破解相关数据，同样适用CountVectorizer词集模型进行特征化。</p><h3 id="3-朴素贝叶斯"><a class="markdownIt-Anchor" href="#3-朴素贝叶斯"></a> 3. 朴素贝叶斯</h3><p>贝叶斯分类是一系列分类算法的总称，这类算法均以贝叶斯定理为基础。其中朴素贝叶斯（Naive Bayesian, NB）是其中应用最为广泛的分类算法之一。NB算法是<strong>基于贝叶斯定理与特征条件独立假设</strong>的分类方法。</p><p>NB发源于古典数学理论，有着<strong>坚实的数学基础</strong>以及稳定的分类效率。同时，NB<strong>所需估计的参数很少</strong>，<strong>对缺失数据不太敏感</strong>，算法也比较<strong>简单</strong>。理论上，NB模型与其他分类方法相比具有最小的误差率。但是实际上NB模型假设属性之间相互独立，往往不成立，给模型的正确分类带来了一定影响。</p><p>典型算法包括高斯朴素贝叶斯（Gaussian Naive Bayes）、多项式朴素贝叶斯（Multinomial Naive Bayes）、伯努利朴素贝叶斯（Bernoulli Naive Bayes）。</p><p>早期垃圾邮件检测多使用朴素贝叶斯算法。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> GaussianNBgnb = GaussianNB()</code></pre><h4 id="应用案例1异常操作"><a class="markdownIt-Anchor" href="#应用案例1异常操作"></a> 应用案例1——异常操作</h4><p>同KNN部分。</p><h4 id="应用案例2检测webshell"><a class="markdownIt-Anchor" href="#应用案例2检测webshell"></a> 应用案例2——检测Webshell</h4><p>基于Webshell的文本特征，以网上搜集的Webshell作为黑样本，当前最新wordpress源码作为白样本，一个PHP文件作为一个字符串处理，进行基于单词的2-gram切割形成词汇表，将每个PHP文件向量化。</p><pre><code class="hljs python"><span class="hljs-comment"># 2-gram进行切割，ignore 忽略异常字符影响，token_pattern=r'\b\w+\b'表示按单词切割</span>webshell_bigram_vectorizer = CountVectorizer(ngram_range(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), decode_error=<span class="hljs-string">'ignore'</span>, token_pattern=<span class="hljs-string">r'\b\w+\b'</span>, min_df =<span class="hljs-number">1</span>)<span class="hljs-comment"># 处理白样本的时候也适用黑样本生成的词汇表进行向量化</span>wp_bigram_vectorizer = CountVectorizer(ngram_range(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), decode_error = <span class="hljs-string">'ignore'</span>, token_pattern = <span class="hljs-string">r'\b\w+\b'</span>, min_df=<span class="hljs-number">1</span>, vocabulary=vocabulary)</code></pre><p>针对函数调用建立特征，适用1-gram生成全局词汇表，基于函数和字符串常量进行切割，所以要修改token设置如下：</p><pre><code class="hljs python">webshell_bigram_vectorizer = CountVectorizer(ngram_range(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), decode_error=<span class="hljs-string">'ignore'</span>, token_pattern=<span class="hljs-string">r'\b\w+\b\(|'</span>\w+\<span class="hljs-string">''</span>, min_df =<span class="hljs-number">1</span>)</code></pre><h4 id="应用案例3检测dga域名"><a class="markdownIt-Anchor" href="#应用案例3检测dga域名"></a> 应用案例3——检测DGA域名</h4><p>以Alexa为白样本，以cryptolocker和post-tovar-goz家族DGA域名为黑样本。</p><p>以2-gram处理DGA域名，切割单元为字符</p><pre><code class="hljs python">cv = CountVectorizer(ngram_range(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), decode_error=<span class="hljs-string">'ignore'</span>,token_pattern=<span class="hljs-string">r'\w'</span>, min_df =<span class="hljs-number">1</span>)x = cv.fit_transform(x_domain_list).toarray()</code></pre><h3 id="应用案例4检测针对apache的ddos攻击"><a class="markdownIt-Anchor" href="#应用案例4检测针对apache的ddos攻击"></a> 应用案例4——检测针对Apache的DDoS攻击</h3><p>使用KDD99数据集，筛选网络连接基本特征、基于时间的网络流量统计特征、基于主机的网络流量统计特征。</p><h3 id="4-逻辑回归"><a class="markdownIt-Anchor" href="#4-逻辑回归"></a> 4. 逻辑回归</h3><p>逻辑回归也叫回归分析，是分类和预测算法中的一种。属于线性模型。在sklearn官网可以看到相关参数，比较重要的有：</p><p>1）random_state；2）C：正则化系数，越小正则化程度越高；3）Solver，算法包括“newton-cg”、“lbfgs”、“liblinear”、“sag”等，默认使用“liblinear”；4）n_jobs：并发任务数。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegressionclf = LogisticRegression(C=le5)</code></pre><h4 id="应用案例1java溢出攻击"><a class="markdownIt-Anchor" href="#应用案例1java溢出攻击"></a> 应用案例1——Java溢出攻击</h4><p>ADFA-LD数据集中Java溢出攻击相关数据，使用CountVecoriizer，使用le5逻辑回归算法。</p><h3 id="5-支持向量机svm"><a class="markdownIt-Anchor" href="#5-支持向量机svm"></a> 5. 支持向量机SVM</h3><p>基本上所有的分类问题，尤其是二分类问题，都可以先用它试下。</p><p>依靠核函数映射到高维空间，解决线性不可分问题。[<strong>升维和线性化</strong>]</p><p>应用核函数的展开定理，就不需要知道非线性映射的显式表达式；由于是在高维特征空间中建立线性学习机，所以与线性模型相比，不但几乎不增加计算的复杂性，而且在某种程度上避免了“维数灾难”。[<strong>核函数展开和计算理论</strong>]</p><p>常用核函数有：线性核函数、多项式核函数、径向基核函数和二层神经网络核函数。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svmclf = svm.SVC(kernel = <span class="hljs-string">'linear'</span>)clf.fit(X,Y)<span class="hljs-comment"># 构建超平面</span>w = clf.coef_[<span class="hljs-number">0</span>]a = -w[<span class="hljs-number">0</span>]/w[<span class="hljs-number">1</span>]xx = np.linspace(<span class="hljs-number">-5</span>, <span class="hljs-number">5</span>)yy = a * xx - (clf.intercept_[<span class="hljs-number">0</span>] / w[<span class="hljs-number">1</span>])<span class="hljs-comment"># 支持向量</span>b = clf.support_vectors_[<span class="hljs-number">0</span>]yy_down = a * xx + (b[<span class="hljs-number">1</span>] - a * b[<span class="hljs-number">0</span>])b = clf.support_vectors_[<span class="hljs-number">-1</span>]yy_up = a * xx + (b[<span class="hljs-number">1</span>] - a * b[<span class="hljs-number">0</span>])<span class="hljs-comment"># 画图</span>plt.plot(xx, yy, <span class="hljs-string">'k-'</span>)plt.plot(xx, yy_down, <span class="hljs-string">'k--'</span>)plt.plot(xx, yy_up, <span class="hljs-string">'k--'</span>)plt.scatter(clf.support_vectors_[:,<span class="hljs-number">0</span>], clf.support_vectors_[:,<span class="hljs-number">1</span>], s=<span class="hljs-number">80</span>, facecolors=<span class="hljs-string">'none'</span>)plt.scatter(X[:,<span class="hljs-number">0</span>], X[:,<span class="hljs-number">1</span>], c=Y, cmap=plt.cm.Paired)plt.axis(<span class="hljs-string">'tight'</span>)plt.show()</code></pre><h4 id="应用场景1识别xss"><a class="markdownIt-Anchor" href="#应用场景1识别xss"></a> 应用场景1——识别XSS</h4><p>参考《基于WAVSEP的靶场搭建指南》，使用WVS等扫描器仅扫描XSS相关漏洞即可获取XSS攻击的Web日志。</p><p>选取Web日志中的，url长度、url中包含的第三方域名个数（http个数、https个数）、敏感字符个数（&lt;, ', &quot;, &gt;）和敏感关键字（alert, script=, onerror, onload, eval, src=）个数为特征，进行标准化（标准化、均值方差缩放、去均值）。</p><pre><code class="hljs python"><span class="hljs-comment"># 特征提取</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gen_len</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-keyword">return</span> len(url)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_url_count</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-keyword">if</span> re.search(<span class="hljs-string">'(http://)|(https://)'</span>, url, re.IGNORECASE): <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>    <span class="hljs-keyword">else</span>: <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_evil_char</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-keyword">return</span> len(re.findall(<span class="hljs-string">"[&lt;&gt;,\'\"/]"</span>, url, re.IGNORECASE))<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_evil_word</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-keyword">return</span> len(re.findall(<span class="hljs-string">"(alert)|(script=)(%3c)|(%3e)|(%20)|(onerror)|(onload)|(eval)|(src=)|(prompt)"</span>,url, re.IGNORECASE))<span class="hljs-comment"># 模型存储</span>joblib.dump(clf, <span class="hljs-string">"xss-svm-2000-module.m"</span>)clf = joblib.load(<span class="hljs-string">"xss-svm-2000-module.m"</span>)</code></pre><h4 id="应用场景2dga域名"><a class="markdownIt-Anchor" href="#应用场景2dga域名"></a> 应用场景2——DGA域名</h4><p>使用元音字母个数、去重后字母数字个数与域名长度比例、平均jarccard系数（两个域名之间）、HMM系数为特征。</p><p>将这些特征分布情况（纵轴为特征，横轴为域名长度）作图。</p><h3 id="6-k-means和dbscan"><a class="markdownIt-Anchor" href="#6-k-means和dbscan"></a> 6. K-Means和DBSCAN</h3><p>K-Means算法是最经典的基于划分的聚类方法，以空间中k个点为中心进行聚类，通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> K-Meansy_pred = KMeans(n_clusters=<span class="hljs-number">3</span>, random_state=random_state).fit_predict(X)plt.scatter(X[:<span class="hljs-number">0</span>], X[:<span class="hljs-number">1</span>], c=y_pred)plt.show()</code></pre><p>DBSCAN是一个比较有代表性的基于密度的聚类算法，与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。<strong>不需要事先知道要形成的簇类的数量</strong>，就可以发现任意形状的簇类，并能够识别出噪声点。</p><p>DBSCAN核心参数为：1）eps，同一聚类集合两个样本的最大距离；2）min_samples，同一集合中最小样本数；3）algorithm，分为’auto‘，’ball_tree’，‘kd_tree’，‘brute’；4）leaf_size，使用BallTree或cKDTree算法的叶子节点数；5）n_jobs，并发任务数。</p><pre><code class="hljs python"><span class="hljs-comment"># 聚类与可视化</span><span class="hljs-keyword">import</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCANdb = DBSCAN (eps=<span class="hljs-number">0.3</span>, min_samples=<span class="hljs-number">10</span>).fit(X)core_samples_mask = np.zeros_like(db.labels_, dtype=bool)core_sampls_mask[db.core_sample_indices_] = <span class="hljs-literal">True</span>labels = db.labels_colors = plt.cm.Spectral(np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, len(unique_labels)))<span class="hljs-keyword">for</span> k, col <span class="hljs-keyword">in</span> zip(unique_labels, colors):    <span class="hljs-keyword">if</span> k == <span class="hljs-number">-1</span>:        col = <span class="hljs-string">'k'</span>    class_member_mask = (labels == k)    xy = X[class_member_mask &amp; core_samples_mask]    plt.plot(xy[:,<span class="hljs-number">0</span>], xy[:,<span class="hljs-number">1</span>], <span class="hljs-string">'o'</span>, markerfacecolor=col, markeredgecolor=<span class="hljs-string">'k'</span>, markersize=<span class="hljs-number">14</span>)    xy = X[class_member_mask &amp; ~core_samples_mask]plt.plot(xy[:,<span class="hljs-number">0</span>], xy[:,<span class="hljs-number">1</span>], <span class="hljs-string">'o'</span>, markerfacecolor=col, markeredgecolor=<span class="hljs-string">'k'</span>, markersize=<span class="hljs-number">6</span>)plt.show()</code></pre><h4 id="应用案例检测dga"><a class="markdownIt-Anchor" href="#应用案例检测dga"></a> 应用案例——检测DGA</h4><p>同上文一样，以2-gram分割域名，进行词表映射。</p><p>可视化的时候使用TSNE作图。</p><pre><code class="hljs python">tsne = TSNE(learning_rate=<span class="hljs-number">100</span>)x = tsne.fit_transform(x)<span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> enumerate(X):    x1, x2 = x[i]    <span class="hljs-keyword">if</span> y_pred[i] == <span class="hljs-number">1</span>:        plt.scatter(x1, x2, marker=<span class="hljs-string">'o'</span>)    <span class="hljs-keyword">else</span>:        plt.scatter(x1, x2, marker=<span class="hljs-string">'x'</span>)    plt.show()</code></pre><h3 id="7-fp-growth-和-apriori"><a class="markdownIt-Anchor" href="#7-fp-growth-和-apriori"></a> 7. FP-growth 和 Apriori</h3><p>关联算法，无监督算法，自动从数据中挖掘出潜在的关联关系。</p><p>三个基本概念：支持度、置信度、频繁k项集。</p><ul><li>支持度，既有A又有B的概率，它表现的是A和B两个事件相对整个数据集合同时发生的频繁程度；</li><li>置信度，AB两个事件的相关程度，比如尿布和啤酒的置信度为0.8，表明在同时购买了两者的消费者中，购买尿布的80%又购买了啤酒。不满足交换律；</li><li>频繁k项集，满足最小支持度阈值的事件称为频繁k项集；</li></ul><p><strong>Apriori算法</strong>挖掘同时满足最小支持度阈值和最小置信度阈值的关联规则。使用频繁项集的先验知识，以一种称为“逐层搜索”的迭代方法，k项集用于探索（k+1）项集。任一频繁项集的所有非空子集也必须是频繁的。</p><p>主流的机器学习库对Apriori支持很少，但可以找到开源实现。</p><p>安全领域的Aprior应用非常广泛，比如挖掘关联WAF的accesslog与后端数据库的sqllog，识别SSH操作日志中异常操作等。</p><p><strong>FP-growth算法</strong>基于Apriori构建，采用了高级的数据结构减少扫描次数，大大加快了算法速度。FP-growth算法只需要对数据库进行两次扫描，而Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁。</p><p>基本过程包括构建FP树和从FP树中挖掘频繁项集。</p><p>一棵FP树通过链接来连接相似元素，被连起来的元素项可以看成一个链表。一个元素项可以在一棵FP树种出现多次。FP树的解读方式是读取某个节点开始到根节点的路径。路径上的元素构成一个频繁项集，开始节点的值表示这个项集的支持度。FP树中会多次出现相同的元素项，也是因为同一个元素项会存在于多条路径，构成多个频繁项集。但是频繁项集的共享路径是会合并的。取一个最小阈值，出现次数低于最小阈值的元素项将被直接忽略。</p><p>名气较大的开源实现是<strong>pyfpgrowth</strong>，support代表支持度，minConf代表置信度。</p><pre><code class="hljs python">patterns = pyfpgrowth.find_frequent_patterns(transactions, support)rules = pyfpgrowth.generate_association_rules(patterns, minConf)</code></pre><h4 id="应用案例1xss相关参数"><a class="markdownIt-Anchor" href="#应用案例1xss相关参数"></a> 应用案例1——XSS相关参数</h4><p>以xssed网站样例以及WAF的拦截日志中提取XSS攻击日志为样本，目标是分析出潜在关联关系，作为SVM、KNN等算法的特征提取依据。将每行日志文本按照一定分隔符切割成单词向量，完成向量化。</p><p>以十分严格的置信度运行（0.99）试图找到关联关系接近100%的情况。将支持度也下降到0.001，这意味着即使对应的关联关系出现的概率只有千分之一，只要它对应的是强关联，置信度超过0.99，我们也认为这是一种有价值的关联。</p><p>筛选结果，形成对应举例。</p><h4 id="应用案例2挖掘疑似僵尸主机"><a class="markdownIt-Anchor" href="#应用案例2挖掘疑似僵尸主机"></a> 应用案例2——挖掘疑似僵尸主机</h4><p>僵尸主机频繁更换IP难以挖掘，使用FP-growth分析防火墙拦截日志，挖掘浏览器的user-agent字段和被攻击目标URL之间的关联关系，<strong>初步确定</strong>潜在的僵尸主机。</p><h3 id="8-隐式马尔科夫模型"><a class="markdownIt-Anchor" href="#8-隐式马尔科夫模型"></a> 8. 隐式马尔科夫模型</h3><p>特别适合处理时序数据，挖掘时序数据前后的关系。在网络安全领域也广泛存在时序数据，比如网站的访问顺序、系统调用的顺序、管理员的操作命令等。</p><p>隐式马尔可夫模型（Hidden Markov Model, HMM）的<strong>基础假设</strong>是，一个连续的时间序列事件，它的状态由且仅由它前面的N个事件决定，对应的时间序列可以成为<strong>N阶马尔可夫链</strong>。需要通过可观察序列推测隐藏序列，就是隐式。</p><p><strong>HMMLearn</strong>是Python下的一个HMM实现，是从Scikit-Learn独立出来的一个项目。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> hmmlearn <span class="hljs-keyword">import</span> hmm<span class="hljs-comment"># 训练</span>l = hmm.GaussianHMM(n_components=<span class="hljs-number">4</span>, covariance_type=<span class="hljs-string">'full'</span>)model.startprob_ = startprobmodel.transmat_ = transmatmodel.means_ = meansmodel.covars_ = covars<span class="hljs-comment"># 可视化</span>X, Z = model.sample(<span class="hljs-number">500</span>)plt.plot(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], <span class="hljs-string">".-"</span>, label=<span class="hljs-string">"observations"</span>, ms=<span class="hljs-number">6</span>,         mfc=<span class="hljs-string">"orange"</span>, alpha=<span class="hljs-number">0.7</span>)<span class="hljs-keyword">for</span> i, m <span class="hljs-keyword">in</span> enumerate(means):    plt.text(m[<span class="hljs-number">0</span>], m[<span class="hljs-number">1</span>], <span class="hljs-string">'Component %i'</span> % (i + <span class="hljs-number">1</span>),             size=<span class="hljs-number">17</span>, horizontalalignment=<span class="hljs-string">'center'</span>,             bbox=dict(alpha=<span class="hljs-number">.7</span>, facecolor=<span class="hljs-string">'w'</span>))plt.legend(loc=<span class="hljs-string">'best'</span>)plt.show()</code></pre><div align="center">  <img src="/2021/06/29/dou-brother/hmm.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>HMM模型完成训练后通常可以<strong>解决3大类问题</strong>：</p><ul><li>输入观察序列获取概率最大的隐藏序列，最典型的应用就是语音解码以及词性标注；</li><li>输入部分观察序列预测概率最大的下一个值，比如搜索词猜想补齐等；</li><li>输入观察序列获取概率，从而判断观察序列的合法性。参数异常检测就输入第三种。</li></ul><h4 id="应用案例1识别xss攻击"><a class="markdownIt-Anchor" href="#应用案例1识别xss攻击"></a> 应用案例1——识别XSS攻击</h4><p>两种检测思路；第一种是学习正常业务模型识别异常；第二种是学习攻击模型进一步识别满足攻击语法的攻击行为。</p><p>正常的http请求中参数的取值范围都是确定的，可以用字母数字特殊字符来表示。</p><p>隐藏序列S1～S4四个状态间循环转化，概率符合<strong>转移概率矩阵</strong>，同时四个状态都有确定的概率，以观察序列中的A、C、N、T 4个状态展现，这个转换的概率称为<strong>发射概率矩阵</strong>。<strong>HMM建模过程就是通过学习样本，生成这两个矩阵的过程</strong>。</p><p>HMM的使用方式是通过学习正常来识别异常，即通常说的“以白找黑”。缺点是扫描器访问、代码异常、用户的错误操作、业务代码的升级等，都会产生大量误报。</p><p>目前另外一种开始流行的方法是，通过学习攻击报文，训练攻击模型，然后“以黑找黑”。这种方法虽然理论上可能会遗漏真实攻击，但是结果更加可控，可以达到可运维状态。</p><p><em>需要X_lens的原因是参数样本的长度可能不一致，所以需要单独输入。</em></p><h4 id="应用案例2识别dga域名"><a class="markdownIt-Anchor" href="#应用案例2识别dga域名"></a> 应用案例2——识别DGA域名</h4><p>HMM作为DGA僵尸网络区分的一个变量，非常具有区分性。</p><h3 id="9-图算法与知识图谱"><a class="markdownIt-Anchor" href="#9-图算法与知识图谱"></a> 9. 图算法与知识图谱</h3><p>网络安全领域在风控、威胁情报方面有很多非结构化网状数据，所以会用到图算法。</p><p>Neo4j是一个高性能的图形数据库，将结构化数据存储在网络上而不是表中，具有嵌入式、高性能、轻量级等优势，可进行关联关系展现。</p><h4 id="应用实例1识别webshell"><a class="markdownIt-Anchor" href="#应用实例1识别webshell"></a> 应用实例1——识别Webshell</h4><p>WebShell具有很多访问特征，其中和有向图相关的为：1）入度出度均为0；2）入度出度均为1且自己指向自己。</p><p>在生产环境实际使用中，我们遇到的误报分为以下几种（这个在科研论文中也要加以讨论）。</p><h4 id="应用实例2僵尸网络识别"><a class="markdownIt-Anchor" href="#应用实例2僵尸网络识别"></a> 应用实例2——僵尸网络识别</h4><p>定义阈值R，攻击的域名超过R的IP才列入统计范围。</p><p>定义计算jarccard系数的函数，作为衡量两个IP攻击集合相似度的方式，当两个IP攻击的域名jarccard大于等于N时才列入统计范围。</p><h4 id="应用实例3安全知识图谱"><a class="markdownIt-Anchor" href="#应用实例3安全知识图谱"></a> 应用实例3——安全知识图谱</h4><p>在安全领域应用知识图谱，可以挖掘数据之间潜在的联系，结合这些潜在的联系可以大大扩展我们的数据分析思路。</p><p>在风控领域有较多应用，如<strong>盗号检测、撞库攻击、刷单检测、威胁情报分析</strong>等。</p><p>撞库是黑客通过收集互联网已泄露的用户和密码信息，生成对应的字典表，尝试批量登录其他网站后，得到一系列可以登录的用户。</p><p>刷单一般可分为两种：一是单品刷销量为做爆款等做准备；二是刷信誉以提高店铺整体信誉度。还有一种刷单更为直接，以外卖软件为例子，同一个人使用两台手机，分别安装客户下单软件和商家接单软件，一下一接，捞取补贴。</p><p>在2016年的RSA大会上出现了<strong>10家威胁情报公司</strong>，其中包括老牌安全公司Symantec、Dell Security，也包括大量新秀如Webroot、CrowdStrike，其中还包含国内的一家创业公司ThreatBook。</p><p>可以<strong>挖掘后门文件间的关系</strong>。黑产通常通过传播后门文件入侵主机，组织起庞大的僵尸网络。后门文件通常通过连接C&amp;C服务器的域名来监听控制指令，后门文件中硬编码少量C&amp;C服务器的域名，然后自动化下载最近的C&amp;C服务器列表。通过静态分析后门文件中硬编码的域名，关联分析域名和文件之间的关系，可以挖掘出后门文件之间的潜在联系。</p><p>挖掘域名潜在联系，黑产通常会注册大量的域名用于C&amp;C服务器、钓鱼等，注册域名时会登记注册人的邮箱信息，通过关联IP、注册邮箱、域名可以挖掘潜在的关联关系。</p><h3 id="10-神经网络算法"><a class="markdownIt-Anchor" href="#10-神经网络算法"></a> 10. 神经网络算法</h3><p>浅层学习使用时，需要花费至少一半的时间在数据清洗与特征提取上，即特征工程。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifierclf = MLPClassifier(solver=<span class="hljs-string">'lbfgs'</span>, alpha=le<span class="hljs-number">-5</span>, hidden_layer_sizes=(<span class="hljs-number">5</span>,<span class="hljs-number">2</span>), random_state=<span class="hljs-number">1</span>)clf.fit()</code></pre><h4 id="应用案例1垃圾邮件"><a class="markdownIt-Anchor" href="#应用案例1垃圾邮件"></a> 应用案例1——垃圾邮件</h4><p>SpamBase这个入门级的垃圾邮件，直接跑了个DNN。</p><h4 id="应用案例2恶意评论"><a class="markdownIt-Anchor" href="#应用案例2恶意评论"></a> 应用案例2——恶意评论</h4><p>使用RNN处理Movie Review Data数据集</p><h4 id="应用案例3识别webshell"><a class="markdownIt-Anchor" href="#应用案例3识别webshell"></a> 应用案例3——识别Webshell</h4><h4 id="应用案例4生成常用密码"><a class="markdownIt-Anchor" href="#应用案例4生成常用密码"></a> 应用案例4——生成常用密码</h4><p>调用sklearn的序列生成器，分别设置不同的新颖度Temperature来生成密码。</p><h1 id="web安全之深度学习入门"><a class="markdownIt-Anchor" href="#web安全之深度学习入门"></a> 《Web安全之深度学习入门》</h1><h2 id="一-背景介绍"><a class="markdownIt-Anchor" href="#一-背景介绍"></a> 一. 背景介绍</h2><p>TFLearn是一个模块化和透明的深度学习库，构建在TensorFlow之上，它为TensorFlow提供高层次API，目的是快速搭建试验环境，同时保持对TensorFlow的完全透明和兼容性。</p><p>在CNN出现之前，图像分类算法依赖于复杂的特征工程。常用的特征提取方法包括SIFT（Scale-Invariant Feature Transform，尺度不变特征转换）、HOG（Histogram of Oriented Gradient，方向梯度直方图）、LBP（Local BianrayPattern，局部二值模式）等，常用的分类算法为SVM。</p><p>AlexNet、VGG等网络架构实现。</p><p>可以使用一维的卷积函数处理文字片段，提炼高级特征进行进一步分析。</p><p>序列生成是RNN非常有意思的一类应用，机器学习专家通过使用RNN<strong>学习大量Java源码和Linux内核源码</strong>，生成的Java代码和Linux内核源码几乎都可以编译通过。</p><p>序列生成中使用最多的是<strong>char RNN模型</strong>，以字符为最小的单元，把每个字符当做一个输入，这样一个单词、一句话甚至一篇文章都可以看成由字符组成的一个序列，通常这样的字符集合会包括字母、数字和常用标点。RNN本质上只能理解数字序列，所以需要建立一个映射关系，把字符映射成数字，这映射关系称为char-idx。</p><p>自然语言分析技术大致分为3个层面：词法分析、句法分析和语义分析。</p><p>CRF是一种概率化结构模型，可以看做是一个概率无向图模型，节点表示随机变量，边表示随机变量之间的概率依赖关系。</p><p>Seq2seq任务，使用编码器-解码器架构，在编码阶段将整个源序列编码成一个向量，在解码阶段通过最大化预测序列概率，从中解码出整个目标序列。</p><p><strong>OpenSOC是思科公司</strong>2014年在BroCon大会上公布的开源项目，但是没有真正开源其源代码，只是发布了其技术框架。主要由数据源系统、数据收集层、消息系统层、实时处理层、存储层、分析处理层组成。目前OpenSOC已经加入Apache工程改名为Apache Metron。</p><div align="center">  <img src="/2021/06/29/dou-brother/metron.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>相关数据如下所示：</p><ul><li>网络流量主要分为网络全流量和Netflow两种，网络全流量包含完整的网络数据，包含TCP/IP协议栈的数据，比如MAC头、IP头、TCP头、HTTP头以及HTTP载荷数据；常见获取方式为交换机镜像、分光镜和网络分流器3种。分光器成本低廉，并且特性稳定，是大型网络中流量复制的首选方案。但是也有其局限性，比如当光衰过大时无法使用。另外，如果网络中存在一定的电口链路，也是无法使用分光器进行流量复制的。这个时候就需要使用专用的流量复制设备——网络分流器（Network Tap）。Netflow常见的版本包括数据流时戳、源IP地址和目的IP地址、源端口号和目的端口号、输入接口号和输出接口号、下一跳IP地址、信息流中的总字节数和信息流中的数据包数量。</li><li>日志文件</li><li>Syslog是在一个网络中转发系统日志信息的标准，可用它记录设备的日志</li><li>SNMP是基于TCP/IP协议族的网络管理标准，是一种在网络中管理网络节点（如服务器、工作站、路由器、交换机等）的标准协议。SNMP能够使网络管理员提高网络管理效能，及时发现并解决网络问题以及规划网络的增长。网络管理员还可以通过SNMP接收网络节点的通知消息以及告警事件报告等来获知网络出现的问题。常见的网络设备都支持把日志和报警以SNMP的形式发送出来，通常把主动发送SNMP称为SNMP trap。</li><li>JDBC是一种用于执行SQL语句的Java API，可以为多种关系数据库提供统一访问，它由一组用Java语言编写的类和接口组成</li><li>爬虫</li></ul><p>数据收集层常用软件包括Logstash和Flume，针对网络全流量收集还有Bro。其中Logstash和Flume功能完全一样，选择一个即可。<strong>Logstash</strong>常用于日志处理，包括Inputs、Filters和Outputs。</p><p><strong>Bro是一款被动的开源流量分析器</strong>，主要用于对链路上所有深层次的可疑行为流量进行安全监控，为网络流量分析提供了一个综合平台，特别侧重于语义安全监控。Bro的目标在于搜寻攻击活动并提供其背景信息与使用模式。</p><p><strong>最常使用的消息系统是Kafka</strong>。Kafka是一种高吞吐量的分布式发布、订阅消息系统。系统由 Broker、topic、partition、Producer、consumer、Consumer Group组成。Kafka的正常运行需要依赖ZooKeeper, Kafka的安装包里默认会包含ZooKeeper。</p><p><strong>实时处理层主要使用Storm</strong>, Storm是一个免费开源、分布式、高容错的实时计算系统。Storm经常用于实时分析、在线机器学习、持续计算、分布式远程调用和ETL等领域。主要分为两种组件Nimbus和Supervisor，这两种组件本地都不保存状态，任务状态和心跳信息等都保存在ZooKeeper上。</p><p>Hadoop分布式文件系统（HDFS）被设计成适合运行在通用硬件上的分布式文件系统。HBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。HBase是GoogleBigtable的开源实现。Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变得非常简单。Sqoop则为HBase提供了方便的RDBMS数据导入功能，使得传统数据库数据向HBase中迁移变得非常方便。</p><p>Elasticsearch是一个基于Lucene的搜索服务，它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful的Web接口。</p><p>Apache Spark是专为大规模数据处理而设计的快速通用的计算引擎。</p><p><strong>Spark</strong>拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是，Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。与Hadoop不同，Spark和Scala能够紧密集成，其中的Scala可以像操作本地集合对象一样轻松地操作分布式数据集。Spark基于内存的计算模型天生就擅长迭代计算，多个步骤计算直接在内存中完成，只有在必要时才会操作磁盘和网络，所以说Spark正是机器学习的理想的平台。MLlib目前支持4种常见的机器学习问题：分类、回归、聚类和协同过滤。MLlib基于RDD，天生就可以与Spark SQL、GraphX、Spark Streaming无缝集成。</p><p><strong>ELK架构</strong>。</p><div align="center">  <img src="/2021/06/29/dou-brother/frame.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><div align="center">  <img src="/2021/06/29/dou-brother/kafa.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="二-相关数据集"><a class="markdownIt-Anchor" href="#二-相关数据集"></a> 二. 相关数据集</h2><ul><li>垃圾邮件，Enron-Spam数据集，都是真实环境下的真实邮件，非常具有实际意义。</li><li>互联网电影资料库（Internet Movie Database, IMDB）</li><li>SMS Spam Collection数据集骚扰短信，真实短信内容，包括4831条正常短信和747条骚扰短信。</li><li>SEA数据集，恶意操作</li><li>在Github上可以搜索一些Webshell样本</li><li>MIST数据集Malware Instruction Set for Behaviour Analysis，通过分析大量的恶意程序，提取静态的文件特征以及动态的程序行为特征，特征提取主要使用CWSandbox，MIST目前包含的恶意程序如下，主要分为APT、Crypto、Locker和Zeus。</li><li>Kaggle恶意软件数据集500G。</li><li>Kaggle Credit Card Fraud Detection数据集，极不平衡，诈骗频率只占交易频次0.172%。</li></ul><p>之后的章节基本上是对每个场景，提取特征，之后使用SVM、NV、MLP、RNN、CNN等进行处理。</p><h2 id="三-一些算法"><a class="markdownIt-Anchor" href="#三-一些算法"></a> 三. 一些算法</h2><p>TF-IDF，Word2vec，Doc2vec</p><p>XGBoost</p><h2 id="四-垃圾邮件识别"><a class="markdownIt-Anchor" href="#四-垃圾邮件识别"></a> 四. 垃圾邮件识别</h2><p>为了抵御垃圾邮件侵扰，通常会使用商用的邮件安全解决方案，常见的国外厂商包括Cisco、Blue Coat、Websense、Zscaler以及McAfee。</p><p>词袋、词集模型。借鉴了词袋模型的思想，使用生成的词汇表对原有句子按照单词逐个进行编码，tensorflow默认支持这种方式。</p><p>TF-IDF，字词的重要性与它在文件中出现的次数成正比，但同时与它在语料库中出现的频率成反比。TF表示词条在文档d中出现的频率。逆向文件频率（inverse document frequency, IDF）的主要思想是：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。TF-IDF模型<strong>通常和词袋模型配合使用</strong>，对词袋模型生成的数组进一步处理。</p><p>在朴素贝叶斯的实验中我们发现，并非词袋抽取的单词个数越多，垃圾邮件识别概率越大，而是有个中间点可以达到效果最佳，并且TF-IDF结合词袋模型会提升检测能力。</p><h2 id="五-恶意评论"><a class="markdownIt-Anchor" href="#五-恶意评论"></a> 五. 恶意评论</h2><p>使用Scikit-Learn的CountVectorizer对象进行词袋化处理，同时将抽取的词汇表保存，用于词袋化测试数据集。词汇表保存在CountVectorizer对象的vocabulary属性中，初始化CountVectorizer对象。其中有几个非常重要的参数。</p><p>借鉴了词袋模型的思想，使用生成的词汇表对原有句子按照单词逐个进行编码。在本例中，使用TensorFlow的tflearn.data_utils.VocabularyProcessor函数即可</p><p>CBOW模型能够根据输入周围n-1个词来预测出这个词本身，而Skip-gram模型能够根据词本身来预测周围有哪些词。Word2Vec最常用的开源实现之一就是gensim。参数min_count可以对字典做截断，出现少于min_count次数的单词会被丢弃掉；神经网络的隐藏层的单元数，推荐值为几十到几百。gensim的官方文档中强调增加训练次数可以提高生成的Word2Vec的质量，可以通过设置epochs参数来提高训练次数，默认的训练次数为5。</p><p>一部分单词找不到对应的Word2Vec。需要捕捉这个异常，通常使用python的KeyError异常捕捉即可。</p><p><strong>Doc2Vec</strong>原理与Word2Vec相同，分为分布式存储（DistributedMemory, DM）和分布式词袋（Distributed Bag of Words, DBOW）。Doc2Vec处理的每个英文段落，需要使用一个唯一的标识来标记，并且使用一种特殊定义的数据格式保存需要处理的英文段落。以英文段落为单位。为了提高效率，可以把之前训练得到的Word2Vec和Doc2Vec模型保存成文件形式。</p><h2 id="六-骚扰短信识别"><a class="markdownIt-Anchor" href="#六-骚扰短信识别"></a> 六. 骚扰短信识别</h2><p>特征提取处理流程和上文相同。</p><pre><code class="hljs python">vectorizer = CountVectorizer(                             decode_error=<span class="hljs-string">'ignore'</span>,                             strip_accents=<span class="hljs-string">'ascii'</span>,                             max_features=max_features,                             stop_words=<span class="hljs-string">'english'</span>,                             max_df=<span class="hljs-number">1.0</span>,                             min_df=<span class="hljs-number">1</span> )<span class="hljs-keyword">print</span> vectorizerx_train=vectorizer.fit_transform(x_train)x_train=x_train.toarray()vocabulary=vectorizer.vocabulary_<span class="hljs-comment"># tensorflow支持的词汇表模型</span>vp=tflearn.data_utils.VocabularyProcessor(max_document_length=max_document_length,                                          min_frequency=<span class="hljs-number">0</span>,                                          vocabulary=<span class="hljs-literal">None</span>,                                          tokenizer_fn=<span class="hljs-literal">None</span>)x_train=vp.fit_transform(x_train, unused_y=<span class="hljs-literal">None</span>)x_train=np.array(list(x_train))</code></pre><p>Word2vec训练完成后，单词对应的Word2Vec会保存在model变量中，可以使用类似字典的方式直接访问。一句话或者几个单词组成的短语含义可以通过把全部单词的Word2Vec值相加取平均值来获取。再做一步数据标准化。</p><p>XGBoost的支持Scikit-Learn风格的API接口，完全可以当做一个加强版的决策树来使用。</p><h2 id="七-linux后门检测"><a class="markdownIt-Anchor" href="#七-linux后门检测"></a> 七. Linux后门检测</h2><p>使用3-gram处理，TF-IDF提升分类性能，</p><h2 id="八-用户行为分析与恶意行为检测"><a class="markdownIt-Anchor" href="#八-用户行为分析与恶意行为检测"></a> 八. 用户行为分析与恶意行为检测</h2><p>将恶意内部人员和内部员工的异常操作统称为恶意操作。用户行为分析（User Behawiors Analysis, UBA），可提供以往被遗漏的数据保护和欺诈检测功能。</p><h2 id="九-webshell检测"><a class="markdownIt-Anchor" href="#九-webshell检测"></a> 九. WebShell检测</h2><p>“Web”的含义是需要服务器提供Web服务，“Shell”的含义是取得对服务器某种程度的操作权限。WebShell常常被入侵者利用，通过网站服务端口对网站服务器获取某种程度的操作权限。</p><p>常见Webshell检测方法有：</p><ul><li>静态检测，通过匹配特征码、特征值、危险操作函数来查找。只能查找已知的WebShell，并且误报率、漏报率会比较高。</li><li>动态检测，基于检测执行时刻表现出来的特征，比如数据库操作、敏感文件读取等。</li><li>语法检测，根据PHP语言扫描编译的实现方式，进行剥离代码和注释，通过分析变量、函数、字符串、语言结构的方式，来实现关键危险函数的捕捉。</li><li>统计学检测，通过信息熵、最长单词、重合指数、压缩比等进行检测。</li></ul><p>以WordPress、PHPCMS、Smarty、Yii等使用PHP开发的应用为白样本。</p><h2 id="十-智能扫描器"><a class="markdownIt-Anchor" href="#十-智能扫描器"></a> 十. 智能扫描器</h2><p>扫描器通过对目标网站发送攻击请求，根据应答内容判断是否存在漏洞，整个过程就是模拟黑客踩点和渗透的过程，常见的开源扫描器有Nikto、Paros proxy、Weblnspect、OWASP ZAP、WebScarab、Burpsuite等。</p><p>靶场，可以理解为人为搭建的具有各种Web漏洞的网站，主要用于测试Web扫描器、IDS、WAF以及进行攻防演练。比如WAVSEP。</p><p>是否可以让机器通过学习攻击样本，自动生成攻击载荷，而不是死板地照套模板规则呢？这个基本是RNN序列生成问题。</p><p>《Method of Detecting Vulnerability in Web apps using machine learning》</p><p>是否可以让机器自动识别登录页面并自动识别参数进行登录呢？</p><h2 id="十一恶意程序识别"><a class="markdownIt-Anchor" href="#十一恶意程序识别"></a> 十一.恶意程序识别</h2><p>将整个文件以字符串的形式保存，会用空格替换一些关键字。给不同种类的恶意软件分配对应的标签。</p><p>以单词为单位切分文件，提取N-Gram特征。</p><p>使用TF-IDF进一步处理，提升算法分类性能。</p><p>随机划分训练集、测试集。训练、测试、验证。</p><h2 id="十二-反对信用卡欺诈"><a class="markdownIt-Anchor" href="#十二-反对信用卡欺诈"></a> 十二. 反对信用卡欺诈</h2><p>常见的信用卡欺诈主要包括：</p><ul><li>失卡冒用：一是发卡银行在向持卡人寄卡时丢失，即未达卡；二是持卡人自己保管不善丢失；三是被不法分子窃取。</li><li>假冒申请：利用他人资料申请信用卡，或是故意填写虚假资料。最常见的是伪造身份证，填报虚假单位或家庭地址。</li><li>伪造信用卡：团伙做案，从盗取卡资料、制造假卡、贩卖假卡，到用假卡作案。伪造者经常利用一些最新的科技手段盗取真实的信用卡资料，有些是用微型测录机窃取信用卡资料，有些是伺机偷改授权机终端功能窃取信用卡资料，当窃取真实的信用卡资料后，便进行批量性的制造假卡，然后通过贩卖假卡大肆作案，牟取暴利。</li></ul><p>黑白样本的比例完全失衡，解决此类问题存在两种方法：</p><ul><li>降采样，就是从数据量占优势的数据集中随机选取一定数量的样本，通常选择的数量与数据量小的样本数量相当。比如使用np.random.choice函数随机选取。</li><li>过采样，保留数量占优势的样本，通过一定的算法，在数量较少样本的基础上生成新样本。常见的生成算法是<strong>Smote</strong>。</li></ul><p>Smote思想：1）随机选定n个少类样本；2）找出最靠近它的m个少类样本；3）任选最临近的m个少类样本中的任意一点，在两点上任选一点就是新增的数据样本。<strong>Python实现是imblearn</strong>。</p><h1 id="web安全之强化学习与gan"><a class="markdownIt-Anchor" href="#web安全之强化学习与gan"></a> 《Web安全之强化学习与GAN》</h1><p>网络安全专家一直试图把自己对网络威胁的理解转换成机器可以理解的方式，比如黑白名单、正则表达式，然后利用机器强大的计算能力从流量、日志、文件中寻找似曾相识的各类威胁。</p><h2 id="一-背景"><a class="markdownIt-Anchor" href="#一-背景"></a> 一. 背景</h2><p>在2017年的BlackHat安全会议上，阿里巴巴安全部门的研究人员演示了用声音和超声攻击依赖于陀螺仪、加速度计等微机电系统传感器输入信号的智能设备。AI设备的安全显然是AI安全的一个重要领域。</p><p>AI安全大体可以归纳为4类：AI设备的安全、AI模型的安全、使用AI进行安全建设以及使用AI发起攻击。</p><p>OpenAI Gym是一款用于研发和比较强化学习算法的工具包，其中包括了各种环境，目前有模拟的机器人学任务、桌面游戏、多位数加法之类的计算任务等。Keras-rl是Keras的一套强化学习库。</p><p><strong>XGBoost</strong>所应用的算法就是梯度提升决策树，既可以用于分类也可以用于回归问题中。XGBoost最大的特点在于，它能够自动利用CPU的多线程进行并行计算，同时在算法上加以改进，提高了精度。</p><p><strong>集成学习的思路</strong>是通过对多个分类器的分类结果进行某种组合来决定最终的分类，以取得比单个分类器更好的性能。可以分为两类：第一类是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是Boosting系列算法（adaboost和GBDT）；第二类是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是Bagging和随机森林（Random Forest）系列算法。</p><p><strong>Keras</strong>有两种类型的模型：序列模型（Sequential）和函数式模型（Model）。前者比较常见，函数式模型更为通用，序列模型是函数式模型的一种特殊情况。还可以<strong>支持多进多出</strong>的情况。网络可视化方法plot_model。</p><ul><li>Dense层，全连接；</li><li>activation层，relu、leakyrelu、tanh、sigmoid</li><li>dropout层，每次训练的时候随机选择一定的节点临时失效；</li><li>Embedding层，将输入的向量按照一定的规则改变维度；</li><li>Flatten层，将输入压平，多维输入一维化；</li><li>Permute层，将输入的维度按照给定模式进行重排；</li></ul><p>损失函数包括MSE、MAE、MAPE、MSLE、squared_hinge、hinge等，常用优化器为SGD、RMSprop、Adam等。</p><p>LSTM有通过精心设计的称作“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。</p><h2 id="二-强化学习"><a class="markdownIt-Anchor" href="#二-强化学习"></a> 二. 强化学习</h2><h3 id="1-单智力体强化学习"><a class="markdownIt-Anchor" href="#1-单智力体强化学习"></a> 1. 单智力体强化学习</h3><p>一个系统的状态的迁移变化，只与当前状态或者当前的N个状态有关，那么我们就称这个系统具有马尔可夫性。马尔可夫决策过程（Markov Decision Process, MDP）也具有马尔可夫性，并且MDP的状态迁移还与当前采取的动作有关。</p><div align="center">  <img src="/2021/06/29/dou-brother/mdp.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>强化学习从MDP发展而来，包括Environment、Agent、Action、Observation和Reward。Agent在具体环境下基于一定的策略判断后执行动作，然后会得到环境的奖励并迁移到新的状态。Agent判断的策略是基于特定的状态s下，选择未来带来奖励最多的动作a，<strong>特定s和a下的代表未来的奖励称为Q函数</strong>，Q函数通常表示为Q(s, a)。</p><p>贪婪算法与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi><mo>−</mo></mrow><annotation encoding="application/x-tex">\theta-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mord">−</span></span></span></span>贪婪算法、Sarse算法、Q-Learning算法（更新Q值的方法有所不同）、Deep Q Network（可以处理状态空间和动作空间为连续的情况）。DQN的核心思想：</p><ul><li>使用深度学习网络表示Q函数，训练的数据是状态s，训练的标签是状态s对应的每个动作的Q值，即标签是由Q值组成的向量，向量的长度与动作空间的长度相同。</li><li>动作选择的算法使用贪婪算法，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>可以是静态的也可以是随着时间动态变化的。</li><li>Q值的更新与Q Leaning算法相同</li><li>定义一段所谓的记忆体，在记忆体中保存具体某一时刻的当前状态、奖励、动作、迁移到的下一个状态、状态是否结束等信息，定期从记忆体中随机选择固定大小的一段记忆，用于训练深度神经网络。</li></ul><div align="center">  <img src="/2021/06/29/dou-brother/rl.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>Keras-rl统一了智能体类的API，主要包括Fit、Test、compile（主要编译用户自定义的深度神经网络）函数。常用对象有记忆体SequentialMemory和选择策略Policy。</p><p>OpenAI Gym除了自己提供了机器人学任务、桌面游戏之类强化学习开发测试环境，还提供了一个非常简便的开发框架，便于大家开发自己的强化学习算法。这些环境都有一个通用交互界面，使用户能够编写可以应用于许多不同环境的通用算法。OpenAI Gym中包含一些经典的控制问题场景，比如独臂支撑（CartPole）、多连臂（Acrobot）和过山车（MountainCar）。</p><h2 id="三-恶意程序检测"><a class="markdownIt-Anchor" href="#三-恶意程序检测"></a> 三. 恶意程序检测</h2><h3 id="1-检测"><a class="markdownIt-Anchor" href="#1-检测"></a> 1. 检测</h3><p>代码参考了Endgame公司的开源实现（<a href="https://github.com/endgameinc/%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://github.com/endgameinc/）。</a></p><p>PE（Portable Executable）文件，意为可移植的、可执行的文件，常见的EXE、DLL、OCX、SYS、COM都是PE文件，PE文件是微软Windows操作系统上的可执行文件的标准格式。PE文件包括几个比较重要的部分，DOS头、文件头、可选头、数据目录，以及节头和节区。</p><p>特征提取方法总结《Deep Neural Network Based Malware Detection Using Two DimensionalBinary Program Features》。一类是通过PE文件可以直接获取到的特征，比如字节直方图，字节熵直方图和字符串特征等；另一类特征是需要解析PE文件结构，从各个节分析出的特征，比如节头特征，导入和导出表特征，文件头特征等。</p><p>numpy.bincount专门用于以字节为单位统计个数，minlength参数用于指定返回的统计数组的最小长度，不足最小长度的会自动补0。</p><p>单纯统计直方图<strong>非常容易过拟合</strong>，因为字节直方图对于PE文件的二进制特征过于依赖，PE文件增加一个无意义的0字节都会改变直方图。一种常见的处理方式是，增加一个维度的变量，用于统计PE文件的字节总数，同时原有直方图按照字节总数取平均值。</p><h3 id="2-免杀"><a class="markdownIt-Anchor" href="#2-免杀"></a> 2. 免杀</h3><p>LIEF是Library to Instrument Executable Formats的简称，它提供了跨平台解析和修改常见的可执行文件的能力。</p><p>对于依赖文件哈希值的杀毒软件，只需要在PE文件后面追加随机内容即可以绕过检测。为了更加逼真，还需要使用常见的库和导入函数来创建导入表。</p><p>修改已经存在的节的名称也可以迷惑杀毒软件，常见的方式是随机选择已经存在的节，并把节的名称修改为常见的节的名称。</p><p>增加新的节，节的名称既可以参考修改节名称的方式从常见的节名称中随机选择，也可以直接随机生成。</p><p><strong>加壳</strong>是名气最大的免杀方式，最入门级的工具是UPX。UPX的加壳过程可以分为两步：第一步，在PE文件的特定位置增加一段代码A；第二步，将代码段1、2和3无损压缩成代码段B，然后和代码段A一起组成新的程序。其中代码段A的主要功能是解压缩后面的代码段B。</p><p>杀毒软件无法根据签名和debug信息断定程序是否为恶意程序，但是攻击者却可以通过删除debug信息改变文件的哈希值和二进制特征。</p><p>将可选头的交验和设置为空也可以迷惑杀毒软件。</p><h3 id="3-智能提升"><a class="markdownIt-Anchor" href="#3-智能提升"></a> 3. 智能提升</h3><p>软件开发者通常他们会在写完程序后，使用常见的几种杀毒软件检测，如果被检测出来，就尝试使用不同的免杀技术直到杀毒软件无法检测为止。</p><p>2017年7月DEFCON，EndGame公司演示了<strong>使用机器学习创建恶意代码，从而绕过杀毒软件的检测</strong>。整个方案基于OpenAI Gym框架开发，称为Gym-Malware（<a href="https://github.com/endgameinc/gym-malware%EF%BC%89%E3%80%82%E4%BD%BF%E7%94%A8%E4%BA%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%8C%E9%80%9A%E8%BF%87%E8%87%AA%E5%8A%A8%E5%8C%96%E7%9A%84%E6%96%B9%E5%BC%8F%E5%B0%9D%E8%AF%95%E4%B8%8D%E5%90%8C%E5%85%8D%E6%9D%80%E6%96%B9%E6%B3%95%EF%BC%8C%E6%9C%80%E7%BB%88%E5%9C%A8%E4%B8%8E%E6%9D%80%E6%AF%92%E8%BD%AF%E4%BB%B6%E7%9A%84%E5%AF%B9%E6%8A%97%E4%B8%AD%E5%AD%A6%E4%B9%A0%E5%87%BA%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E5%85%8D%E6%9D%80%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6%E3%80%82" target="_blank" rel="noopener">https://github.com/endgameinc/gym-malware）。使用了强化学习的方式，通过自动化的方式尝试不同免杀方法，最终在与杀毒软件的对抗中学习出如何生成免杀恶意软件。</a></p><div align="center">  <img src="/2021/06/29/dou-brother/gymmal.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>PEFeatureExtractor的主要功能就是把PE文件转换成特征向量。Interface模块基于GBDT模型针对PE文件进行检测。MalwareManipulator模块封装了对PE文件的各种免杀操作。DQNAgent具体实现了强化学习算法。MalwareEnv对外主要提供了step和reset两个接口，Init函数主要负责创建MalwareEnv时完成一系列初始化工作，Step函数完成了最重要的动作执行、病毒检测以及反馈状态的功能，Reset函数负责重置环境的状态，并随机从样本中选择一个，转换成特征向量后作为初始状态。</p><h2 id="四-提升waf防护能力"><a class="markdownIt-Anchor" href="#四-提升waf防护能力"></a> 四. 提升WAF防护能力</h2><p>WAF的基本原理是，作为一道墙接受用户对Web服务器的请求，然后转发给后端真实的Web服务器，并将应答内容返回给用户。整个过程中，WAF针对请求和应答内容，按照既定的拦截规则进行过滤。</p><p>使用强化学习，模拟黑客的这一绕过思路，自动化地发现现有WAF的绕过方式，从而不断提升WAF的防护能力。</p><p>以XSS为例，有多种常见攻击方式和相应的检测方法。</p><p>常见的XSS绕过WAF的方式：</p><ul><li>16进制编码，浏览器可以正常解析，但是却可以绕过基于规则的WAF；</li><li>10进制编码</li><li>插入注释、回车、TAB</li><li>大小写混淆</li></ul><p>使用和恶意程序生成同样的框架完成这项工作。主要由DQNAgent, WafEnv_v0,Waf_Check, Xss_Manipulator和Features组成。Features将XSS样本转换成向量，Waf_Check基于规则用于XSS检测，XSS的特征向量作为状态传递。DQNAgent基于当前状态和一定的策略，选择免杀动作。WafEnv_v0根据免杀动作，通过Xss_Manipulator针对XSS样本执行免杀操作，然后使用Features重新计算特征，再使用Waf_Check判断，如果不是XSS，反馈10并结束本轮学习；如果是XSS，反馈0以及新状态给DQNAgent, DQNAgent继续选择下一步免杀操作，如此循环。（感觉整个过程并不是特别像强化学习，其实就是把各种免杀招数都用一用，智能化可能体现在有选择地使用）。</p><div align="center">  <img src="/2021/06/29/dou-brother/xss.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h2 id="五-提升垃圾邮件检测能力"><a class="markdownIt-Anchor" href="#五-提升垃圾邮件检测能力"></a> 五. 提升垃圾邮件检测能力</h2><p>一般的绕过可以通过随机增加TAB、回车、换行符、连字符，大小写混淆，使用错别字等方式实现。</p><p>整个系统的框架和上面相同。SpamEnv_v0类实现了强化学习中环境的主要功能，涉及的主要函数有Step和Reset。Step函数根据输入的动作动作序号，修改当前样本，再检测是否为垃圾邮件；Reset负责重置环境，从训练样本列表中随机选择一个作为当前样本并转换成对应的特征向量，作为初始状态。</p><h2 id="六-生成对抗网络"><a class="markdownIt-Anchor" href="#六-生成对抗网络"></a> 六. 生成对抗网络</h2><p>GAN包括噪音源、Generator和Discriminator，数据集包含真实样本即可。</p><p>常见的噪音就是分布满足<strong>平均分布和正态分布</strong>的随机数。</p><p>DCGAN，使用深度卷积神经网络作为图像识别和生成的工具。</p><p>ACGAN，基于分类优化的GAN（Auxiliary Classifier Generative Adversarial Network），在生成图像和进行图像分类的环节引入了图像内容的标签，所谓图像内容的标签。</p><p>WGAN，彻底解决了GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度，而且不需要精心设计的网络架构，不像DCGAN必须有BatchNormalization（批量正则化）。主要改进点在于：</p><ul><li>生成器和判别器的损失函数中不取log；</li><li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c；</li><li>优化算法使用RMSProp或者SGD；</li></ul><h2 id="七-攻击机器学习模型"><a class="markdownIt-Anchor" href="#七-攻击机器学习模型"></a> 七. 攻击机器学习模型</h2><p>常见图像分类算法有AlexNet、VGG16、ResNet50和InceptionV3。</p><p><strong>基于梯度上升的攻击原理</strong>：把图像分类抽象成一个二分类问题，通过微小改变特征的值，越过分割线，然后获得一个错误的分类结果。假如现在我们手上有一个家猪的照片，我们想伪造成烤面包机的照片，我们可以把损失函数定义为1减去烤面包机标签的概率，那么就可以使用梯度下降算法，或者把损失函数定义为烤面包机标签的概率，使用梯度上升算法，迭代调整图片的内容（也就是多维向量的数值）进行训练。常用方法FGSM（Fast Gradient Sign Method）。</p><p>强化学习模型也很脆弱，比如DQN、TRPO和A3C。《Vulnerability of Deep Reinforcement Learning to Policy InductionAttacks》</p><p>以MNIST手写字体识别为例，攻击CNN、自编码器、VAE等模型。</p><h1 id="ai安全之对抗样本入门"><a class="markdownIt-Anchor" href="#ai安全之对抗样本入门"></a> 《AI安全之对抗样本入门》</h1><p>这本书主要以图像识别、目标检测领域介绍对抗样本，和网络安全基本没有关系了。</p><h2 id="一-背景-2"><a class="markdownIt-Anchor" href="#一-背景-2"></a> 一. 背景</h2><p>百度AI模型安全的开源项目AdvBox（<a href="https://github.com/advboxes/AdvBox%EF%BC%89%EF%BC%8C%E7%B1%BB%E4%BC%BC%E7%9A%84%E6%A1%86%E6%9E%B6%E8%BF%98%E6%9C%89foolbox%E3%80%81adversarial-robustness-toolbox%E5%92%8Ccleverhans%E3%80%82" target="_blank" rel="noopener">https://github.com/advboxes/AdvBox），类似的框架还有foolbox、adversarial-robustness-toolbox和cleverhans。</a></p><p>深度学习的脆弱性主要体现在：偷取模型、数据投毒（攻击者把重点放到了在线学习的场景，比较典型的是推荐系统）、对抗样本（定向攻击和无定向攻击；黑盒、白盒、真实世界/物理攻击Real-world attack/ Physical attack）。</p><ul><li>White-Box Attack，攻击难度最低，前提是能够完整获取模型的结构，并且可以完整控制模型的输入。常见攻击方法如ILCM（最相似迭代）、FGSM（快速梯度）、BIM（基础迭代）、JSMA（显著图攻击）、DeepFool、C/W算法。</li><li>Black-BoxAttack完全把被攻击模型当成一个黑盒，对模型的结构没有了解，只能控制输入，通过比对输入和输出的反馈来进行下一步攻击。常见方法如单像素攻击、本地搜索攻击等。</li><li>Real-World Attack/Physical Attack是这三种攻击中难度最大的，除了不了解模型的结构，甚至对于输入的控制也很弱。</li></ul><p>常见的加固方法包括特征凝结（Feature squeezing）、空间平滑（Spatial smoothing）、标签平滑（Lable smoothing）、对抗训练、虚拟对抗训练、高斯数据增强等。</p><p>范数是一种强化了的距离概念，通常为了提高模型的抗过拟合能力而加入到损失函数中。</p><ul><li>L0范数，并不是一个真正的范数，它主要用于度量向量中非零元素的个数。对抗样本中通常指相对于原始图片修改的像素的个数。</li><li>L1范数，曼哈顿距离、最小绝对误差等，可以度量两个向量之间差异；</li><li>L2范数，欧氏距离就是一种L2范数；</li><li>无穷范数，主要用于度量向量元素的最大值；</li></ul><p>CNN之前图像分类算法依赖于复杂的特征工程，最常用SIFT、HOG、LBP等进行特征提取，常用分类算法是SVM。</p><p>常用CNN模型结构有AlexNet、VGG、ResNet50（残差结构）、InceptionV3（单层使用不同尺度的卷积核，基本单元为Inception）。</p><p>可视化CNN。使用一个卷积核处理图像数据后，卷积核会提取它关注的特征并形成新的图像，该图像也被称为<strong>特征图</strong>。可以基于梯度，<strong>迭代调整输入图像的值，让特征图的值最大化</strong>。当特征图的值达到最大或者迭代求解趋于稳定时，可以认为这时的输入图像就是该卷积核的可视化图像。</p><h2 id="二-图像处理基础"><a class="markdownIt-Anchor" href="#二-图像处理基础"></a> 二. 图像处理基础</h2><p>常用存储格式为BMP（与硬件无关的无损压缩格式）、JPEG（有损压缩）、GIF（基于LZW算法的连续色调无损压缩，压缩率在50%左右，可以存储多幅彩色图像形成动画）和PNG等。</p><p>CWH格式中C代表通道维度，W表示宽，H表示高。像素深度即表达一个像素点需要几位二进制数。</p><p>常见的图像转换包括缩放、旋转、剪切、平移、翻转、亮度与对比度，也叫作图像增强。</p><p>仿射变换（Affine Transformation）是空间直角坐标系的变换，从一个二维坐标变换到另一个二维坐标，比较常用的变换有缩放、旋转、平移、剪切和翻转。仿射变换是一个线性变换，它保持了图像的“平行性”和“平直性”，即图像中原来的直线和平行线，变换后仍然保持原来的直线和平行线。可通过OpenCV中的warpAffine函数实现。</p><p>常见的图像噪声是<strong>高斯噪声和椒盐噪声</strong>。一种是盐噪声（Salt noise），另一种是胡椒噪声（Pepper noise）。盐噪声为白色，胡椒噪声为黑色。可以使用skimage库给图像添加噪声。</p><p>滤波可以去噪，<strong>中值滤波</strong>使用邻域内所有像素的中位数替换中心像素的值，可以在滤除异常值的情况下较好地保留纹理信息。<strong>均值滤波</strong>不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊。均值滤波和中值滤波的效果非常接近。高斯滤波使用邻域内所有像素的加权平均值替换中心像素的值。<strong>高斯滤波</strong>适用于消除高斯噪声，广泛应用于图像处理的减噪过程。<strong>高斯双边滤波</strong>同时考虑空间域和值域，是结合图像的空间邻近度和像素值相似度的一种折中处理，达到保护边缘并去除噪声的目的（保边去噪）。</p><h2 id="三-攻击方法"><a class="markdownIt-Anchor" href="#三-攻击方法"></a> 三. 攻击方法</h2><h3 id="1-白盒"><a class="markdownIt-Anchor" href="#1-白盒"></a> 1. 白盒</h3><p>重点介绍基于梯度的对抗样本生成算法，其中比较基础的关键概念如下：</p><ul><li>Method，攻击算法</li><li>Black/white，是黑盒还是白盒</li><li>Targeted/Non-targeted，定向攻击还是非定向攻击；</li><li>Image-specific/universal，是否具有通用性，前者针对不同图片生成不同对抗样本，后者表示通用对抗扰动；</li><li>Perturbation norm，算法支持的范数</li><li>Learning，算法是基于迭代优化的还是一步操作就可以完成的；</li><li>Strength，算法的攻击强度，即攻击成功率。</li></ul><h4 id="fgmfgsm算法"><a class="markdownIt-Anchor" href="#fgmfgsm算法"></a> FGM/FGSM算法</h4><p>快速梯度算法《Explaining and Harnessing Adversarial Examples》，支持无定向和定向攻击。</p><h4 id="deepfool算法"><a class="markdownIt-Anchor" href="#deepfool算法"></a> DeepFool算法</h4><p>《DeepFool：a simple and accurate method tofool deep neural networks》，通常无定向，相对FGM而言，不用指定学习速率ε，算法本身可以计算出相对FGM更小的扰动来达到攻击目的。</p><h4 id="jsma算法"><a class="markdownIt-Anchor" href="#jsma算法"></a> JSMA算法</h4><p>《The Limitations of Deep Learning in Adversarial Settings》典型的l0范数的白盒定向攻击算法，追求的是尽量减少修改的像素个数。引入了<strong>Saliency Map即显著图</strong>的概念，用以表征输入特征对预测结果的影响程度。</p><h4 id="cw算法"><a class="markdownIt-Anchor" href="#cw算法"></a> CW算法</h4><p>《Towards Evaluating theRobustness of Neural Networks》，攻击能力最强的白盒算法之一，是一种基于优化的算法。同时支持l0、l2和l∞攻击。CW算法的一个核心点就是通过二分查找计算出尽量小的c值。另外一个特色就是对数据截断的处理（Projected gradient descent，即梯度投影下降、Clipped gradient descent，即梯度截断下降）。</p><h3 id="2-黑盒"><a class="markdownIt-Anchor" href="#2-黑盒"></a> 2. 黑盒</h3><p>目前常见的黑盒攻击算法主要分为两类，<strong>一类是基于一定的算法构造输入</strong>，然后根据模型的反馈不断迭代修改输入，比较典型的就是单像素攻击算法和本地搜索攻击算法；<strong>另一类是基于迁移学习的思想</strong>，使用与白盒攻击类似的开源模型，之后用生成的攻击样本进行黑盒攻击。</p><h4 id="单像素攻击算法"><a class="markdownIt-Anchor" href="#单像素攻击算法"></a> 单像素攻击算法</h4><p>《Simple Black-Box AdversarialPerturbations for Deep Networks》</p><p>AdvBox和Foolbox均实现了这个算法。</p><h4 id="本地搜索攻击算法"><a class="markdownIt-Anchor" href="#本地搜索攻击算法"></a> 本地搜索攻击算法</h4><p>当图像较大时，一个像素点的改变很难影响到分类结果。并且随着图像文件的增大，搜索空间也迅速增大，单像素攻击的效率也会快速下降。</p><p>《Simple Black-Box Adversarial Perturbations for Deep Networks》</p><p>单像素攻击算法没有很好地利用模型的反馈信息去优化扰动，很大程度上依赖随机选择像素和迭代调整像素点的值。本地搜索攻击算法的主要改进点就是<strong>根据模型的反馈信息去选择扰动的点</strong>，并随机选择对分类结果影响大的点周围的点，进一步进行选择。</p><h4 id="迁移学习攻击"><a class="markdownIt-Anchor" href="#迁移学习攻击"></a> 迁移学习攻击</h4><p>基本思想是，结构类似的深度学习网络，在面对相同的对抗样本的攻击时，具有类似的表现。</p><p>对抗样本在不同算法之间有一定的迁移性，或者称<strong>对抗样本的传递性</strong>。虽然基于对抗样本的传递性可以构造出对抗样本，但是成功率并不能令人满意。基于此，Yanpei Liu、Xinyun Chen和Dawn Song等人提出了<strong>对抗样本领域的集成学习的方法</strong>，以多个深度神经网络模型为基础构造对抗样本。</p><h4 id="universal对抗样本"><a class="markdownIt-Anchor" href="#universal对抗样本"></a> Universal对抗样本</h4><p>通用对抗样本事实上是一类精心构造的扰动，具有通用性，而且可以足够小，所以也被称为通用对抗扰动。</p><p>通用对抗扰动的获取是一个迭代求解的过程，通常需要准备一份测试数据，该测试数据需要覆盖全部分类类型，并且尽可能使各个分类的数量都比较均匀。</p><h2 id="四-目标检测领域"><a class="markdownIt-Anchor" href="#四-目标检测领域"></a> 四. 目标检测领域</h2><p>在无人驾驶领域的应用场景比如车道偏离预警、前方防碰撞预警、交通标志识别、行人防碰撞、疲劳驾驶检测预警、自动泊车等。</p><p>在智能安防领域的场景比如，人脸检索、行为识别。</p><p>边缘检测（soble，拉普拉斯，Canny，），直线检测、圆形检测等。</p><p>目标检测领域常用概念如下：</p><ul><li>Region Proposals，推荐框，在图片中识别出物体的轮廓，并用一个长方形表示物体的范围；</li><li>Ground Truth，标定过的真实数据，即人工标记的物体的真实范围；</li><li>IoU，系统预测出来的与原来图片中标记的框的重合程度；</li></ul><p>Faster RCNN是经典的目标检测算法，由RCNN、Fast RCNN演变而来。RCNN最花费时间的阶段就是要针对每个Region Proposals经过CNN计算，FastRCNN创新地把CNN阶段提前，整张图片仅进行一次CNN计算，然后再计算Region Proposals。</p><p>YOLO算法全称You Only look Once Detector，是一个可以一次性预测多个检测框位置和类别的卷积神经网络，能够实现端到端的目标检测和识别，其最大的优势就是速度快。YOLO有众多版本，其中最著名是<strong>YOLO V3</strong>。</p><p>SSD算法全称Single Shot MultiBox Detector，是主流检测框架之一，相比Faster RCNN有明显的速度优势，相比YOLO又有明显的mAP优势。主要特点如下：</p><ul><li>从YOLO中继承了将检测问题转化为回归问题的思路，同时一次即可完成网络训练；</li><li>基于Faster RCNN中的anchor，提出相似的prior box；</li><li>加入基于特征金字塔的检测方式；</li></ul><h2 id="五-常见防御算法"><a class="markdownIt-Anchor" href="#五-常见防御算法"></a> 五. 常见防御算法</h2><p>图片旋转、滤波器、亮度和对比度、噪声等都会对模型鲁棒性产生影响。</p><p>常见的防御措施包括图像预处理，在具体的参数，比如旋转的角度、滤波器的设置等要根据实际的模型和数据集进行优化调整。</p><p>对抗训练，可以首先使用常见的对抗样本算法，针对被攻击模型生成大量的对抗样本，然后把对抗样本和原始数据放到模型里重新训练，进行有监督学习，这样就获得了加固后的模型。</p><p>高斯数据增强，算法认为绝大多数的对抗样本相当于在原始图像上叠加了噪声，理想情况下可以用高斯噪声模拟这种噪声。</p><p>自编码器去噪，输入层和输出层分别代表神经网络的输入层和输出层，隐藏层承担编码器和解码器的工作，编码的过程就是从高维度的输入层转化到低维度的隐藏层的过程，反之，解码过程就是低维度的隐藏层到高维度的输出层的转化过程，可见自编码器是个有损转化的过程，通过对比输入和输出的差别来定义损失函数。</p><p>训练的过程不需要对数据进行标记。</p><p>自编码器通过学习数据集自身的特征，在一定程度上能过滤掉叠加到原始数据上的不规则的噪音。</p><p>大多数白盒攻击通过计算模型的梯度来运行，因此如果不能通过计算得到有效的梯度，那么攻击就会失效。<strong>梯度掩蔽</strong>通过在某种程度上改变模型，使其不可微分，或者使其在大多数情况下具有零梯度，或者梯度点远离决策边界。此后研究者应用新开发的攻击技术，解决了梯度掩蔽问题。（<a href="https://github.com/anishathalye/obfuscated-gradients%EF%BC%89" target="_blank" rel="noopener">https://github.com/anishathalye/obfuscated-gradients）</a></p><h2 id="六-常见工具箱"><a class="markdownIt-Anchor" href="#六-常见工具箱"></a> 六. 常见工具箱</h2><p>包括AdvBox、ART、FoolBox和Cleverhans等。</p><p>对抗样本质量衡量指标包括攻击成功率、扰动的L0、L2、Linf范数。</p><ul><li>AdvBox是一款由百度安全实验室研发，在百度大范围使用的AI模型安全工具箱，目前原生支持PaddlePaddle、PyTorch、Caffe2、MXNet、Keras以及TensorFlow平台，同时支持GraphPipe，屏蔽了底层使用的深度学习平台。</li><li>ART（Adversarial Robustness Toolbox）是IBM研究团队开源的用于检测模型及对抗攻击的工具箱。</li><li>FoolBox由Bethge Lab[插图]的三名德国科学家开发，能够帮助用户在解析“黑匣子”时更轻松地构建起攻击模型，并且在名人面部识别与高知名度Logo识别方面成功骗过美国热门的图片识别工具。</li><li>Cleverhans是用于对机器学习模型进行对抗性攻击、防御和基准测试的Python库。</li><li>robust-ml[插图]是一个轻量级的攻防对抗环境，可以很方便地在ImageNet 2012这样的大型数据集上验证白盒攻击或者防御算法的有效性。</li></ul><p>GraphPipe是甲骨文开源的通用深度学习模型部署框架。官方对GraphPipe的定义为，这是一种协议和软件集合，旨在简化机器学习模型部署并将其与特定于框架的模型实现分离。</p><p>ONNX（Open Neural Network Exchange）[插图]，即开放的神经网络切换。顾名思义，该项目的目的是让不同的神经网络开发框架做到互通互用。开发者能更方便地在不同框架间切换，为不同任务选择最优工具。</p><p>NIPS对抗攻击防御是由Ian Goodfellow牵头组织的对抗样本领域顶级竞赛，包括无目标对抗攻击、有目标对抗攻击和针对对抗攻击的防御。</p><h1 id="其它记录"><a class="markdownIt-Anchor" href="#其它记录"></a> 其它记录</h1><p>2015年，微软在Kaggle上发起了一个恶意代码分类比赛，提供超过500G的原始数据，冠军队伍选择了三个黄金特征：恶意代码图像（二进制文件灰度图）、OpCode n-gram和Headers个数，其它包括ByteCode n-gram，指令频数等，机器学习选择了随机森林算法并用到XGBoost和pypy加快训练速度。</p><p>百度王磊团队，《AI Based Antivirus∶ Can Alphaav Win The Battle in Which Man HasFailed?》对APK提取三大类特征：</p><ul><li>结构化特征，APK申请的权限的个数，资源文件中包含的图像文件个数和参数大于20的函数的个数等；</li><li>统计类特征</li><li>经验特征，资源文件中是否包含可执行文件，assets文件夹中是否包含APK文件等；</li></ul><h3 id="一些文件操作"><a class="markdownIt-Anchor" href="#一些文件操作"></a> 一些文件操作</h3><pre><code class="hljs python"><span class="hljs-comment"># 递归遍历目录下文件的函数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dirlist</span><span class="hljs-params">(path, allfile)</span>:</span>    filelist = os.listdir(path)    <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filelist:        filepath = os.path.join(path, filename)        <span class="hljs-keyword">if</span> os.path.isdir(filepath):            dirlist(filepath, allfile)        <span class="hljs-keyword">else</span>:            allfile.append(filepath)    <span class="hljs-keyword">return</span> allfile<span class="hljs-comment"># 按后缀名筛选</span>file.endswith(<span class="hljs-string">'.php'</span>)<span class="hljs-comment">#pickle.dump保存、加载中间结果</span>pickle.dump(char_idx, open(cahr_idx_file,<span class="hljs-string">'wb'</span>))<span class="hljs-keyword">if</span> os.path.isfile(char_idx_file):    print(<span class="hljs-string">'Loading previsou file'</span>)    char_idx = pickle.load(open(char_idx_file, <span class="hljs-string">'rb'</span>))    <span class="hljs-comment"># 统计个数</span>np.bincount(np.array([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">7</span>]))</code></pre><p>urlparse.parse_qsl解析url请求切割参数时，遇到’; ‘会截断，导致获取的参数值缺失’; '后面的内容，这是个大坑。</p><p>tensorflow提供API支持char_idx结构定义的转换关系。temperature可以理解为新颖程度，越小生成序列就越接近于原有序列。</p><p>Opcode是计算机指令中的一部分，用于指定要执行的操作，指令的格式和规范由处理器的指令规范指定。通常opcode还有另一种称谓——字节码（bytecodes）。opcode缓存技术[插图]可以有效减少不必要的编译步骤，减少CPU和内存的消耗。</p><p>Scikit-Learn的GridSearchCV模块，能够在指定的范围内自动搜索具有不同超参数的不同模型组合，大大提高了我们的参数优化效率。</p><p>Python的<strong>pickle模块</strong>实现了基本的数据序列和反序列化。通过pickle模块的序列化操作，我们能够将程序中运行的对象信息保存到文件中永久存储，通过pickle模块的反序列化操作，我们能够从文件中创建上一次程序保存的对象。通常持久化的文件后缀为．pkl或者．pickle，使用joblib可以很方便地把模型保存成文件或者从文件中加载模型。</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>网络安全</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网络安全——应用技术与工程实践</title>
    <link href="/2021/06/22/security-base/"/>
    <url>/2021/06/22/security-base/</url>
    
    <content type="html"><![CDATA[<p>北京理工大学闫怀志老师的在线课程（<a href="https://www.xuetangx.com/course/bit08091002817/5884048?channel=learn_title%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://www.xuetangx.com/course/bit08091002817/5884048?channel=learn_title）。</a></p><p>比较笼统浅显，对于入门和掌握全局概念来说非常推荐。</p><h2 id="一背景"><a class="markdownIt-Anchor" href="#一背景"></a> 一.背景</h2><p>信息是用来消除不确定性的东西（香农），是人们在适应外部世界并使这种适应反作用于外部世界的过程中同外部世界进行互相交换的内容和名称（控制论创始人维纳）。现代科学认为信息是对世界各种事物的运动状态和变化的反映。</p><p>信息具有可识别性、可传载性、可共享性和可度量性。</p><p>信息系统的<strong>计算模式</strong>，从集中计算模式、分布式计算模式再到云计算模式，计算架构包括单机、C/S、B/S、C/S和B/S混合、P2P、多层、SOA架构、微服务架构等。</p><p><strong>安全领域CIKW模型</strong>，底层为安全相关数据（实体环境采集的原始素材），到安全相关信息（加工处理后的逻辑数据），进一步到安全相关知识（提炼信息之间的关联），最后到智能（前瞻性预测与决策支撑）。</p><p>网络空间（ITU，国际电信联盟）定义，是由计算机、计算机系统网络及其软件支持、计算机数据、内容数据、流量数据以及用户等上述全部或部分要素创建或组成的物理或非物理领域。是陆海空天之后的第五维空间。</p><p>以<strong>安全属性</strong>的不变应信息系统形态、处理技术和<strong>安全技术</strong>的万变。</p><p>信息安全属性包括：保密性（C）、完整性（I）和可用性（A）。扩展的安全属性还包括，不可否认性（non-repudiation）、可控性（Controllability）、可追溯性等。</p><p>网络空间安全领域牵涉到自然科学（数学、通信、信息、计算机等）和社会科学（法律、心理学、教育和管理等）两个方面。</p><p>关于网络空间安全的八个基本观点：相对安全、适度安全、木桶理论、安全困境（保密性vs可用性）、墨菲定律、蝴蝶效应、冰山原理和剃刀定律。</p><h2 id="二-安全技术体系"><a class="markdownIt-Anchor" href="#二-安全技术体系"></a> 二. 安全技术体系</h2><p>内部是信息系统的安全构建，包括网络、应用系统及实现，外部包括防火墙、入侵检测、态势感知等。</p><p>网络安全滑动标尺模型，由基础架构安全能力、被动防御能力、主动防御能力、智能分析能力到攻击反制能力。</p><p>安全三要素包括，资产（识别）、脆弱性（利用）和威胁（应对）。</p><h3 id="1-基本思想"><a class="markdownIt-Anchor" href="#1-基本思想"></a> 1. 基本思想</h3><p>基本思想，<strong>网络安全系统是一个复杂系统，需要采用系统工程方法</strong>。复杂性主要体现在层次性、不确定性、开放性、非线性、系统规模、主动性、智能性以及动态性等。当前发展体现在由<strong>基元性转向组织性</strong>（考虑相互的组织与协同）、线性转向非线性（关注关联、交互、涌现等问题）、简单性转向复杂性。</p><p>网络安全系统是一种复杂系统，网络安全风险是一种客观存在，需要采用系统思想来考察，涉及系统论、信息论、控制论、耗散结构理论、协同论与突变论等。系统工程方法包括霍尔三维（时间维、逻辑维、知识维）结构方法论、切克兰德方法论、从定性到定量的系统综合方法、基于模型的系统工程、体系工程等。</p><p>经典系统科学理论（老三论）包括系统论、信息论和控制论。系统论可以从宏观、中观、微观三个层面指导网络空间安全体系的构建。信息论的应用可包括密码学、信息隐藏、隐私保护等。控制论则可以指导安全控制、攻防对抗、防御构建等。</p><p>新兴系统科学理论包括耗散结构理论、协同论和突变论。耗散结构理论主要解释复杂系统的自组织运动规律，比如大规模网络故障预测与控制，舆情管理等；协同论研究相互协调、共同作用，构建度量信息安全技术、管理各因素（子系统）之间协同程度的评价指标和评价模型；突变论是对量化和质变规律的深化，如网络安全突发事件的机理分析及应对等。</p><h3 id="2-滑动标尺模型"><a class="markdownIt-Anchor" href="#2-滑动标尺模型"></a> 2. 滑动标尺模型</h3><p>对于网络安全防御体系，国内提出P2DR、OODA、PDCA等动态循环模型。美国SNAS提出网络安全滑动标尺模型获得广泛应用。此模型将安全防御分为逻辑递进的五个阶段：基础架构安全、被动防御、主动防御、智能分析以及反制攻击。</p><div align="center">  <img src="/2021/06/22/security-base/slide.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="3-法律法规"><a class="markdownIt-Anchor" href="#3-法律法规"></a> 3. 法律法规</h3><div align="center">  <img src="/2021/06/22/security-base/law.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="三-基础架构安全概述"><a class="markdownIt-Anchor" href="#三-基础架构安全概述"></a> 三. 基础架构安全概述</h2><p>网络结构需要有合理的安全域划分，软件采用合理架构和技术体系。信息系统的实现可能会引入缺陷和漏洞，在基础实现层面应该尽力避免。</p><div align="center">  <img src="/2021/06/22/security-base/base.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="1-物理与环境安全"><a class="markdownIt-Anchor" href="#1-物理与环境安全"></a> 1. 物理与环境安全</h3><p>物理位置选择，物理访问控制，防盗窃和防破坏，防雷击，防火，防水和防潮，防静电，温湿度控制，电力供应和电磁防护。</p><p><strong>扩展要求</strong>：针对云计算安全、移动互联安全、物联网安全、工业控制系统安全等网络空间信息系统新形态新应用有特殊保护要求。</p><p>云计算安全，保证云计算基础设施、云服务客户数据、用户个人信息等存储于中国境内；</p><p>移动互联安全，选择合理位置，避免过度覆盖和电磁干扰；</p><p>物联网安全，感知节点设备物理环境不对其造成物理损害，能正确反映环境状态，不对其正常工作造成影响。关键网关节点、感知节点有可供长时间工作的电力供应。</p><p>工业控制安全，室外防火、透风、防盗、防雨、散热；远离强电磁干扰，远离强热源等；</p><h3 id="2-网络与主机安全"><a class="markdownIt-Anchor" href="#2-网络与主机安全"></a> 2. 网络与主机安全</h3><p><strong>网络安全</strong>重点是针对通信网络的安全控制，对象是广域网、城域网和局域网等，涉及网络设备的业务处理能力、网络带宽、网络区域划分及通信线路等。</p><div align="center">  <img src="/2021/06/22/security-base/device.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>第一，合理选用网络组建设备。</p><p>第二，合理划分网络区域，实施不同安全策略。</p><p>第三，规划IP地址分配策略、路由、交换策略，可根据情况选用静态分配地址、动态分配地址、NAT措施等。</p><p>第四，实现网络线路和设备的冗余配置，确保系统可用性。</p><p>第五，网络边界部署安全设备，规划具体位置和部署措施，部署网络安全审计系统。</p><p>第六，设计远程安全访问系统，部署IPsec安全通信设备，部署SSL，VPN安全设备</p><p>扩展要求：</p><p>云计算要实现客户虚拟网络的隔离，为客户提供定制、自主式的安全能力。</p><p>工业控制系统内部根据业务特点划分为不同安全域并采取隔离，涉及<strong>实时</strong>传输和数据传输的，应独立组网并与其它网络实现物理安全隔离。</p><p><strong>主机安全</strong>主要是操作系统安全和数据库安全。</p><p>操作系统安全涉及自身安全机制、安全配置及安全使用等。考虑三个方面：1）资源利用、数据传输和客体重用；2）密码支持、安全功能保护、访问控制、身份鉴别、审计、数据完整性等；3）安全管理、生存周期支持、配置管理、脆弱性（漏洞）管理等。</p><p>数据库安全包括自身安全和数据安全。保护数据库系统、服务器和数据库中的数据、应用、存储以及相关网络连接。防止数据库系统及数据遭到泄露、篡改或破坏。</p><h3 id="3-加密认证技术"><a class="markdownIt-Anchor" href="#3-加密认证技术"></a> 3. 加密认证技术</h3><h4 id="基础"><a class="markdownIt-Anchor" href="#基础"></a> 基础</h4><p>涉及密码编码学与密码分析学。</p><p>古代加密方法阶段，人工方式加密，比如“藏头诗”。</p><p>古典密码阶段，纯机械或者电子机械方式加密，比如单标代替密码、多表代替密码、转轮密码等。</p><p>近现代密码阶段，采用计算机等先进计算手段作为加密工具。1949年香农发表《保密系统通信原理》，1976年发表《密码学的新方向》。</p><div align="center">  <img src="/2021/06/22/security-base/encrypted.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/clas.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>主要用到数学（概率论、统计、数论、有限域理论）、信息论（自信息、条件自信息、互信息、熵、条件熵、联合熵）、计算复杂性理论（时间复杂性、空间复杂性、复杂性渐进表示、NP难、NPC等）等。</p><p>设计准则包括，计算安全性（原理上可通过计算破解）、可证明安全性（理论证明算法安全性）、无条件安全性（原理上不可破译，基本只有一次一密能达到）。</p><p>良好的密码系统有三个条件：1）给定加密算法、明文、秘钥，容易得到密文，反之亦然；2）解密算法、解密秘钥未知，不能推明文；3）密文不应使别人看起来异样。</p><p>柯克霍夫斯密码设计原则：1）易于计算；2）难于破解；3）一切秘密寓于秘钥；4）整体安全原则。</p><div align="center">  <img src="/2021/06/22/security-base/save.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="对称密码-vs-非对称密码"><a class="markdownIt-Anchor" href="#对称密码-vs-非对称密码"></a> 对称密码 VS 非对称密码</h4><p>对称，即加密解密秘钥相同，或者二者可以简单推算导出。</p><p>对称密码中，秘钥的保密程度决定了体制的安全性。密钥管理的高难度和复杂性。</p><p>对称密码包括<strong>序列密码（流密码）<strong>和</strong>分组密码（块密码）</strong>。</p><p>序列密码起源于香农证明的绝对安全的一次一密，对明文逐字符或逐位加密，加密变换随时间而变。算法要素包括明文、密文、密钥（种子密钥与密钥流）、密钥流字母表、加密算法、解密算法。典型算法有RC4（字节流方式）和A5（密钥流位）。</p><p>优点在于处理速度快、误差传播少，缺点在于扩散（扰乱及混淆）不足，对插入及修改不敏感。</p><div align="center">  <img src="/2021/06/22/security-base/seq.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>分组密码采用软件加密，避免了对硬件的依赖。一次加密明文中的一个组（通常为64或128），加密输出的每一位数字与一组长为m的明文数字有关。算法要素包括明文、密文、密钥、加密算法、解密算法。分组密码包括DES（传统换位和置换方法，64位）、3DES（提升安全性做了扩展，但效率极低）和<strong>AES（代换和混淆方法，安全性高、计算效率高、实现性好、灵活性强，是目前常用的对称加密算法）</strong>。</p><p>优点在于扩散性好、对插入敏感，缺点是处理速度慢、误差易传播。</p><div align="center">  <img src="/2021/06/22/security-base/group.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/work.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>非对称密码体制包括公钥和私钥，也叫作公钥密钥体制，起源于1976年《密码学新方向》。公钥加密的数据只有私钥可解，反之亦然。极大方便了密钥管理。常用算法分为大整数分解类（RSA）、离散对数问题类（Diffie-Hellman）和椭圆曲线类（ElGamal）。</p><p>通过<strong>单项函数、单向陷门函数</strong>两类正向易于计算，反向难以计算的函数实现公钥密码构造。</p><blockquote><p>单向函数：难以根据输出推算输入，即正向计算容易但求逆计算不可行。</p><p>单向陷门函数：如果给定某些额外数据（陷门）容易计算单向函数的逆函数，陷门为解密秘钥。</p></blockquote><p>日常对称密码和非对称密码会<strong>组合使用</strong>，实现加密强度、运算效率和密钥分配管理的平衡。一般采用对称密码进行数据加密，<strong>用非对称密码对对称密码的加密秘钥加密</strong>。核心、关键的机密数据依然用非对称加密。</p><h4 id="摘要算法"><a class="markdownIt-Anchor" href="#摘要算法"></a> 摘要算法</h4><p>将任意长的输入消息串变换为唯一的固定长度的输出，实现<strong>唯一认证标识</strong>，也称为无秘钥加密算法、哈希算法。经常与对称密码、非对称密码组合使用，构成完整的加密认证体系。</p><div align="center">  <img src="/2021/06/22/security-base/hash.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常用MD系列、SHA系列。</p><p>MD5概要：输入是任意长度输出128位哈希。2004年王小云找出了碰撞，网上有MD5密文与明文的对照库，几乎已经淘汰。</p><p>SHA算法：安全哈希算法，已经逐步替代MD5，2004年王小云找到了SHA-1的碰撞，现在建议安全级别高的选择SHA-2。</p><p>在认证领域的具体应用有<strong>数字签名</strong>。附加在数据单元的一些数据，或者是对数据单元的密码变换，用于鉴定签名人身份及数据内容认可，防止源点或重点抵赖或欺骗。常用哈希算法和非对称加密。</p><ul><li>使用哈希算法，首先计算发送文件的哈希值，用私钥对该摘要进行签名，形成发送方数字签名。</li><li>使用非对称加密算法，首先将明文用自己的私钥加密得到数字签名，再将其和验证原文一起发送。</li></ul><div align="center">  <img src="/2021/06/22/security-base/digit.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/digit_p.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="密钥管理"><a class="markdownIt-Anchor" href="#密钥管理"></a> 密钥管理</h4><div align="center">  <img src="/2021/06/22/security-base/mima.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>公钥安全基础设施（PKI）</strong>，利用公钥技术实施和提供安全服务的普适性安全基础设施，是管理密钥和证书的系统或平台，在开放环境中为开放性业务提供公钥加密或数字签名。组成部分包括：公钥密码系统、数字证书、认证中心（CA）以及相关公钥的安全策略。</p><div align="center">  <img src="/2021/06/22/security-base/pki.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ul><li>认证中心：核心执行机构，具有权威性（国家批准）、可信任性（高度安全）、公正性（第三方机构），负责所有证书的签发、注销以及证书注销列表的签发等。</li><li>注册中心：证书注册审批机构，负责证书申请者的信息录入、审核以及证书发放等。发放的证书可以存储在U盘、硬盘中。</li><li>密钥管理中心：CA密钥是整个PKI的核心机密，也管理由客户机生成的用户密钥。</li><li>在线证书状态查询服务：发布用户证书和相关黑名单的信息。</li></ul><p>数字证书用于保证公钥的真实性和有效性，是经过CA数字签名的包含公钥拥有者信息以及公钥的权威性电子文档。由权威公正的第三方CA签发，具有一定有效期，证明某个实体身份及其公钥的匹配绑定关系和合法性。当前常用X.509标准，包括基础版和扩展版。</p><div align="center">  <img src="/2021/06/22/security-base/ca.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>PKI中以分层结构信任即严格层次模型最为常用。</p><div align="center">  <img src="/2021/06/22/security-base/liang.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/china.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>### 4. 协议安全与安全协议<p>TCP/IP协议族面临的安全威胁举例：</p><p>1）明文传输；2）网络窥探；3）IP地址欺骗；4）路由攻击；5）IP隧道攻击；6）网络控制报文协议攻击；7）IP层拒绝服务型攻击；8）IP栈攻击；9）IP地址鉴别攻击；10）TCP序列号攻击。</p><p>网络安全协议基于密码学，用于保障信息安全，作用于TCP/IP下的安全协议或套件。以OSI网络层次及TCP/IP协议族为基础，通过加密、认证及相关协议进行密码学安全加固。</p><p>常用的网络安全协议举例（八种）：</p><p>1）L2TP，链路层扩展；2）IPsec，IP层安全；3）SSL/TLS，传输层安全；4）SSH，会话安全；5）Socks，代理安全；6）Kerberos，认证协议；7）PGP，应用安全；8）HTTPS，应用安全。</p><h4 id="l2tp链路层扩展第二层隧道协议"><a class="markdownIt-Anchor" href="#l2tp链路层扩展第二层隧道协议"></a> L2TP（链路层扩展）第二层隧道协议</h4><p>虚拟隧道协议，常用于VPN，支持IP、ATM、帧中继等网络类型。不提供加密和可靠性验证，通常需要与安全协议（特别是IPSec）搭配使用，实现数据加密传输。</p><h4 id="ipsecip层安全"><a class="markdownIt-Anchor" href="#ipsecip层安全"></a> IPsec（IP层安全）</h4><p>对IP协议的分组进行加密和认证，包括认证头（AH）、封装安全载荷（ESP）、安全关联（SA），提供不可否认性、数据完整性以及不可否认性等。</p><h4 id="ssltls传输层安全"><a class="markdownIt-Anchor" href="#ssltls传输层安全"></a> SSL/TLS（传输层安全）</h4><p>SSL协议位于TCP/IP协议与各种应用层之间，为高层协议提供数据封装、压缩、加密等基本功能，提供保密性和数据完整性。</p><h4 id="ssh协议会话安全"><a class="markdownIt-Anchor" href="#ssh协议会话安全"></a> SSH协议（会话安全）</h4><p>安全外壳协议，建立在应用层基础上，提供基于口令和基于密钥的两种安全验证。</p><h4 id="socks代理安全"><a class="markdownIt-Anchor" href="#socks代理安全"></a> Socks（代理安全）</h4><p>工作在会话层，使用UDP传输数据，提供一个通用框架使FTP、Telnet、SMTP等协议安全透明地穿过防火墙。只是简单地传递数据包，不关心是何种应用协议。</p><h4 id="kerberos认证协议"><a class="markdownIt-Anchor" href="#kerberos认证协议"></a> Kerberos（认证协议）</h4><p>通过密钥系统为客户机，服务器应用程序提供认证服务。基于可信第三方密钥分配中心，由两个独立的逻辑部分组成，即Kerberos认证服务器（AS）和票据授权服务器（TGS）。</p><h4 id="pgp应用安全"><a class="markdownIt-Anchor" href="#pgp应用安全"></a> PGP（应用安全）</h4><p>优良保密协议（Pretty Good Privacy），用于消息加密、验证的应用程序，采用RSA加密、IDEA散列算法验证，由一系列散列、数据压缩、对称秘钥加密，以及公钥加密的算法组成，常用于电子邮件加密，提供数据加密和数字签名服务。</p><h4 id="https应用安全"><a class="markdownIt-Anchor" href="#https应用安全"></a> HTTPS（应用安全）</h4><p>HTTP + 加密 + 认证 + 完整性保护 = HTTPS。</p><p>是HTTP先和SSL协议通信，SSL再和TCP通信，可解决明文传输、认证以及数据完整性问题。</p><h3 id="5-应用软件安全实现"><a class="markdownIt-Anchor" href="#5-应用软件安全实现"></a> 5. 应用软件安全实现</h3><p>应用软件安全需求获取；</p><p>应用软件安全设计：MVC、微内核架构、管道-过滤器架构、SOA架构、巨石型架构、微服务架构等；</p><p>应用软件安全编码：安全编码八大原则、安全编码标准与指南；</p><p>应用软件安全测试：安全功能测试、安全漏洞测试；正向测试（静态分析、基于模型、模糊测试、基于风险、基于故障树、渗透测试、故障注入等）、逆向测试（依据已知软件缺陷空间，建立威胁模型，寻找潜在入侵点进行测试）；</p><h3 id="6-备份与灾难恢复技术"><a class="markdownIt-Anchor" href="#6-备份与灾难恢复技术"></a> 6. 备份与灾难恢复技术</h3><p>数据量爆炸式增长，访问方式和访问时间要求高，异构数据（结构化、半结构化、非结构化）大量存在，云计算、物联网、移动通信、工业互联网等领域新问题。</p><p>直接连接存储（DAS）、网络接入存储（NAS）、存储区域网络（SAN）。</p><p>虚拟化存储，包括带内虚拟化、带外虚拟化；主机虚拟化、子系统虚拟化、网络级虚拟化。</p><div align="center">  <img src="/2021/06/22/security-base/vistual.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>数据备份策略：完全备份、差异备份、增量备份。实现方式包括Host-based、LAN-based、SAN LAN-Free、SAN Server-Free。</p><div align="center">  <img src="/2021/06/22/security-base/drp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="四-被动防御技术体系"><a class="markdownIt-Anchor" href="#四-被动防御技术体系"></a> 四. 被动防御技术体系</h2><p>依赖于专家经验，通过预先设计的规则对已知攻击手段和威胁方式进行防御。</p><div align="center">  <img src="/2021/06/22/security-base/negative.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>技术局限性：1）取决于威胁特征，包括来源、特征、途径、行为等先验特征的精确表达；2）非对称和不确定背景下，攻击行为感知困难。</p><h3 id="1防火墙技术"><a class="markdownIt-Anchor" href="#1防火墙技术"></a> 1.防火墙技术</h3><p>防火墙是网络访问控制机制的一种具体实现，部署在不同网络或不同安全域之间，实现不同信任域之间的安全隔离。</p><p><strong>传统防火墙</strong>基本假设：部署于被保护信息系统边界的唯一信息交互通道，通过的信息需要防护策略明确授权，软硬件实现具有高安全性和高可靠性。视内网为可信区，外网为不可信区域，中间为隔离区。</p><div align="center">  <img src="/2021/06/22/security-base/shixian.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>1）过滤机制：</p><p><strong>包过滤机制</strong>，作用于IP层和传输层，与应用层无关，主要检查IP地址、端口、协议类型、IP选项、TCP选项以及数据包流向等；具体规则包括拒绝/允许与某主机或某网段的所有连接、指定端口的连接等。优点为易于实现与部署、处理速度快、效率高，尤其适合小规模网络应用。缺点是无法过滤数据包内容，不支持用户认证、大规模网络的规则过滤表配置困难。</p><p><strong>应用代理过滤机制</strong>，代理设备Proxy实现外网和内网的隔离，应用于应用层，类似一个中间人。支持用户级身份认证、日志记录功能强大。但额外增加的连接处理易导致性能瓶颈、应用层协议总量受限、无法改进底层协议安全性。</p><div align="center">  <img src="/2021/06/22/security-base/app.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>状态检测过滤机制</strong>，基于会话机制，将属于同一连接的所有包视为一个整体。其检测规则包括多种连接状态信息，并结合通信数据、状态信息及关联、安全策略、应用识别知识等。工作于链路层和网络层之间，从底层捕获信息从高层分析，层次高于传统包过滤但低于应用代理。所以网络层和传输层的控制能力强，应用协议适用性强、配置方便、效率高，但无法彻底识别数据包中的恶意内容等。</p><p>2）软硬件实现方式：</p><p><strong>软件防火墙</strong>，运行于操作系统之上而不依赖于硬件。扩充空间大、配置灵活，但需要占用驻留主机资源，存在性能的双向影响。多用于安全性要求不高的单机系统。</p><p><strong>软硬件结合实现</strong>，安装在专作防火墙的主机之上。</p><p><strong>硬件实现</strong>，软件运算硬件化，将主要的运算程序通过X86、专用集成电路、网络处理器、可编程门阵列等专用硬件架构来实现。采用专用硬件加速，是中高端防火墙的首选。功能完善、效率高、性能稳定，但配置专业性强，价格比较高。</p><p><strong>虚拟防火墙</strong>，将物理实体防火墙划分为多个防火墙，每个虚拟防火墙有独立系统资源、安全策略，可以部署于核心交换机和主要服务器群处、物理网络和虚拟网络之间或两个虚拟网络之间。</p><p>防火墙性能指标主要分为四大类：吞吐量、时延、新建连接速率和并发连接数。</p><div align="center">  <img src="/2021/06/22/security-base/firewall.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ol start="3"><li>部署方式：</li></ol><p>需要考虑具体网络应用环境和性能指标的综合平衡。</p><p><strong>网络式防火墙</strong>，双穴主机（一个主机具有两个地址）网关型部署、单穴堡垒主机型部署(屏蔽主机网关型)、双穴堡垒主机型、屏蔽子网型。</p><p><strong>分布式防火墙</strong>，整体安全策略由安全策略管理服务器负责定义，并由分布于网络中的各防火墙断点执行。分为实体部署方式和虚拟化部署方式。</p><h4 id="web应用防火墙waf技术"><a class="markdownIt-Anchor" href="#web应用防火墙waf技术"></a> <strong>Web应用防火墙（WAF）技术</strong></h4><p>主要应对Web2.0架构中的HTTP/HTTPS应用的安全威胁，比如XSS（跨站脚本攻击）、SQL注入、DDos、Cookie篡改、网页挂马、网页篡改、敏感信息泄露等。</p><p>部署在Web服务器和客户端之间，采用在线（串行部署，采用透明代理、反向代理、路由代理等模式）或离线模式（旁路部署，仅检测而无法阻断），此外还有云WAF。</p><h4 id="工业防火墙"><a class="markdownIt-Anchor" href="#工业防火墙"></a> 工业防火墙</h4><p>特殊要求：机械侵入防护能力、气候适应、电磁兼容；工业通信协议的脆弱性及其解析与过滤，比如工业Ethernet、RS232/422/485、IEC104、OPC等；实时性（硬件加速）与可靠性（软件旁路保护、双机热备、电源冗余）保障。</p><h4 id="攻击与防范技术"><a class="markdownIt-Anchor" href="#攻击与防范技术"></a> 攻击与防范技术</h4><p>通常采用多层次安全性机制保护防火墙，硬件层面包括ASIC、FPGA、NP或多核架构；软件层面采用分权分域的权限管理，采用最小特权策略，细粒度访问控制等；系统层面，优先使用非Windows系统，对外仅提供电源、数据及管理等接口，弱化启发服务功能。</p><p>针对防火墙的<strong>探测</strong>工具常用NMAP、Firewalk以及shodan等，为防范上述工具，要尽量减少端口开放，关闭不必要的服务，并获取扫描服务器IP，添加到防火墙黑名单。</p><p>绕过防火墙方法主要有，协议隧道攻击（防御措施包括，最小开放、合理的策略、白名单机制等），分片绕过防火墙认证攻击（分析分片报文的统计数据）、IP地址欺骗攻击（阻止有风险IP、数据进站过滤、反向路径转发等，对于普通木马要合理配置强化IP包过滤规则，对于反弹木马要监测非法连接，关闭相关端口）等。</p><h3 id="2脆弱性与漏洞检测与防范"><a class="markdownIt-Anchor" href="#2脆弱性与漏洞检测与防范"></a> 2.脆弱性与漏洞检测与防范</h3><p>Vulnerability，脆弱性，源自信息系统在硬件、软件、协议等设计和实现中的先天缺陷，安全策略配置不当或者系统使用失误。脆弱性是信息系统的客观存在。</p><p>威胁来源通过<strong>攻击向量</strong>作用于信息系统的安全弱点，通过安全控制实现对信息系统资产或功能的技术影响，最终实现对系统的业务功能产生影响。攻击向量（AV）指攻击者用来攻击信息系统的一种手段，即可资网络渗透攻击利用的各种维度，比如恶意代码、恶意邮件、网页、社会工程攻击法等。</p><div align="center">  <img src="/2021/06/22/security-base/vul.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="漏洞分类"><a class="markdownIt-Anchor" href="#漏洞分类"></a> 漏洞分类</h4><p>漏洞是脆弱性之一，是存在于信息系统之中的、作用于一定环境的、可能被利用且会对系统中的组成、运行和数据造成损害的一切因素。国际漏洞库如CVE、CWE，国内有CNNVD、CNVD。</p><p>漏洞分级包括访问路径（远程、邻接和本地），漏洞被利用的复杂程度（简单、复杂），影响程度（完全、部分、轻微和无影响），综合判断（超危、高危、中危和低危）。</p><p>脆弱性扫描/漏洞扫描，扫描主要是逐个发现单个脆弱性，分析是度量网络系统整体脆弱性。</p><div align="center">  <img src="/2021/06/22/security-base/lou.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/loug.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="漏洞分析"><a class="markdownIt-Anchor" href="#漏洞分析"></a> 漏洞分析</h4><p><strong>常用的脆弱性分析方法包括图论分析法、攻击树分析法、可生存性分析法</strong>:</p><ul><li>图论分析法，将常见脆弱性利用方法、利用步骤、网络拓扑数据、系统基础参数等作为基础输入信息，选取图的节点和路径进行分析。</li><li>攻击树分析法，将外部攻击步骤映射为决策树，对信息系统的脆弱性进行量化处理，以概率描述各脆弱性被利用的可能。</li><li>可生存性分析法，在规范的网络系统中设置脆弱性和攻击事件，运用贝叶斯定理等概率计算来图形化描述和揭示安全脆弱性被利用的影响程度。</li></ul><div align="center">  <img src="/2021/06/22/security-base/live.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>漏洞检测和分析的新需求和技术<strong>发展趋势</strong></p><div align="center">  <img src="/2021/06/22/security-base/new.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="防范技术"><a class="markdownIt-Anchor" href="#防范技术"></a> 防范技术</h4><p>OWASP Top10，开放式Web应用程序安全项目，按照严重程度分别给出十大Web应用安全脆弱性和漏洞，并提供一些防范措施。</p><p>注入攻击、身份认证失效、敏感数据泄露、XML外部实体（XXE）、访问控制失效、安全配置错误、跨站脚本攻击（XSS）、不安全反序列化、使用含有已知漏洞的组件、日志记录和监控的欠缺。</p><h3 id="3-恶意代码检测及防范技术"><a class="markdownIt-Anchor" href="#3-恶意代码检测及防范技术"></a> 3. 恶意代码检测及防范技术</h3><p>恶意代码不仅自身有安全危害，更能够借助信息系统的脆弱性来实现传播和破坏。</p><h4 id="基本分类"><a class="markdownIt-Anchor" href="#基本分类"></a> 基本分类</h4><p>恶意代码有很多形式，比如病毒、蠕虫、木马、间谍软件、僵尸程序、恶意脚本、流氓软件、逻辑炸弹、后门、网络钓鱼工具等。恶意代码在<strong>代码独立性</strong>和<strong>自我复制性</strong>方面各有区别，其分类界限并不明显。</p><p><strong>病毒</strong>，可自我复制的一组代码，具有传染性、非授权可执行性、隐蔽性、潜伏性、可触发性以及破坏性等特征。</p><p><strong>蠕虫</strong>，消耗宿主资源，能够传播，自我复制、独立运行、消耗系统资源；分为传播、隐藏及目的功能模块。</p><p><strong>木马</strong>，隐藏在正常程序中的一段后门程序，伪装自身吸引用户下载执行，安置“后门”以便实施攻击，没有自我复制功能。</p><p><strong>间谍软件</strong>，在用户不知情或未经用户准许的情况下搜集、使用、并散播用户的个人数据或敏感信息。</p><p><strong>僵尸程序</strong>，具有恶意控制功能，形成一对多的控制网络。</p><p><strong>恶意脚本</strong>，利用脚本形式/纯文本形式保存。</p><p><strong>流氓软件</strong>，未明确提示用户或未经允许情况下强行安装，抵制卸载，浏览器劫持，恶意捆绑等。</p><p><strong>逻辑炸弹</strong>，隐含在正常功能中，因一定条件的触发而具有破坏性。</p><p><strong>后门</strong>，绕过安全控制而获取对程序或系统访问权的方法，以便于再次秘密进入或控制系统。</p><p><strong>网络钓鱼</strong>，利用欺骗性电子邮件和伪造的Web站点进行诈骗活动。</p><h4 id="分析检测技术"><a class="markdownIt-Anchor" href="#分析检测技术"></a> 分析检测技术</h4><p>特征主要有<strong>特征码</strong>（恶意代码的结构自身）和<strong>特征行为</strong>（需要动态运行），基于特征码校验的恶意代码检测为静态检测，而基于特征行为的恶意代码检测为动态检测。</p><p>特征码字符串多为恶意代码文件里对应代码或汇编指令的地址，也可以直接采用入口点的代码段来生成特征码。</p><div align="center">  <img src="/2021/06/22/security-base/tezheng.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>各类恶意软件的特征码会被写入代码特征库中，包括特征起始地址、特征长度、具体特征等，之后进行匹配检测。准确度高、简单快捷，但一定会出现滞后问题。</p><p>使用静态手段检测<strong>多态性恶意代码</strong>（Chameleon，Casper）有一定难度，因为它每次复制时都会更改其自身，变形方式包括采用不固定密钥或随机数加密病毒程序代码、病毒运行过程中改变代码、通过一系列模糊变换、等价指令替换、加壳、花指令、混淆等变换实现多态。同一种恶意代码的多个实例样本的代码都不相同。</p><p><strong>恶意行为</strong>指代码运行时执行的若干互不重复的内置恶意动作以及一系列扩展的时序恶意动作。孤立行为往往难以成为恶意行为特征，而且要厘清需要关注的重点行为。主要采用动态污点跟踪或状态机模型方法。</p><p>另外还有一种使用校验和的检测方法。优势在于考察程序代码完整性，不依赖外部信息，既可检测已知恶意代码，又可检测未知。缺点在于无法识别恶意代码种类和具体版本，程序自身变化导致<strong>误报率高</strong>。</p><div align="center">  <img src="/2021/06/22/security-base/sum.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="防范与治理"><a class="markdownIt-Anchor" href="#防范与治理"></a> 防范与治理</h4><div align="center">  <img src="/2021/06/22/security-base/fang.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>恶意代码清除，代码附着于宿主系统之上，只要正确恢复了文件头就可以实现清除病毒的目的。</p><p>恶意代码删除，对于无需宿主系统的代码。</p><p>恶意代码隔离，采用加密或者修改文件后缀名的方式。</p><p>当前面临的挑战：</p><ol><li>病毒采用自身加密和压缩（如加壳）</li><li>僵尸网络采用C&amp;C加密、通信采用快通量DNS</li><li>虚拟分析的虚拟机逃逸</li><li>需要提升对恶意代码的容忍和沙箱分析手段</li></ol><h3 id="4-入侵检测技术"><a class="markdownIt-Anchor" href="#4-入侵检测技术"></a> 4. 入侵检测技术</h3><p>防火墙之后的有一道防线，通用架构如下，其中目录服务组件，负责实现组件定位、数据传输控制及认证、密钥发布和管理。</p><div align="center">  <img src="/2021/06/22/security-base/in.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/ids.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>IDS核心模块主要是安全数据采集、入侵分析和入侵告警。</p><h4 id="基础架构模块"><a class="markdownIt-Anchor" href="#基础架构模块"></a> 基础架构模块</h4><p>安全数据包括主机数据（系统日志、状态、服务、事件等）、网络数据、其它特定网络组件分析处理后的抽象或结论数据。</p><p><strong>主机数据</strong>包括如下几类：</p><ul><li>内核信息：在操作系统内核获取入侵信息；</li><li>审计记录：系统身份认证无法确认的身份、访问安全等级不相符的数据、对系统运行产生重要影响的动作、其它与安全相关的动作等。使用审计探测器，记录审计产生并检查按时序排列的系统事件记录过程；</li><li>日志信息：系统程序按照每次一行写成的文本文件，反映了不同系统事件和设置。使用日志探测器，包括系统日志、安全日志、应用日志等。</li><li>系统资源信息：使用系统资源监视器，包括系统参数如CPU占用、内存利用率、I/O、网络连接数等。</li><li>应用程序信息：应用程序事件日志和内部数据。</li></ul><p><strong>网络数据</strong>包括特定协议/地址/端口的报文和字节流量的采集，反映受保护网段网路攻击或异常行为的网络包数据。使用共享式网络安全数据采集器，交换式网络安全数据采集器（增加总线式集线器、SPAN端口、TAP等）。</p><p><strong>其它数据</strong>主要由防火墙（报警）、交换机、路由器数据组成。</p><p>入侵分析是基于采集到的安全数据进行统计、模式匹配、协议解析、数据关联等，发现违反安全策略的行为或潜在的入侵行为。</p><p><strong>异常检测</strong>：假设入侵行为和网络正常行为存在较大差异。无需对规则库进行不断更新维护，可以根据异常现象发现未知攻击方式的入侵。但是批处理导致检测实时性不强，统计分析无法反映攻击事件时间相关性特征，难以确定具体的攻击方式。</p><div align="center">  <img src="/2021/06/22/security-base/ano.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>误用检测</strong>，假设已知攻击行为可以采用特定模型或规则来表示。包括协议解析、模式匹配、关联分析等技术。</p><div align="center">  <img src="/2021/06/22/security-base/wuyong.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ul><li>协议解析，利用网络协议的高度规则性，实现入侵行为的快速检测。可处理数据包、帧和链接等元素。检测速度快、准确率高、资源消耗少，可具体针对IPv4/v6以及工控协议等，但是协议树动态构建困难，高速网络下协议识别性能受限。</li><li>模式匹配，将入侵场景抽象为特征组合，与已知的攻击方法抽象得到的模式知识库进行匹配。检测速度快、准确率高、资源消耗少，可具体针对IPv4/v6以及工控协议等，但攻击模式抽取、枚举与更新工作量大，检测准确度依赖于知识库，难以检测已知攻击变种以及未知攻击。</li><li>关联分析，基于探测到的数据信息构建规则集并进行分类匹配，通过相似度对比来抖索关联规则库，采用该规则库进行规则匹配。可以分析隐含性、未知性、异常性的特征，能够实现时间、事件等关联。但是相似度定义和匹配判定比较困难，智能化要求高。</li></ul><p>总结来看，异常检测通用性强，对于未知模式的新攻击有一定检测能力，但无法准确判别攻击类型。而误用检测需要实现构建并维护较为完备的入侵检测特征知识库，检测能力依赖于特征知识库的完备性，对未知模式的新攻击检测能力不足。</p><p>入侵响应包括报警、预警和攻击阻断。</p><h4 id="性能指标与应用部署"><a class="markdownIt-Anchor" href="#性能指标与应用部署"></a> 性能指标与应用部署</h4><p>准确性（检测率、误报率、漏报率等）、效率（最大处理能力、每秒并发会话数、系统延迟时间、负荷能力等）、可用性（稳定性、可持续性、易用性）以及自身安全性（体系结构、软件实现、安全管理）等。</p><p>部署时需要考虑整体安全策略、应用环境和已有安全机制、IDS性能等。基本部署在防火墙之内/外，骨干网络及关键子网上，待监测目标主机之上等。</p><h2 id="五-网络安全主动防御"><a class="markdownIt-Anchor" href="#五-网络安全主动防御"></a> 五. 网络安全主动防御</h2><p>攻防不对称性，威胁不确定性。被动防御是主动防御的基础，主动防御是被动防御的延伸。</p><p>主动功能包括：1）攻击或者安全破坏发生之前，实现及时、准确识别和预警；2）主动规避、转移、降低或消除网络空间所面临的安全风险。</p><div align="center">  <img src="/2021/06/22/security-base/active.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="1-入侵防御容忍技术"><a class="markdownIt-Anchor" href="#1-入侵防御容忍技术"></a> 1. 入侵防御/容忍技术</h3><div align="center">  <img src="/2021/06/22/security-base/fy.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>入侵防御技术（IPS），基于入侵检测的深度防御安全机制，通过主动响应方式，实时阻断入侵行为，降低或减缓网络异常状况处理的资源消耗，以保护网络空间信息系统免受更多侵害。技术体现在智能检测、深层防御和主动响应。</p><ul><li><strong>智能检测</strong>主要是高效检测和联动，作用基础是相对完备的恶意代码特征知识库和入侵攻击特征知识库。实现智能协议识别与解析、数据报文的深层检测等。将基于特征的检测和基于统计的检测方法智能结合。</li><li><strong>深层防御</strong>，主要作用于恶意代码和应用层数据解析，应对多种威胁如恶意代码、通用网关接口、跨站脚本攻击、注入攻击、信息泄露等，可进行应用层攻击载荷防护，需要检测报文应用层内容和数据流重组分析和检测等。</li><li><strong>主动响应</strong>，自动执行或用户驱动来阻断、延缓入侵进程或改变受攻击信息系统环境配置，融合恶意代码检测、脆弱性评估、防火墙等进行安全联动。</li></ul><div align="center">  <img src="/2021/06/22/security-base/method.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>入侵容忍技术</strong>是为了提升系统自身的免疫力，包括可生存技术、容错技术。</p><div align="center">  <img src="/2021/06/22/security-base/basen.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>容错技术在网络攻防对抗环境下适用性不强。因为容错技术并非针对恶意入侵/攻击而设计，而且并非所有攻击/入侵均表现为信息和系统的破坏进而呈现显示错误。</p><div align="center">  <img src="/2021/06/22/security-base/wrong.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>入侵容忍技术（ITS）提出的原因是IDS、IPS无法完全检测所有的入侵行为，一旦有入侵发生，可以继续保障机密性、完整性、真实性和安全性。核心目标是实现系统权限的分立以及单点失效的技术防范，确保任何少数设备、局部网络以及单一场点均不可能拥有特权或对系统整体运行构成威胁。主要分为<strong>攻击响应（重新分配资源，实现系统重构）<strong>和</strong>攻击遮蔽技术（适度冗余配置，实现高生存性）</strong>。</p><p>系统资源调整方式：</p><div align="center">  <img src="/2021/06/22/security-base/adjust.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>攻击遮蔽中的冗余技术无需再系统遭受入侵或失效时重构，系统操作以及连接可以继续保持。</p><h3 id="2-蜜罐密网技术"><a class="markdownIt-Anchor" href="#2-蜜罐密网技术"></a> 2. 蜜罐/密网技术</h3><p>&quot;欺骗型&quot;主动防御的关键技术之一，专门设置特定的脆弱系统来诱捕攻击。蜜罐技术采用单台主机构建诱骗环境，密网技术采用网络系统构建诱骗环境。</p><p>部署具有脆弱性或漏洞的相应主机、网络服务或信息作为诱饵，引诱攻击者发起攻击，为防御者提供安全威胁提示，最后通过技术和管理手段提升系统安全防御能力。</p><p><strong>蜜罐</strong>中不进行传统目的的网络活动，即不运行其它进程、服务和后台程序。蜜罐系统中的所有交互行为，都视为恶意活动的嫌疑对象，有利于攻击活动的检测与识别。</p><div align="center">  <img src="/2021/06/22/security-base/honey.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>低交互蜜罐，仅让攻击者有限访问系统；静态欺骗环境（仅模拟存在漏洞的真实操作系统的某个部分）、部署简便、诱骗能力有限。</p><p>高交互蜜罐，完整服务栈，部署复杂，安全风险较大（被攻破可能会作为跳板）。</p><p>详细攻击信息获取选用高交互方式，而简单的恶意样本获取可以用低交互方式。</p><p>蜜罐系统局限性：</p><ul><li>只能对其内部情况进行监视分析，无法监控整个网络；</li><li>适度的安全平衡难以把控</li><li>如果攻击活动位于IPsec、SSH、SSL等加密通道，蜜罐系统难以分析处理。</li></ul><p>将蜜罐有机组合成网络分布式结构模拟整个网络，形成<strong>密网</strong>。</p><div align="center">  <img src="/2021/06/22/security-base/honeynet.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>低交互密网，模拟一个生产环境，仅提供有限信息，未给攻击者提供一个可用操作系统。</p><p>高交互密网，为攻击者提供真实的支撑操作系统，设置全功能要素应用环境。</p><p>部署时采用重定向技术、虚拟密网技术和密网应用技术（蠕虫病毒防范、网络钓鱼识别和僵尸网络捕获）。</p><ul><li>重定向技术实现关键主机或服务器的隐蔽，可以捕获攻击者的全部攻击流程，实现了硬件资源的节省以及蜜罐管理的方便；</li><li>部署成本下降，管理方便，但安全风险比较大</li></ul><p>密网技术的可移植性不强，需要提高跨平台部署与运行能力；交互性和风险控制的平衡；信息控制和记录功能有待提升。</p><h3 id="3-沙箱技术"><a class="markdownIt-Anchor" href="#3-沙箱技术"></a> 3. 沙箱技术</h3><p>沙箱测试、安全沙箱、虚拟化沙箱。发现可疑行为后让程序继续运行，充分暴露恶意行为特征属性。</p><div align="center">  <img src="/2021/06/22/security-base/sandbox.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>基于虚拟化的沙箱</strong>，系统虚拟化沙箱（为不可资源提供完整的操作环境，常用VMware、VirtualBox等，硬件和目的操作环境之间增加操作层）、容器虚拟化沙箱（提供不完整的操作环境，轻量级虚拟化技术实现，操作系统和应用程序之间增加虚拟化层）。</p><p><strong>基于规则的沙箱</strong>，不需要系统资源的复制，使用访问控制规则实现特定控制，通常包括访问控制规则引擎、程序监控器等。</p><p>系统级应用场景，比如安卓沙箱；应用级场景，比如浏览器沙箱。</p><p>不足在于：1）实践中产生完全相同的结果非常困难；2）高度依赖于恶意行为的展现；3）高度依赖于对恶意威胁的有效检测，难以应对虚拟机检测、时间炸弹、调试循环、鼠标点击、系统重启等逃逸行为。</p><h3 id="4-可信计算与可信平台"><a class="markdownIt-Anchor" href="#4-可信计算与可信平台"></a> 4. 可信计算与可信平台</h3><p>可信性出现了“Dependability”、“Trustworthiness”以及“High Confidence”等不同叫法。</p><div align="center">  <img src="/2021/06/22/security-base/trust.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="信任模型"><a class="markdownIt-Anchor" href="#信任模型"></a> 信任模型</h4><p>信任属性包括主客观二重性、不完全对称性、可度量性、可传递性以及动态性等。</p><div align="center">  <img src="/2021/06/22/security-base/model.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>信任模型是不同的主客体之间的相互信任关系的组织方式，分为基于信任度表征的不确定性描述，基于结构，基于信任管理方式三种。</p><p>基于结构的信任模型划分，包括星型结构（唯一公共授权节点）、分层结构（以公共根授权节点为核心）和点对点结构（自组网直接信任关系集合）。</p><h4 id="可信计算"><a class="markdownIt-Anchor" href="#可信计算"></a> 可信计算</h4><p>2003年成立可信计算组（TCG）。</p><p>可信计算是计算运算与安全防护同步进行的计算新模式，计算过程可测可控、不受干扰，具有身份识别、状态度量、保密存储等功能，使得计算结果总是符合预期。</p><ul><li>理论基础包括可信计算模型、度量理论、信任理论与信任链以及可信软件理论；</li><li>应用范围包括从硬件到软件、从芯片到网络、从基础软件到应用程序、从设计环节到运行环境；</li><li>关键技术有软硬件结构、可信计算平台、密码技术、信任链技术、可信软件和可信网络等；</li></ul><h4 id="可信平台"><a class="markdownIt-Anchor" href="#可信平台"></a> 可信平台</h4><div align="center">  <img src="/2021/06/22/security-base/trusts.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>信任根是系统的可信基点</strong>，假定可被无条件信任，并不是客观的绝对可信任，而是“被信任”。工程实践中，信任根的可信性通常由物理安全、技术安全以及管理安全等综合措施共同保障。采用<strong>可信度量存储报告机制</strong>，分为可信度量根（负责完整性度量），可信存储根（存储度量的可信性）和可信报告根（向访问客体提供报告）。</p><p>信任链是基于信任根构建信任度量模型的实施技术方案。<strong>可信根之外的所有模块或组件在未经度量前均不可信</strong>。实施可信度量并与预期相符的才纳入可信边界，逐步扩大可信边界直到涵盖整个信息系统。</p><div align="center">  <img src="/2021/06/22/security-base/chain.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>可信度量报告存储机制，是可信计算系统和普通计算系统在安全机制上的核心区别</strong>。常见可信平台包括可信PC机、服务器、移动终端、网络和可信云。</p><p><strong>可信平台模块</strong>和<strong>可信软件协议栈</strong>最重要。</p><ul><li>可信平台模块（TPM）:有密码运算部件和存储部件的小型片上系统，作为密钥生成器和密钥管理器，完成计算平台的可靠性认证、用户身份认证和数字签名等功能。</li><li>可信软件栈（TSS）：在TPM之上、应用程序之下的一种用以支撑可信计算平台的软件中间件，提供TPM接口，实现TPM管理，常用结构中分为内核层、系统服务层和用户程序层。</li></ul><div align="center">  <img src="/2021/06/22/security-base/tpm.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>可信网络连接架构包括可信网络连接（TNC，由TCG提出本质上为二元架构，仅涉及访问请求者和网络接入方）和可信连接架构（TCA，中国提出，采用三元、三层、对等、集中管理）。</p><p>TNC架构如下图所示，包括完整性度量层（搜集访问请求者的完整性信息并验证）、完整性评估层（根据给定范文策略实现AR完整性评估）和网络访问层（从属于传统网络互联和安全层，支持VPN和802.1X等）。</p><div align="center">  <img src="/2021/06/22/security-base/tnc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>TCA架构则在完整性度量层加入了访问控制器，可信评估层加入可信网络连接接入点，网络访问层实现终端连接到网络的可信网络连接。</p><h3 id="5-移动目标防御与拟态防御技术"><a class="markdownIt-Anchor" href="#5-移动目标防御与拟态防御技术"></a> 5. 移动目标防御与拟态防御技术</h3><p>通过系统自身和攻击面动态变化来增大攻击难度。</p><h4 id="移动目标防御技术mtd"><a class="markdownIt-Anchor" href="#移动目标防御技术mtd"></a> 移动目标防御技术（MTD）</h4><p>构建一种动态、异构、不确定的网络，增大试试成功攻击的代价和复杂度，降低成功概率，实现攻防对称平衡。</p><blockquote><p>攻击面，可暴露在攻击者面前的系统资源（软硬件、通信协议/端口号及系统各组件的脆弱性等）以及已被侵害的可用于系统渗透的网络资源集合。</p><p>主要包括：物理环境、网络通信、区域边界、计算环境；数据、软件、运行环境、平台、网络</p></blockquote><p>在系统方面不会为攻击者提供一成不变的基础设施和攻击面；安全体系方面不会将防火墙、IDS/IPS、恶意代码查杀等逐一部署；检测防护方面无需依赖脆弱性、威胁以及攻击等知识及特征库。</p><p>MTD主要<strong>实现策略</strong>是变化系统配置，使其生命周期缩短，增强系统弹性。可实现参数可变性（IP地址、端口、通道数、路由、IPsec信道等）和主机随机性（网络/主机身份、执行代码、地址空间、指令集以及数据等）。</p><p>MTD的攻击面变换技术主要包括，扰乱（SDN架构）、多样化（软件/虚拟服务器/底层）、冗余（自保护/容灾网络）等。</p><p>攻击面变换策略中要保持<strong>安全性-可靠性-开销</strong>之间的平衡，考虑变换时机（时间或事件驱动）、频率。</p><h4 id="拟态防御技术cmd"><a class="markdownIt-Anchor" href="#拟态防御技术cmd"></a> 拟态防御技术（CMD）</h4><p>中国科学家提出，基于属性特征和行为形态实现外界防御。构建外界无法掌握规律、无法破解结构的安全防御体系，避免恶意攻击。</p><div align="center">  <img src="/2021/06/22/security-base/mimc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/cmd.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常用的拟态防御设备组件有，拟态域名服务器/路由器、拟态Web服务器/Web虚拟机、拟态文件和存储系统、拟态工业控制处理器、拟态防火墙。</p><h4 id="mtd与cmd异同点总结"><a class="markdownIt-Anchor" href="#mtd与cmd异同点总结"></a> MTD与CMD异同点总结</h4><p>共同点为，均为动态化、多样化、随机化的主动防御，旨在破解“易守难攻”的不对称局面；均以通过实施己方可控的变化来迷惑攻击者，增加攻击难度和代价。</p><p>差异在于：</p><div align="center">  <img src="/2021/06/22/security-base/diff.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="六-网络安全智能分析与反制技术"><a class="markdownIt-Anchor" href="#六-网络安全智能分析与反制技术"></a> 六. 网络安全智能分析与反制技术</h2><p>智能分析的目标是全天候全方位感知网络安全态势、增强网络安全防御能力。通过智能化处理过程得到安全情报、形成智能知识和能力。</p><div align="center">  <img src="/2021/06/22/security-base/qing.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以情报驱动防御，以威胁为中心，进行敌我分析，主要包括风险评估、态势感知和威胁情报等关键技术。</p><h3 id="1-风险评估"><a class="markdownIt-Anchor" href="#1-风险评估"></a> 1. 风险评估</h3><p>风险要素识别、分析、计算与判定。使防御方准确定位管控的策略、实践和工具，聚焦于重要问题。</p><p>针对以下要素进行识别与赋值。安全就是可接受的风险程度。绝对安全既无可能也无必要。</p><div align="center">  <img src="/2021/06/22/security-base/eva.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/security.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/value.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="2-态势感知"><a class="markdownIt-Anchor" href="#2-态势感知"></a> 2. 态势感知</h3><p>识别安全威胁，动态研判安全风险，输出威胁情报。包括安全态势认知、要素提取、态势理解、态势预测以及可视化展现等。</p><p>网络空间既有和现有的状态以及未来发展趋势。</p><p>以安全大数据为基础，从全局视角提升对安全威胁的发现识别、理解分析及响应处理能力。包括三个原则：</p><ul><li>环境性，范围较大有一定规模的网络</li><li>动态性，态势随时间而变，涵盖过去和当前的状态以及对未来趋势的预测</li><li>整体性，各要素关系符合整体性原理，不可割裂</li></ul><div align="center">  <img src="/2021/06/22/security-base/taishi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>网络安全态势感知需要持续监控能力、分析及可视化能力和安全预警能力。</p><div align="center">  <img src="/2021/06/22/security-base/ke.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>关键技术包括，态势认知和要素提取（系统庞大且灵活性强）、态势理解（多方融合，完成从数据到信息，从信息到情报的转化）、态势预测（预测未来趋势）以及可视化展现。</p><p>要素提取：时间维度（实时或准实时数据、更长时间的数据发现多阶段攻击）和数据维（网络数据、安全数据、网络流量、内容载荷、终端行为）；</p><ul><li>态势理解：对多种安全要素数据进行融合处理、综合分析，并跟踪分析；</li><li>态势预测：从已知威胁推测未知威胁；</li><li>可视化：数据转化、图像映射和视图转变；</li></ul><h3 id="3-威胁情报"><a class="markdownIt-Anchor" href="#3-威胁情报"></a> 3. 威胁情报</h3><p>生成并输出，为其它环节应用。</p><p>情报是某种基于证据的网络安全知识，包括环境上下文、机制、指标、影响和可行性建议等信息，用于描述现有的或可能出现的威胁。</p><p>分为战略威胁情报（高层决策）、运营威胁情报（预测何时何地发生）、战术威胁情报（响应）、技术威胁情报（应用最广泛）。</p><p>分为安全集成、应急响应和资产管理三个方面。</p><p>生成过程分为五步，即明确需求和目标、采集、分析、传播共享、评估和反馈。</p><p>威胁情报来源有内部（系统的各类数据和信息）和外部（厂商威胁数据、网站论坛公开信息、暗网地下论坛等）。</p><p><strong>常用威胁情报分析模型</strong>分为三类：</p><ul><li>网络杀伤链，侦查、武器化、交付、利用、部署、命令与控制、目标达成7个阶段；</li><li>钻石模型，将科学原理用于入侵分析，可衡量、可测试、可重复。可对攻击活动进行记录合成、关联。</li><li>ATT&amp;CK，将已知攻击者行为转化为结构化列表，汇总成12种战术和244种技术，全面呈现网络攻击行为。</li></ul><p>情报标准格式分为国际常用标准（STIX、TAXII、OpenIOC）和中国常用标准（《网络安全威胁信息格式规范》GB/T36643，八个组件、三个域）。</p><h2 id="七-攻击与反制技术"><a class="markdownIt-Anchor" href="#七-攻击与反制技术"></a> 七. 攻击与反制技术</h2><div align="center">  <img src="/2021/06/22/security-base/attack.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/defence.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>攻击向量/矢量，攻击者到达并接触任意给定的漏洞代码的过程，比如通过恶意软件、垃圾邮件等；</p><p>攻击面（攻击表面、层面），可被各类用户利用、遭受攻击的系统资源集合，攻击者可通过攻击面实施攻击。</p><p><strong>攻击溯源</strong>是找到攻击者具体位置包括攻击源IP、攻击类型（手段）、攻击目标、攻击路径、攻击时间等跟攻击相关的核心要素。溯源分为攻击主机溯源（定位直接实施攻击的主机）、攻击控制主机溯源和攻击者及组织机构溯源。</p><p>攻击主机溯源又称为IP追踪，包括主动追踪（基于日志存储查询、基于概率数据包标记、基于确定数据包标记）和反应追踪（路由器输入调试、基于ICMP拥塞追踪）。</p><p>攻击控制主机溯源，抓住幕后黑手，包括反射、跳板、非标准化软件、僵尸溯源、物理等溯源方法。</p><h2 id="七-网络安全度量-分析与测评技术"><a class="markdownIt-Anchor" href="#七-网络安全度量-分析与测评技术"></a> 七. 网络安全度量、分析与测评技术</h2><div align="center">  <img src="/2021/06/22/security-base/framework.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="0-指标体系"><a class="markdownIt-Anchor" href="#0-指标体系"></a> 0. 指标体系</h3><p>若干个反映信息系统安全性总体数量特征的、相对独立又相互联系的统计指标所组成的有机整体。</p><p>安全指标复杂性体现在如下四个方面：</p><div align="center">  <img src="/2021/06/22/security-base/zhibiao7.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>遵循如下六条原则：</p><div align="center">  <img src="/2021/06/22/security-base/six.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>基于安全性要求</strong>的指标体系，直观易理解，适用面广泛，但指标关联性强、构建比较复杂。</p><p><strong>基于安全域划分</strong>的指标体系，如下图所示的IP网络、工控网络。直观容易理解，适用于安全域边界清晰，但依赖于安全域的合理划分，聚合权值确定困难。</p><div align="center">  <img src="/2021/06/22/security-base/ip.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/kong.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>基于合规性要求</strong>的指标体系，符合我国网络安全等级保护2.0标准规范。操作性较强，适用面广，但依赖于合规的合理性，聚合权值确定困难。</p><div align="center">  <img src="/2021/06/22/security-base/deng.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth" < div><p>基于防御环节组成的指标体系，与防御环节（滑动标尺模型）适配，安全能力生成过程清晰，相对较为抽象，仅适用于大型复杂信息系统。</p><h3 id="1-安全度量"><a class="markdownIt-Anchor" href="#1-安全度量"></a> 1. 安全度量</h3><p>理论基础，获取安全基础数据。</p><div align="center">  <img src="/2021/06/22/security-base/archite.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>基于控制论的系统共轭理论，安全能力生成和安全状态观测度量的<strong>共轭机制</strong>。</p><p>测度选择六原则：</p><div align="center">  <img src="/2021/06/22/security-base/sixrule.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth" < div><p>安全数据采集需要遵循体系化原则、成本效益原则、合规性原则、质量平衡原则和非破坏性采集原则。</p><p>常见采集对象分为：网络基础设施（网络交换机、路由器和网关）、网络安全设备组件（恶意代码防范工具、防火墙/入侵检测系统、网闸）、网络安全系统（集中授权系统、身份认证系统、安全管理中心等）、通用应用系统（数据库系统、中间件系统、邮件系统等）、具体业务系统（ERP系统、SCADA系统、DCS系统等）。</p><h3 id="2-安全分析"><a class="markdownIt-Anchor" href="#2-安全分析"></a> 2. 安全分析</h3><p>方法支撑</p><div align="center">  <img src="/2021/06/22/security-base/any.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>安全定性分析，样本不足导致代表性受限，因果关系推断导致存在较大不确定性。常用逻辑分析法（不涉及/缺乏数据）和德尔菲法（基于定量分析的更高层次定性分析）。</p><div align="center">  <img src="/2021/06/22/security-base/derfi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>安全定量分析，容易忽视人的主观判断，受限于数据质量和分析方法。</p><div align="center">  <img src="/2021/06/22/security-base/tree.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>一般使用定性与定量形结合，如下：</p><div align="center">  <img src="/2021/06/22/security-base/combine.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="3-安全测评"><a class="markdownIt-Anchor" href="#3-安全测评"></a> 3. 安全测评</h3><p>工程实施，分为信息安全模块、信息安全系统和产品三个模块。</p><div align="center">  <img src="/2021/06/22/security-base/cep.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>主要包括验证技术、测试技术和评估技术。</p><ul><li>验证技术，证实网络安全性质或采集安全度量数据。针对网络安全产品和网络信息系统。</li><li>测试技术，获得反映安全产品或系统安全性能度量指标值，分为真实场景测试和仿真场景测试。</li><li>评估技术，客观公正地评价由验证和测试结果所反映的网络安全性能。</li></ul><div align="center">  <img src="/2021/06/22/security-base/test.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常用的等保测评是一种合规测评，流程包括确定测评对象（范围、标准、步骤）、核心流程（单向、整体）和结果分析。</p><p>常用测评工具分类如下：</p><div align="center">  <img src="/2021/06/22/security-base/tools.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="八-新应用安全技术"><a class="markdownIt-Anchor" href="#八-新应用安全技术"></a> 八. 新应用安全技术</h2><h3 id="1-云安全"><a class="markdownIt-Anchor" href="#1-云安全"></a> 1. 云安全</h3><p>资源配置动态化、需求服务自主化、网络访问便捷化、服务可计量化、资源虚拟化。</p><p>主要安全威胁：数据泄露、配置错误和变更控制不足、缺乏云安全架构和策略、身份访问和密钥管理不力、账户劫持、内部威胁、不安全的接口和API、控制平台薄弱、元结构和应用程序结构故障、云计算使用情况有限可见性、滥用和恶意使用云计算服务。</p><p>其存在的传统安全威胁主要在合规、数据和安全管理层面，特有安全风险在云计算平台、用户访问和管理方面。</p><p>云计算安全技术主要涉及云计算基础设施（物理安全、边界安全、虚拟化安全）、云应用（应用软件安全、虚拟桌面安全、云终端安全）、云数据（加密、备份与容灾、隐私保护）、云密钥管理、云身份管理和访问控制、云安全服务与评估几个方面。</p><p>安全功能的服务化，包括防火墙、IDS等安全防御服务；风险评估、安全测评等云安全服务；系统备份、故障诊断与灾难恢复等运维服务。</p><p>云计算在网络安全中的应用。许多信息安全活动都以云计算模式的服务呈现，利用云计算系统的超强存储、网络和计算能力，可以提升威胁情报分析、智能检测、加密通信等安全防御能力。比如基于云和客户端的恶意代码查杀，基于复杂运算并行化的并行漏洞挖掘，基于存储资源调度备份策略的网络安全存储与备份。</p><h3 id="2-移动互联网安全"><a class="markdownIt-Anchor" href="#2-移动互联网安全"></a> 2. 移动互联网安全</h3><p>移动互联是采用无线通信技术将移动终端接入有线网络的过程。</p><p>移动互联网（WMI），是互联网技术、平台、商业模式和应用与移动通信技术结合并实践的活动的总称。</p><p>移动互联系统主要包括移动终端、移动应用和无线网络。移动互联特点体现在终端移动性、业务私密性、能力局限性和系统关联性。</p><p>无线网络主要接入类型有无线个域网（WPAN，蓝牙、IEEE 802.15、UWB、Zig-Bee等）、无线局域网（WLAN）、无线城域网（WMAN）、移动通信网（WWAN）。</p><p>移动互联网安全体现在传统安全（合规、数据、安全管理）和特有安全（平台、终端软硬件、管理）两个方面，分为移动终端、APP和无线网络三层次威胁。</p><ul><li>移动终端风险：网络窃听、物理访问、恶意应用和网络攻击</li><li>移动应用：安全漏洞、恶意应用、仿冒威胁</li><li>无线网络：被动窃听和流量分析、消息拦截和篡改、伪装欺诈钓鱼攻击、拒绝服务攻击</li></ul><p>安全体系和相关技术，在终端实现通信保护、存储保护、配置授权和鉴别，完整性等；移动应用部分进行恶意行为防范；无线网络部分进行安全隔离、通信安全、双因素认证、合规检查、权限控制、仿冒行为管控等。</p><div align="center">  <img src="/2021/06/22/security-base/devicep.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/appp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/netp1.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/netp2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="3-物联网安全"><a class="markdownIt-Anchor" href="#3-物联网安全"></a> 3.  物联网安全</h3><p>物联网（IoT），通过感知设备，按照约定协议，连接物、人、系统和信息资源，实现对物理和虚拟世界的信息处理并作出反应的智能服务系统。</p><p>物联网三层架构包括感知层、网络层（传输信息）和应用层（智能处理）。实现包括物联网终端、物联网网关、云平台、Web和/或移动客户端。</p><p>物联网主要特点：</p><ul><li>全面感知，利用RFID（无线射频识别）、传感器、定位器和二维码等手段随时随地采集和获取物体信息；</li><li>可靠传递，融合电信网和因特网；</li><li>智能处理，接收海量数据进行智能计算分析；</li></ul><p>安全威胁层面：</p><ul><li>感知层，资产暴露、物联网设备脆弱性；</li><li>网络层，与传统网络安全类似，部分通信协议有区别；</li><li>应用层，与传统网络安全类似，涉及基础架构、验证、授权和加密；</li><li>系统整体，各层脆弱性相互连接、传播，攻击相互渗透</li></ul><p>OWASP IoT Top 10 包括：</p><p>1）弱密码、可猜测密码或硬编码密码；2）不安全的网络服务；3）不安全的生态接口；4）缺乏安全的更新机制；5）使用不安全或已弃用的组件；6）隐私保护不充分；7）不安全的数据传输和存储；8）缺乏设备管理；9）不安全的默认设置；10）缺乏物理加固措施。</p><p>重点是物联网设备的安全风险，包括弱口令、硬件接口暴露、信息泄露、未授权访问。</p><div align="center">  <img src="/2021/06/22/security-base/iot.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="4工控网安全"><a class="markdownIt-Anchor" href="#4工控网安全"></a> 4.工控网安全</h3><p>工控系统是传统工业控制系统不断向信息化、网络化方向演进而得到的结果。是数据采集与监视控制系统（SCADA）、分布式控制系统（DCS）、编程逻辑控制器（PLC）等工业场景控制系统总称。</p><p>工业互联网是互联网创新发展与工业4.0持续交汇，将工业领域人、机器、物体全面互联。其功能体系分为三个部分，即网络功能体系（企业内网、外网IP化）、平台功能体系（IIP）和安全功能体系。</p><div align="center">  <img src="/2021/06/22/security-base/indus.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>工业互联网与外界连接，打破了工控系统和工控网络的封闭性，称为黑客的重要攻击目标，而其特殊通信协议和系统更面临严重威胁。传统安全防御难以奏效，工业互联网可用性保障困难，控制设备“重功能，轻安全”的现象普遍，工控网络的单向通信存在安全风险。</p><p>工业现场控制系统（涉及现场控制层和工业现场设备）脆弱性：</p><div align="center">  <img src="/2021/06/22/security-base/now.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>工控安全技术体系，包括设备与控制安全、网络安全（内外网络隔离、内网安全区划分及访问控制、工业防火墙、工业IDS、介质安全、工业协议安全和身份鉴别与认证）、应用安全（工业应用软件、微服务和容错设计）和数据安全（采集、传输、存储和处理）。</p><h3 id="5-大数据安全技术"><a class="markdownIt-Anchor" href="#5-大数据安全技术"></a> 5. 大数据安全技术</h3><p>大数据：在可承受时间范围内无法用常规软件工具提取、存储、搜索、共享、分析和处理的海量复杂数据集合。概括为5V特点，容量、种类、速度（增长、处理与时效性）、价值（密度低）、真实性（质量）。大数据核心是发现数据价值。</p><p>思维变革：</p><ul><li>由随机抽样到全数据模式</li><li>由追求精确思维到接收混杂现实</li><li>由探究因果关系到揭示相关关系</li></ul><div align="center">  <img src="/2021/06/22/security-base/bigdata.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>大数据自身安全包括：采集、存储（云安全）、计算（隐私保护）方面。将大数据技术应用于安全则包括态势感知、舆情监控、情报分析等。</p><div align="center">  <img src="/2021/06/22/security-base/frame.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>面临的安全威胁，传统层面（保密性、完整性和可用性）和特有部分（异常流量攻击、隐私泄露、传输和存储风险）。</p><ul><li>异常流量，分布式存储（数据量大且路径视图相对清晰）、认证问题（终端用户多）、APT、攻击目标变化（干预分析结果）等；</li><li>信息和隐私泄露，用户隐私数据，数据量持续快速增大，复杂性高；</li><li>传输安全，生命周期安全（逐步失真）、基础设施安全、个人隐私安全；</li><li>存储管理，数据量增长快，数据类型多，多种应用进程并发。</li></ul><div align="center">  <img src="/2021/06/22/security-base/chall.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>大数据安全包括平台及运行安全（基础设施、传输、存储、计算、平台），数据安全（分类、隔离、溯源、加密）和隐私安全（去标识、匿名化）：</p><div align="center">  <img src="/2021/06/22/security-base/bases.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/datas.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/pris.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>大数据技术在网络安全场景中的应用，以OODA循环为例。</p><h3 id="6-人工智能与网络安全"><a class="markdownIt-Anchor" href="#6-人工智能与网络安全"></a> 6. 人工智能与网络安全</h3><p>包括人工智能技术滥用，比如筛选钓鱼目标，构造恶意代码，绕过验证码等，侵犯隐私，如数据采集中、云计算中和知识抽取中等问题。</p><p>人工智能自身安全问题，比如软件实现漏洞（深度学习框架中）、恶意样本生成（对抗学习）和训练数据污染（伪造脏/假数据）等。会带来驱动系统混乱、漏判或误判、系统崩溃、智能设备变僵尸等问题。</p><p>在网络安全领域的应用，优势在于技术先进（大数据分析识别威胁、关联性安全态势分析、自学习应急响应防御）、场景丰富（IDS、恶意软件检测、用户行为分析、欺诈检测、评分风险等），但存在算法实现欠缺（持续学习、数据饥饿、可解释性差等问题）、数据过度依赖、算力需求高等问题。</p><h3 id="7-量子通信安全"><a class="markdownIt-Anchor" href="#7-量子通信安全"></a> 7. 量子通信安全</h3><p>传统加密通信技术存在很多安全隐患，量子通信为绝对安全提供了可能。</p><ul><li>传统加密算法安全问题，依赖计算复杂度，一旦算力具备都可以破解。RSA算法基于大数因子分解，通过因式分解破解，2019年谷歌使用量子计算机在8小时破解2048位密钥；哈希算法基于抗碰撞性和不可逆性，王小云给出了摘要算法碰撞攻击理论。</li><li>传统通信方式安全问题，保证传输介质和系统加密实现安全，但有线通信存在网络监听、传输数据篡改、密码破解、欺骗攻击、中间人攻击、缓冲区溢出等风险；无线通信存在窃听窃取、恶意操作、破坏及信息泄露、数据篡改、数据欺骗、隐私侵犯等问题</li></ul><p>量子通信安全基于<strong>量子的制备和测量特性</strong>（不可克隆原理和海森堡不确定性原理）以及<strong>量子纠缠特性</strong>（产生影响）。</p><div align="center">  <img src="/2021/06/22/security-base/clone.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/22/security-base/certain.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>量子密钥分发（QKD），量子通信一旦被窃听会改变，保证“一次一密”，实用化的协议是未利用纠缠态的BB84协议，以光子为量子，以偏振态传递信息。信息还是通过传统方式加密，如果遭到窃听就重新协商密钥。</p><p>量子态隐形传输（QT），利用跨越空间的量子纠缠把一个量子比特无损地从一个地点传到另一个地点。并不移动量子态的物理载体本身。</p><h3 id="8-区块链安全"><a class="markdownIt-Anchor" href="#8-区块链安全"></a> 8. 区块链安全</h3><p>课件出现错误。</p><h1 id="相关资料"><a class="markdownIt-Anchor" href="#相关资料"></a> 相关资料</h1><p>要不要考个证噻？<a href="http://zhuanlan.zhihu.com/p/101574681" target="_blank" rel="noopener">http://zhuanlan.zhihu.com/p/101574681</a></p></div></div>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>网络安全</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>耶鲁幸福课</title>
    <link href="/2021/06/21/active/"/>
    <url>/2021/06/21/active/</url>
    
    <content type="html"><![CDATA[<h2 id="耶鲁幸福课简课"><a class="markdownIt-Anchor" href="#耶鲁幸福课简课"></a> 耶鲁“幸福课”简课</h2><p>B站视频（<a href="https://www.bilibili.com/video/BV1aX4y1G7we%EF%BC%9Bhttps://www.youtube.com/watch?v=duhW4of8bOA&amp;t=5s%EF%BC%89" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1aX4y1G7we；https://www.youtube.com/watch?v=duhW4of8bOA&amp;t=5s）</a></p><p>2018年的《心理学与美好生活》课程，完整版在Coursera上（<a href="https://www.coursera.org/learn/the-science-of-well-being%EF%BC%89" target="_blank" rel="noopener">https://www.coursera.org/learn/the-science-of-well-being）</a></p><p>寻找比较“快乐”的人，关注他们的生活方式，总结出以下十条观点：</p><h5 id="1-we-can-control-more-of-our-happiness-than-we-think"><a class="markdownIt-Anchor" href="#1-we-can-control-more-of-our-happiness-than-we-think"></a> 1. We can control more of our happiness than we think</h5><p>比如半空半满态度，基因在50%的程度上决定着我们的幸福水平。</p><h5 id="2-our-life-circumstances-dont-matter-as-much-as-we-think"><a class="markdownIt-Anchor" href="#2-our-life-circumstances-dont-matter-as-much-as-we-think"></a> 2. Our life circumstances don’t matter as much as we think</h5><p>比如中了彩票大奖的人或遭遇重大挫折的人，在半年之后幸福水平会回归他的常态值。遭遇重大挫折或重大疾病的人反而会逐渐获得平静（但我们不能总是期待依靠失去生命而学会如何真正地活着）。</p><p>外部环境在10%的程度上决定着我们的幸福水平。</p><p>所以还剩下整整40%的部分是可以由我们自己决定的。</p><h5 id="3-you-can-become-happier-but-it-takes-work-and-daily-effort"><a class="markdownIt-Anchor" href="#3-you-can-become-happier-but-it-takes-work-and-daily-effort"></a> 3. You can become happier but it takes work and daily effort</h5><p>像学习其它技能时一样，通过大量、坚持地联系，学会更加幸福。</p><p>knowing is（not even） half the battle（知道是成功的一半，还不到）</p><h5 id="4-your-mind-is-lying-to-you-a-lot-of-the-time"><a class="markdownIt-Anchor" href="#4-your-mind-is-lying-to-you-a-lot-of-the-time"></a> 4. Your mind is lying to you a lot of the time</h5><p>比如人们认为有更多的钱，比如加薪，就可以增加幸福感。</p><p>我们的目标会移动，越来越大。追求物质的人更加不快乐。</p><h5 id="5-make-time-for-making-social-connections"><a class="markdownIt-Anchor" href="#5-make-time-for-making-social-connections"></a> 5. Make time for making social connections</h5><p>**这一点非常重要！**有研究表明，独处的时间越长的人越容易被归类为“不幸福的”。这一点我自己也是有亲身体会，但是这不说明我们不需要独处。</p><p>只是不要过度独处，要与他人（朋友、家人、爱人）建立关系。</p><p>而且要<strong>注意是真正与他人相处</strong>，之前我的社交是“表面社交”，因为在和别人一起时，我其实更多地还是关注自己的心理问题，并没有做到全心全意，有点像只是躯壳在于他人共处。</p><p>与他人建立真正的联系会让我们感到更加幸福。</p><p>虽然可能存在双向相关性，比如是这个人本身很幸福所以才去社交，但研究表明，与他人交往确实会提升我们的幸福感（这一点我也是亲身体会）。</p><h5 id="6-helping-others-makes-us-happier-than-we-expect"><a class="markdownIt-Anchor" href="#6-helping-others-makes-us-happier-than-we-expect"></a> 6. Helping others makes us happier than we expect</h5><p>志愿活动，帮助、关心他人确实会提升幸福感。不要把“善良”、“无私”等词语只挂在嘴边。</p><h5 id="7-make-time-for-gratitude-every-day"><a class="markdownIt-Anchor" href="#7-make-time-for-gratitude-every-day"></a> 7. Make time for gratitude every day</h5><p>写感恩日记（每天写3~5个，坚持两周）可以促成这种习惯，直到别人问你最近怎么样的时候，可以自发自动的首先想到一些好事。</p><p>告诉别人，你真的很感激。比如写感谢信，不要觉得尴尬，想象一下作为收信人的感受。</p><p>注意感恩的力量。</p><h5 id="8-healthy-practices-matter-more-than-we-expect"><a class="markdownIt-Anchor" href="#8-healthy-practices-matter-more-than-we-expect"></a> 8. Healthy practices matter more than we expect</h5><p>锻炼对身体好，对心理健康也好！具有很强的情绪价值。</p><p>好好睡觉也会提升幸福感，非常重要，坏的睡眠习惯甚至会导致抑郁。</p><h5 id="9-being-in-the-present-moment-is-the-happiest-way-to-be"><a class="markdownIt-Anchor" href="#9-being-in-the-present-moment-is-the-happiest-way-to-be"></a> 9. Being in the present moment is the happiest way to be</h5><p>活在当下，体察/品尝当下的你。<strong>冥想</strong>会有所帮助。</p><p>大部分人大部分时间在“心理徘徊”，这意味着我们失去了大部分人生，而且会降低幸福感。</p><p>个人体会是一边看剧一边玩游戏或一边刷知乎，都不会让人感到轻松。组会上划水也特别心累。</p><h5 id="10become-wealthy-in-time-not-in-money"><a class="markdownIt-Anchor" href="#10become-wealthy-in-time-not-in-money"></a> 10.Become wealthy in time not in money</h5><p>并不是客观上的，而是主观上认为自己时间充裕。<strong>追逐时间的人</strong>甚至比追逐金钱的人还要不幸福。</p><h2 id="线上课程"><a class="markdownIt-Anchor" href="#线上课程"></a> 线上课程</h2><p><a href="https://www.bilibili.com/video/BV16U4y1G7W5" target="_blank" rel="noopener">https://www.bilibili.com/video/BV16U4y1G7W5</a></p><p>知道并非是成功的一半。（GIJOY悖论）</p><p>对这些行为的跟踪记录会促进其成为习惯。</p><h4 id="品味savoring"><a class="markdownIt-Anchor" href="#品味savoring"></a> 品味（Savoring）</h4><p>可以阻止一味享乐主义，让我们保持当下的状态，并增加我们的感激之情。</p><p>参与到一项积极事务中，事后细细回味，思考它为什么会让你开心。可以通过拍照等记录手段帮助回味。</p><h4 id="感恩gratitude"><a class="markdownIt-Anchor" href="#感恩gratitude"></a> 感恩（Gratitude）</h4><p>改善自己的心情，减少压力，增强免疫，降低血压，可以体会到更深的社会联系。</p><p>写下值得感恩的事项，好好花时间体验你的感激之情。</p><h4 id="善举kindness"><a class="markdownIt-Anchor" href="#善举kindness"></a> 善举（Kindness）</h4><p>对他人的善举可以改善我们的心情，增加我们的社会联系感。</p><p>试着至少每天做一件随机的、积极的善事，比如捐款、社区志愿或者只是赞美他人。</p><h4 id="社会联系social-cconnection"><a class="markdownIt-Anchor" href="#社会联系social-cconnection"></a> 社会联系（Social Cconnection）</h4><p>社会联系使我们更快乐，比如在火车上、咖啡店里和陌生人交谈。</p><p>寻求更多的社会联系，比如和朋友聊聊天，和很久没见的人一起吃个饭，甚至是和陌生人交流</p><h4 id="锻炼exercise"><a class="markdownIt-Anchor" href="#锻炼exercise"></a> 锻炼（Exercise）</h4><p>多进行运动可以改善情绪，甚至有抗抑郁药物的作用，可以提高学习成绩。</p><p>不需要非常严肃的锻炼计划，可能只是健身半小时或者午餐后和朋友一起散步，在客厅里跳舞等等。</p><p>就是多动一点。</p><h4 id="睡眠sleep"><a class="markdownIt-Anchor" href="#睡眠sleep"></a> 睡眠（Sleep）</h4><p>充足的睡眠可以改善情绪，提高认知能力，还有很多其它健康方面的好处。</p><p>每天睡大概7到8个小时，记录你的睡眠时间和感觉。</p><h4 id="冥想meditation"><a class="markdownIt-Anchor" href="#冥想meditation"></a> 冥想（Meditation）</h4><p>活在当下，改善情绪，降低压力，提升智力，而且只需要几周的时间。</p><h3 id="一-对于幸福的误解"><a class="markdownIt-Anchor" href="#一-对于幸福的误解"></a> 一. 对于幸福的误解</h3><p>没有得到期望中的一份好的工作，其实并不会如我们想象的那样影响我们的幸福指数。</p><p><strong>金钱</strong>是否真的可以让我们感到快乐？收入越高，生活满意度越高么？这也与生活的环境（比如国家整体情况）有关。</p><p>一旦你的基础需求得以满足（在美国的调查大概是7.5万美元，而34岁的耶鲁大学毕业生的平均收入是7.6万美元），幸福指数与收入高低并不太相关。</p><p><strong>Awasome stuff</strong>是否真的可以让我们感到快乐？“If only I had XXXX, I can feel happy.”</p><p>对物质的思考、渴求以及其中的努力是否可以让我们更幸福？Seeking it makes us less happy.</p><p>调查发现物质主义者的生活满意度较低。</p><p><strong>真爱</strong>是否让我们感到快乐？</p><p>调查发现在结婚的头两年，已婚人士确实比未婚人士更幸福，但之后就趋同了。</p><p><strong>Having a perfect face/body/etc</strong>是否会带来幸福？</p><p>调查发现减肥并没有让人更开心。</p><p>接受整容实际上可能本身就是一个不好的信号，但同样的美貌并不会让我们更开心。</p><p><strong>Getting good grades</strong>是否可以呢？</p><p>分数高于/低于预期并不会对幸福感有太多的影响，当然还有有一点点，但是并没有那么重要。</p><p>为什么上述事物（基本上都是我们追求的目标）并没有让我们更幸福？</p><ul><li>也许你的幸福水平是由基因决定的（wrong）</li><li>也许是因为生活很糟糕，很多坏事会突然发生；生活环境非常重要；（wrong）</li><li>真正的原因是，基因占据了百分之五十，生活环境占据百分之十，而我们自己的行动或思想占据了百分之四十。</li></ul><p>本节结论是：</p><blockquote><p>There are things we can do to become happier.</p><p>But most of the goals we think will makes us happy don’t really make us happy.</p><p>We need to pick the right goals.</p></blockquote><h3 id="二-令人恼怒的特征"><a class="markdownIt-Anchor" href="#二-令人恼怒的特征"></a> 二. 令人恼怒的特征</h3><p>为什么我们总是期望一些并不能真的带来幸福的目标?</p><p>Miswanting，The act of being mistaken about what and how much you wil like something in the future.</p><p>Why does this miswanting occur? 大脑有一些令人恼怒的特征。</p><ul><li>很多时候我们大脑的<strong>直觉是错误</strong>的。</li><li>我们的思考不是绝对的，而是以一种<strong>相对的方式</strong>进行，总是包含着<strong>参考点</strong>，而这会误导我们的判断。比如有时银牌获得者不如铜牌获得者开心。与自己现有状态比，与其他人比（Social comparison）。有时人们会通过损己而不利人。我们需要合理设置自己的参考点。调查发现看电视多的人（看到更多高薪、光鲜的人）对自己的财富估计量减少，而消费量会增加。我们不仅有电视、杂志和媒体还有社交网络等可以影响我们参考点设置的因素。</li></ul><div align="center">  <img src="/2021/06/21/active/compare.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ul><li>我们的大脑生来就<strong>会习惯事物</strong>，会适应。比如知觉适应现象。所以美好的事物也不能为我们带来持久的快乐。</li><li>我们不会意识到我们的大脑习惯了某项事物，比如享乐适应症。我们对事物对我们幸福/快乐水平影响的强度和持久度的估计都是有偏差的。<strong>锚定效应</strong>、<strong>免疫性忽视</strong>（我们的复原能力其实非常强大）。会有<strong>很多错误预测</strong>。我们应该了解一下，我们其实有非常强大的复原能力可以面对失败、挫折和灾难，所以没有必要提前害怕消极结果和消极反馈。You‘re just gone to be fine.</li></ul><h3 id="三-如何克服上述问题"><a class="markdownIt-Anchor" href="#三-如何克服上述问题"></a> 三. 如何克服上述问题</h3><p>How can we overcome our biases? 我们可以培养一些习惯，直到可以下意识地使用它们。</p><p>策略一：停止对“好东西”上的投资，即停止物质主义。<strong>我们的体验和经历才是最有价值的</strong>。投资于体验而非投资于物质。提前想象这些体验也会让我们更开心。Investing in experiences makes us happier than investing in stuff.</p><p>策略二：跳出舒适圈</p><ul><li>Savoring，The act of stepping outside of an experience to review and appreciate it. 有一种活在当下的感觉</li><li>Negative visualization，负面可视化。比如情侣想象如果没有遇到对方的情况。</li><li>Make this day your last. 比如明天你就会从学校里毕业，有什么眷恋的地方呢？</li><li>Gratitude，比如每天写下你感激的3-5项事物，进一步可以分享出来，告诉对方，赞美对方</li></ul><p>策略三：重置参考点</p><ul><li>具体的<strong>再体验</strong>，比如重新体验（或者花七八分钟回顾一下即可）那些不好的境遇，然后重新对现有的境遇心怀感激；</li><li>具体的观察，真的去看一看（经常被美化的）过去的境遇，或者去试一试（经常被想象美化的）另一种选择；</li><li>避免社会比较，比如关闭朋友圈等社交app，停止技巧，或者感恩练习（人的注意力是有限的），更多地关注身边的真实的人和事；</li><li>干扰你的“享受”，不要一直做自己特别喜欢、特别沉醉的事，适当打断一下，将它们分散开。而对于你不喜欢的东西，把他们集中到一起做。这是对于“习惯效应”的一种应用。</li><li>让做的事增加一些多样性。</li></ul><h3 id="四-真正的幸福"><a class="markdownIt-Anchor" href="#四-真正的幸福"></a> 四. 真正的幸福</h3><h4 id="好的工作"><a class="markdownIt-Anchor" href="#好的工作"></a> “好”的工作</h4><p>是可以发挥你的优势的工作。另外，要在工作中找到“心流”的状态，要找到能力和任务难度之间的平衡。工作中的过多闲暇并不能让人更快乐。</p><div align="center">  <img src="/2021/06/21/active/char.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="好成绩"><a class="markdownIt-Anchor" href="#好成绩"></a> 好成绩</h4><p>出于外部激励还是内部动机。而外部激励可能会破坏内部动机，剥夺你最初始的热爱，也会破坏<strong>成长型思维</strong>（相对的是固有型思维）。</p><div align="center">  <img src="/2021/06/21/active/grow.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="友好的行为"><a class="markdownIt-Anchor" href="#友好的行为"></a> 友好的行为</h4><p>试着思考世上的一些善举；实际去做一些事；感恩其它人的善举。</p><p>把钱花在别人身上是否会提升我们的幸福感？是的，跨文化也一样。（但不要误会成施舍然后找到优越感那种…）我们不需要首先赚到了“足够的钱”才可以开始这项活动。</p><p>通过给予也可以加深与社会的联结感。</p><p>时间上的充裕感如何培养</p><h4 id="社会联系"><a class="markdownIt-Anchor" href="#社会联系"></a> 社会联系</h4><p>范围可以很广，比如与另一个人的眼神交流等小的互动、谈恋爱、或者参与组织社团等等，而社会孤立是非常恐怖的。</p><p>社会联系对于幸福的作用被我们低估了，很多人会主动回避社交。</p><p>比如与陌生人之间的关系，楼道里的邻里，火车的邻座等。</p><p>你更友善的情况下，会觉得别人更加友善。</p><p>每个人都有有趣的事情要讲。</p><h4 id="时间充裕"><a class="markdownIt-Anchor" href="#时间充裕"></a> 时间充裕</h4><p>有时间做自己想做的事情。</p><p>更重视时间而非金钱的人更容易快乐。比如少工作一点，少挣一点，而有更多的时间关注自己的需求和爱好。</p><h4 id="精神思维控制"><a class="markdownIt-Anchor" href="#精神思维控制"></a> 精神/思维控制</h4><p>走神容易降低幸福感。</p><p>冥想可以改善这个情况。</p><h4 id="健康实践"><a class="markdownIt-Anchor" href="#健康实践"></a> 健康实践</h4><p>某个调查表明，每周三次，每次30分钟的锻炼比抗抑郁药的效果还好。</p><p>运动有助于提升认知能力。</p><p>健康的作息与充足的睡眠！！！睡眠非常非常非常重要！不要舍本逐末！</p><div align="center">  <img src="/2021/06/21/active/sleef.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="五-制定目标"><a class="markdownIt-Anchor" href="#五-制定目标"></a> 五. 制定目标</h3><p>认清自己的目标，非常具体化的目标，需要<strong>精准量化</strong>。</p><p>思考这个目标能够带来什么，以及实现过程中可能会出现什么问题，即<strong>需要克服的困难</strong>。 不要只是幻想美梦，要正视其中的阻碍。这会形成一种心理对比。</p><p>Goal Planning，<strong>实施意图</strong>，即“如果-那么”计划，首先在头脑中写下“if-then”逻辑，比如如果我在食堂里看到披萨，我会转过去拿一个苹果。</p><p>总结起来就是：(WOOP方法)</p><ul><li>Think about your wish</li><li>The <strong>best</strong> outcome，想象中的那份最好的结果</li><li>Potential obstacles，what stops me，<strong>what it really is</strong>, what is in me. 究竟是什么真正阻碍了我的发展</li><li>Your IF/THEN plan</li></ul><p>这个过程中目标或者说wish会变化，是一个搜索的过程，直到发现心中<strong>真正想要的，最好的结果</strong>。</p><p>得到真正的结果，而<strong>不要逃避</strong>。过程中你不可有执念，要懂得letting go一些不可行的“期望”（比如想一上来就可以做到一周七次的冥想或健身）</p><p>认识真正的障碍这一步是非常关键的！</p><p>学会<strong>处理/利用过程中得到的批评或负面反馈</strong>，而不是简单地将它们视为一种对自身的评价/论断。</p><p>WOOP changes with life, you don’t need to be perfect. Don’t go back to your old wishes.</p><p>可以通过以下两个策略帮助形成习惯：</p><p>1）Situation support，寻找可以提供支持的情境，让自己远离诱惑，而非认为自己有“强大的意志力”。</p><p>2）Promote healthy environment，通过长期坚持的小习惯吸引自己的注意力，比如每早冥想，感恩tips；或者找一些同伴一起。</p><h3 id="六-总结与复习"><a class="markdownIt-Anchor" href="#六-总结与复习"></a> 六. 总结与复习</h3><p>我们对可以带来幸福的因素/事物的预测/直觉有很多误区。</p><p>我们需要打破上述偏见，比如重置参考点，打破惯性和享乐适应，明白我们有错误需求（miswanting）。</p><p>制定目标的时候建议多考虑的一些东西，比如：变得更加友善、增进社交、寻找时间充裕感、控制意念和健康实践。</p><div align="center">  <img src="/2021/06/21/active/five.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="终极重塑挑战"><a class="markdownIt-Anchor" href="#终极重塑挑战"></a> 终极重塑挑战</h4><p>所有学到的东西都需要实践！知道与做到之间还有一道鸿沟。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TUM《图与时序数据机器学习》笔记</title>
    <link href="/2021/06/18/tum-mlgs/"/>
    <url>/2021/06/18/tum-mlgs/</url>
    
    <content type="html"><![CDATA[<p>最开始由于“Certified Robustness”部分而开始关注到这个课程，课程官网（<a href="https://www.in.tum.de/daml/teaching/mlgs/%EF%BC%89%EF%BC%8C%E7%94%B1%E4%BA%8E%E7%96%AB%E6%83%85%E9%A6%96%E6%AC%A1%E7%BA%BF%E4%B8%8A%E5%BC%80%E6%94%BE%E3%80%82TUM" target="_blank" rel="noopener">https://www.in.tum.de/daml/teaching/mlgs/），由于疫情首次线上开放。TUM</a> Stephan Günnemann大佬团队出品（好想有机会去这个课题组访问学习一段时间）。</p><p>由标题可以看出，这个课程专注于<strong>non-IID（非独立同分布）数据</strong>，主要分为两大类，即时序数据（temporal data / sequence）和图数据（graphs/networks）。其中，时序数据的non-IID主要体现在当前数值依赖于过去取值，而图数据的non-IID体现在结构信息部分。</p><h2 id="生成模型"><a class="markdownIt-Anchor" href="#生成模型"></a> 生成模型</h2><h2 id="鲁棒性分析"><a class="markdownIt-Anchor" href="#鲁棒性分析"></a> 鲁棒性分析</h2><h3 id="一-背景与问题抽象"><a class="markdownIt-Anchor" href="#一-背景与问题抽象"></a> 一. 背景与问题抽象</h3><p>我们出于以下两个方面考虑而关注模型鲁棒性：1）Real-world risks：对抗样本会给ML模型的实际应用带来威胁，比如自动驾驶领域。而且现实世界中也存在对抗样本，比如3D打印、特殊眼镜等；2）Conceptual gaps，我们原以为模型已经像人一样学习到了数据中的重要语义信息（但对抗样本的存在反证了这一点），而且我们希望了解在worst-case noise下模型的鲁棒性如何。</p><blockquote><p>Small (imperceptible) but specifically crafted perturbations lead to false predictions in ML models.</p></blockquote><p>如何定义对抗样本的“imperceptible”？</p><ul><li>不改变原始样本的语义信息（semantic content）</li><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>范数较小，通常考虑<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>L</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_1,L_2,L_{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</li></ul><p>几个重要概念：evasion attack，posion attack，targeted attack，untargeted attack。</p><h3 id="二-对抗样本生成"><a class="markdownIt-Anchor" href="#二-对抗样本生成"></a> 二. 对抗样本生成</h3><p>对抗样本生成可以抽象为一个优化问题。<strong>第一种方式</strong>如下：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/p1.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>一般使用交叉熵函数为Loss函数。使用<a herf="https://freemind.pluskid.org/machine-learning/projected-gradient-method-and-lasso/">PGD（Projected Gradient Descent）</a>求解此问题。</p><p>整个过程和训练模型类似，只是在这里每次迭代更新的是输入x，而非模型参数w。而模型函数相对其参数是非凸的，所以通常不会找到全局最优解。</p><p>另外还有经典的FGSM算法（with only a single step without projection），使用梯度方向而非梯度来进行参数更新。</p><p><strong>第二种方式</strong>如下，即寻找与原样本距离最小但模型预测结果不同的样本点。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/p2.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>论文[Carlini and Wagner, 2017]将上述问题改写为无约束优化问题：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/p3.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>一种比较有效的损失函数为：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/loss.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>### 三. 鲁棒性提升<p>到目前我们知道ML模型容易受到对抗样本的影响。那么如何进行防御？如何提升模型鲁棒性呢？</p><p>到目前为止一些不太可行的研究角度：</p><ul><li><p><strong>Post-hoc prevention of attacks（事后防御</strong>），比如梯度混淆（gradient obfuscation），随机化或粉碎（shattering）模型的梯度。但这类防御手段事后基本都被更厉害的攻击手段攻破。</p></li><li><p>对抗样本检测，比如out-of-distribution shift，通过数据样本的分布情况检测出对抗样本，此类防御手段可以检测出比较朴素的PGD攻击，但是对targeted attacks基本无效。</p><blockquote><p>Fixing a “bad” model seems not to be the solution.</p></blockquote></li></ul><p>比较可行的方向是直接构建鲁棒的模型，比如使用<strong>鲁棒训练（Robust training）</strong>，本质上是优化最差情况下的损失（the loss achieved under the worst-case perturbation, also called robust loss）。</p><blockquote><p>Robust training refers to training procedures aimed at producing models that are robust to adversarial (and/or other) perturbations.</p><p>A common theme is to optimze a ‘worst-case’ loss</p></blockquote><p>相较于传统的训练方式，鲁棒训练除关注样本x自身的预测结果外，<strong>还关注其周围样本点的预测情况</strong> (sup表示取范围内的最大值)，保证方圆几里都被打上同一标签。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/rb.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>进行鲁棒训练有两种方式：<strong>Adversarial training</strong>（对抗训练）和<strong>robustness certification</strong>（可验证鲁棒性分析）。</p><h3 id="四-对抗训练"><a class="markdownIt-Anchor" href="#四-对抗训练"></a> 四. 对抗训练</h3><p>求解时需要对上述<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>r</mi><mi>o</mi><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{rob}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>使用梯度下降法。而为了计算红框中的值，使用<strong>Danskin’s theorem</strong>证明，如果可以找到worst-case扰动，就可以解决问题。这里使用对抗样本指代worst-case。所以，<strong>对抗训练就是使用对抗样本代表最坏的扰动情况</strong>（use adverserial examples as a proxy for the ‘worst-case’ perturbation）。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/dt.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>Danskin定理的意思是，如果最大值有对应的唯一x取值，那么sup的梯度可以等价于在最大值对应的x处的梯度。但我们应该怎么找到最差情况下的x取值呢？这个任务其实也是NP难的（在可验证鲁棒性部分会细讲）。所以训练过程中我们选择使用对抗样本来替代。<p>总结下来，对抗学习的流程如下：</p><ul><li>在原始数据中采样样本（x, y）;</li><li>进行对抗攻击，找到带来最大损失的扰动样本x’；（这一步包含计算量与攻击强度的trade off）</li><li>使用扰动样本进一步训练模型，更新参数值。</li></ul><p>对抗训练确实可以（经验性地，empirically）提升模型鲁棒性，而且很容易实现，但是存在如下缺点：</p><ul><li>由于中间步骤还包含对抗攻击，所以对抗训练比标准训练要慢很多，大概10倍左右；</li><li>得到的模型虽然鲁棒性提升了，但它在原本“干净”的数据集上的表现有所下降；</li><li>并没有得到有关该模型鲁棒性的<strong>理论性分析与保障</strong>。</li></ul><h3 id="五-可验证鲁棒性"><a class="markdownIt-Anchor" href="#五-可验证鲁棒性"></a> 五. 可验证鲁棒性</h3><p>本质思想是证明在某范围的扰动之内（radius measured by some norm）分类器的预测结果不会变化。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/cr.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>**精准验证（Exact verification）**的意思是当且仅当在该输入扰动范围内没有对抗样本的时候，返回YES。</p><p>求解此问题，<strong>最大的复杂性来自于ReLU</strong>。研究[Katz <a href="http://et.al" target="_blank" rel="noopener">et.al</a> 2017]表明在L_max扰动下针对应用了ReLU激活函数的神经网络<strong>进行确定性鲁棒性分析</strong>是<strong>NP难问题</strong>。</p><p>上述问题可以转化为如下表示形式，m为margin的缩写，即样本分类为c（原始预测结果）和分类为t的概率的差别。如果最坏情况下的margin（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>m</mi><mi>t</mi><mo>∗</mo></msubsup></mrow><annotation encoding="application/x-tex">m_t^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.935696em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>）依然大于零，则可以验证模型鲁棒；否则应该存在对抗样本。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/wcm.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>该问题可进一步拆解（将模型拆开）为如下形式：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/opt.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>更形象化的表达如下图所示：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/robust.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/18/tum-mlgs/not.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="1-混合整形线性规划"><a class="markdownIt-Anchor" href="#1-混合整形线性规划"></a> 1. 混合整形线性规划</h4><p>第一种方法是<strong>MILP（Mixed Integer Linear Programming）</strong>，即有些线性规划条件限制变量是整数，而另一些没。注意，MILP属于精准验证的一种，依然是NP难问题，但可以通过一些近似技巧，对中小型网络进行鲁棒性验证。</p><p>将上述优化问题转为MILP形式需要处理两个地方：1）输入X的扰动范围；2）ReLU函数。</p><p>研究[Tjeng <a href="http://et.al" target="_blank" rel="noopener">et.al</a> 2019]通过<strong>引入输入x的上下界[l,u]<strong>将上述优化问题中的ReLU约束部分</strong>改写</strong>为如下形式。可验证当x分别取大于0和小于0的值时，y值被约束在ReLU形状内。最终问题转化为如下格式：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/relu.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>该方法的关键是计算出每层ReLU的输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">x^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span>的取值范围<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>u</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[l^{(l)},u^{(l)}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>。可以根据如下公式，依次计算出每一层的上下界范围（loose）。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/bound.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如果下界也大于0，则称为“stably active”，而如果上界也小于0，则称为“stably inactive”，这两种情况（stable）下都可以把ReLU相关的约束条件删除，提升求解速度。</p><blockquote><p>Tightness of the upper and lower bounds has no influence on the correctness.</p><p>Tighter bounds lead to more stable units and greatly speed up the optimization.</p></blockquote><h4 id="2-凸松弛convex-relaxation"><a class="markdownIt-Anchor" href="#2-凸松弛convex-relaxation"></a> 2. 凸松弛（Convex Relaxation）</h4><p>由于存在ReLU限制条件，计算最坏情况下的margin（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>m</mi><mi>t</mi><mo>∗</mo></msubsup></mrow><annotation encoding="application/x-tex">m_t^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.935696em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>）是NP难问题。我们可以松弛此优化问题，<strong>计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>m</mi><mi>t</mi><mo>∗</mo></msubsup></mrow><annotation encoding="application/x-tex">m_t^{*}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.935696em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>的下界</strong>以达到多项式时间。如果下限大于零则可确认鲁棒，但代价是某些情况下无法确定。</p><blockquote><p>If returns YES, there are no adversarial examples within an e ball around the input sample;</p><p>If returns POTENTIALLY NOT, there might be adversarial examples or it is adversarial-free.</p></blockquote><div align="center">  <img src="/2021/06/18/tum-mlgs/convex.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>松弛的方式如下所示，将ReLu转化为一个三角区域。给定输入X，Y值不再确定。其实下面的形式与MILP中加入的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>参数等价，只不过将a的取值由0或1，松弛为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">a\in[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/relax.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>整个优化问题转为如下形式，成为一个线性规划问题（LP），可在多项式时间内求解：</p><div align="center">  <img src="/2021/06/18/tum-mlgs/lp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>因为上述优化问题的解和限制条件都和模型参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>有关，所以以上思想也可以<strong>指导鲁棒训练</strong>，找到更“好”的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>。鲁棒训练中的sup值可以等价为worst case的margin值。但是margin值需要通过求解LP问题才能得到，难以进行梯度计算。而<strong>基于强对偶（strong duality）思想</strong>，我们可以直接采用其对偶问题中的任意值作为supremum，更容易计算梯度。（对抗训练中的思想是，用对抗样本替代supremum）</p><div align="center">  <img src="/2021/06/18/tum-mlgs/rt.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><blockquote><p>有关对偶问题与KKT条件，<a href="https://www.cnblogs.com/90zeng/p/Lagrange_duality.html" target="_blank" rel="noopener">https://www.cnblogs.com/90zeng/p/Lagrange_duality.html</a></p></blockquote><h4 id="3-lipschitz连续"><a class="markdownIt-Anchor" href="#3-lipschitz连续"></a> 3. Lipschitz连续</h4><p>本质是研究输入的变动导致输出变动的范围，即计算神经网络F的利普希兹常量。</p><div align="center">  <img src="/2021/06/18/tum-mlgs/lc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>可以先计算每层网络的利普希兹常量，之后相乘得到整个网络的利普希兹常量上界。</p><p><strong>全连接网络Lipschitz常量计算</strong></p><p>全连接网络可以表达为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">f(x) = Wx + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>，将其带入Lipschitz算式，可以得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">(</mo><mi>W</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub><mo>≤</mo><mi>k</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">||(Wx_1+b)-(Wx_2+b)||_p \leq k||x_1-x_2||_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>。带入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">a=x_1-x_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>得到：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>u</mi><msub><mi>p</mi><mrow><mi>a</mi><mi mathvariant="normal">≠</mi><mn>0</mn></mrow></msub><mfrac><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>W</mi><mi>a</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub></mrow><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>a</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">L(f) = sup_{a\neq0}\frac{||Wa||_p}{||a||_p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.399108em;vertical-align:-0.972108em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">u</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="rlap mtight"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel mtight"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">a</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">a</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.972108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><div align="center">  <img src="/2021/06/18/tum-mlgs/norm.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>不同范数下得到的Lipschitz常量不同，具体可以参考[Gouk+ 2018]。</p><p><strong>激励函数Lipschitz常量计算</strong></p><p>基本上大多常用的激活函数（比如ReLU、sigmoid、tanh、softmax等）的Lipschitz常数都不大于1。<strong>ReLU的等于1</strong>。</p><p>所以一个使用ReLU的前向神经网络，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">)</mo><mo>≤</mo><msubsup><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>W</mi><mi>l</mi></msub><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L(F)\le \prod_{l=1}^L||W_l||_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2809409999999999em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>。</p><p>因此我们可以通过正则化模型参数范数，强行控制模型的Lipschitz常数，即控制输出受输入的影响变化范围。或者反过来进行可验证鲁棒性分析。具体步骤如下：</p><ul><li>在logits中计算可以使分类结果变动的最小扰动值；（logit space）</li><li>根据Lipschitz常数，计算产生上述logits扰动的最小输入扰动值。(input space)</li></ul><p>最终保证输入扰动在范围内<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>∈</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>:</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>−</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub><mo>≤</mo><mfrac><mover accent="true"><mi>ϵ</mi><mo>^</mo></mover><mi>k</mi></mfrac></mrow></mrow><annotation encoding="application/x-tex">{\hat x \in P(x) = {\hat x :||x-\hat x||_p \le \frac {\hat \epsilon}{k} }}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mathdefault mtight">ϵ</span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mtight">^</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span>则样本x的预测值不会变化，结果鲁棒。</p><p>我们希望神经网路有比较小的Lipschitez常数，但这也限制了模型的表达能力，所以需要 trade off expressiveness and robustness。</p><blockquote><p>Moreover, [Huster+ 2018] argue that there are theoretical limitations in using only atomic (i.e. layer-wise) Lipschitz constants to upper bound k.</p></blockquote><h4 id="4-随机平滑randomized-smoothing"><a class="markdownIt-Anchor" href="#4-随机平滑randomized-smoothing"></a> 4. 随机平滑（Randomized Smoothing）</h4><p>通过在输入中随机加入噪声，并预测这些样本中<strong>占多数的类别</strong>，将原始模型转化为平滑模型。</p><blockquote><p>Transform any given base classifier into a smoothed classifier by randomly adding noise (e.g. Gaussian) to the input and predicting the majority class given many samples.</p></blockquote><h2 id="序列数据"><a class="markdownIt-Anchor" href="#序列数据"></a> 序列数据</h2><h2 id="图数据"><a class="markdownIt-Anchor" href="#图数据"></a> 图数据</h2><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅《机器/深度学习》2021笔记（三）</title>
    <link href="/2021/06/17/lhy-2021-c/"/>
    <url>/2021/06/17/lhy-2021-c/</url>
    
    <content type="html"><![CDATA[<h2 id="强化学习"><a class="markdownIt-Anchor" href="#强化学习"></a> 强化学习</h2><h3 id="1-基础概念"><a class="markdownIt-Anchor" href="#1-基础概念"></a> 1. 基础概念</h3><p>常用名词包括state、actor、action、environment、reward、return、trajectory（包含状态与动作的交互过程）等。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/rl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>对于计算出的action结果，依照其概率随机采样。</p><p>RL依然可以视为符合传统ML框架的三个阶段，但是模型存在随机性（actor行动选择部分，env和reward都是黑盒），所以在训练过程中的优化方法有所不同。</p><h3 id="2-policy-gradient"><a class="markdownIt-Anchor" href="#2-policy-gradient"></a> 2. Policy Gradient</h3><p>RL与传统ML分类问题的区别在于：1）当前动作会影响到之后的环境以及后续rewards；2）Reward delay，有时候需要牺牲短期利益换取长期收益。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/pc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>训练RL可以有如下几种思路：</p><ul><li>Version 0：直接用当前action在当前state下带来的reward作为label进行训练。但这种方法无法考虑长期策略，比如玩射击游戏时，只开火不移动。</li><li>Version 1：使用cumulated reward而非仅考虑当前步骤的reward。但这种方法过多强调了早期action的重要性。</li><li>Version 2：在计算cumulated reward的时候不是单纯加和而是加入discount factor，逐步减少早期action的重要性。</li><li>Version 3：对cumulated reward进行标准化，比如所有值都减去一个参数b，让最终值有正有负。</li></ul><p>需要注意的是，在policy gradient的训练过程中，需要同步收集数据，每更新模型，重新收集一次数据。所以训练很耗时。这种训练叫做on-policy learning。</p><p>另外也是存在off-policy learning方式，即收集数据时的actor和训练中的actor不同，比较经典的方法是<strong>PPO（Proximal Policy Optimization）</strong>。</p><p>在收集训练数据的时候，使用Exploration方式，故意提升actor采取行动的随机性，有很多相应的技巧。</p><h3 id="3-actor-critc"><a class="markdownIt-Anchor" href="#3-actor-critc"></a> 3. Actor-Critc</h3><p>Critc评判actor在s下的效果。使用value function估测在s下actor <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>的cumulated reward。</p><p>一种是使用基于蒙特卡洛的方法训练critic，直接完成整轮拿到结果，训练Value function；</p><p>第二种是使用基于temporal-difference的方法，解决有些“游戏”很难终止的问题，使用t+1和t两者之间的差别来训练Value function。</p><p>这两种方法可能算出来的Value function会不同。</p><ul><li>Version3.5：之后使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>G</mi><mo mathvariant="normal">′</mo></msup><mo>−</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G&#x27;-V(s_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.835222em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>衡量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">{</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{s_t,a_t\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>的好坏。</li><li>Version 4：使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><msup><mi>V</mi><mi>θ</mi></msup><mo stretchy="false">(</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>−</mo><msup><mi>V</mi><mi>θ</mi></msup><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r_t+V^\theta((s_{t+1})-V^\theta(s_t))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>衡量，这个方法叫做“advantage actor-critic”</li></ul><div align="center">  <img src="/2021/06/17/lhy-2021-c/v3.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/06/17/lhy-2021-c/v4.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>实际实现的时候，发现其实actor和critic是可以共享参数的。</p><p>另外还可以直接使用critic做，比如<strong>DQN</strong>。有一个知名的文章叫做Rainbow。</p><h3 id="4-reward-shaping"><a class="markdownIt-Anchor" href="#4-reward-shaping"></a> 4. Reward shaping</h3><p>如果大多数情况下的reward都是0怎么办？也就是sparse reward问题。</p><p>要想办法提供额外的reward引导agent学习，也就是reward shaping。</p><h3 id="5-learning-from-demonstration"><a class="markdownIt-Anchor" href="#5-learning-from-demonstration"></a> 5. Learning from Demonstration</h3><p>有的时候连reward也没有用，让机器自己来定义（inverse reinforcement learning）。从expert的示范中学习到reward function。</p><p>基本概念是：Teacher is always the best。这个过程中可以把actor想象为generator，把reward function想象为discriminator。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/irl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="终身学习life-long-learning"><a class="markdownIt-Anchor" href="#终身学习life-long-learning"></a> 终身学习（Life Long Learning）</h2><p>还有Continuous Learning, Never Ending Learing, Incremental Learning等名称。</p><p>比如要解决Task1和Task2，一般模型可以同时比较好地学习并解决两个模型，但无法串行学习，会出现学会任务二后忘掉了任务一的现象。有种狗熊掰棒子的感觉，<strong>Catastrophic Forgetting</strong>（灾难性遗忘）。</p><p>由于计算资源和训练时间的限制，我们不能直接用<strong>Mutli-task training</strong>（即把所有任务的数据全部倒在一起同时训练）来解决这一问题。科研中一般把mutli-task training的结果当做life long learning的上限。</p><p>另外，和<strong>迁移学习</strong>的关注点也不同。迁移学习重点在于，由于已经学习过任务1，所以也可以做任务2或者简化任务2的训练过程。而终身学习的重点在于，保证即便学习了任务2，模型也不会忘记如何解决任务1。</p><p>Life long learning的模型评估一般使用多个task的表格，计算accuracy，backward transfer和forward transfer。</p><h4 id="1-selective-synaptic-plasticityregularization-based-approach"><a class="markdownIt-Anchor" href="#1-selective-synaptic-plasticityregularization-based-approach"></a> 1. Selective Synaptic Plasticity（Regularization-based Approach）</h4><p>每个参数对过去学过的任务的重要性不同，希望学习新任务时这些重要的参数不要变化太大。</p><p>给每个参数一个“保镖”<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，表示该参数对过去任务的重要程度。将训练任务的损失函数改为如下形式：</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/lf.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>研究的关键在于如何设定<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。如果设定过大会有intransigence问题，就是新任务学不起来。</p><p>之前还有一个名为**Gradient Episodic Memory（GEM）**的方法，修改参数更新方向。这个方法需要存储过去task上模型的资料，这一点和终身学习的初衷有些偏离。</p><h4 id="2additional-neural-resource-allocation"><a class="markdownIt-Anchor" href="#2additional-neural-resource-allocation"></a> 2.Additional Neural Resource Allocation</h4><p>Progressive Neural Networks，如下图所示，针对新的task新增模型。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/pnn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>PackNet，最开始的时候就开一个大的网络模型，分部分给不同的task使用。</p><p>CPG，有点像上述两个思想的结合。</p><h4 id="3-memory-reply"><a class="markdownIt-Anchor" href="#3-memory-reply"></a> 3. Memory Reply</h4><p>训练生成器，每次产生一些符合原有任务的伪数据。生成器占用的空间比存储先前数据占用的空间小。实验表明这种方法非常有效。</p><p>如果不同任务的样本类别不同如何解决？</p><p>另外，更换学习不同任务的顺序可以改变模型的训练效果，有些任务就没有遗忘问题。研究这方面策略的方向叫做“Curriculum Learning”。</p><h2 id="元学习"><a class="markdownIt-Anchor" href="#元学习"></a> 元学习</h2><p>让机器学习如何学习。</p><p>通常当前的few-shot learning的模型是通过元学习方法得到的。</p><p>一些专有名词：Across-task training/testing，Within-task training/testing，</p><h4 id="1-maml"><a class="markdownIt-Anchor" href="#1-maml"></a> 1. MAML</h4><p>比较难训练，有进阶版本MAML++（《How to train your MAML》）。</p><p>自监督学习中的Pre-training与它有比较大的差别。</p><p>MAML效果好的假设有两个：1）初始参数可以很快地找到最好模型（rapid learning）；2）初始参数原本就与最终最好的模型参数非常接近了（feature reuse）。有研究认为是后者：</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/maml.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>大幅度简化运算的变形FOMAML（First order maml）。后续改进版本还有Reptile。</p><h4 id="2-优化器"><a class="markdownIt-Anchor" href="#2-优化器"></a> 2. 优化器</h4><p>自动根据训练任务学习直接学习最佳优化器。</p><p>《Learn to Learn by gradient descent》NIPS 2016</p><h4 id="3-nasnetwork-architecture-search"><a class="markdownIt-Anchor" href="#3-nasnetwork-architecture-search"></a> 3. NAS（Network architecture search）</h4><p>训练得到网络架构。</p><p>无法计算微分，可以用RL硬做，agent的输出是各种网络架构选项。或者使用evolution algorithm。</p><p>另外有一个经典的算法叫DARTS，硬要网络架构参数可以微分。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/lft.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4data-augmentation"><a class="markdownIt-Anchor" href="#4data-augmentation"></a> 4.Data Augmentation</h4><p>模型自动寻找数据增强方法。</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/da.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="5sample-reweighting"><a class="markdownIt-Anchor" href="#5sample-reweighting"></a> 5.Sample reweighting</h4><p>如何决定边界线上的数据点的权重问题，是提升权重，还是当做噪声点而降低权重？</p><div align="center">  <img src="/2021/06/17/lhy-2021-c/sw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="6beyond-gradient-descent"><a class="markdownIt-Anchor" href="#6beyond-gradient-descent"></a> 6.Beyond gradient descent</h4><p>放弃梯度下降，直接通过数据学习参数。</p><p>《Meta-learning with latent embedding optimization》ICLR 2019</p><h4 id="7-metric-based-approach"><a class="markdownIt-Anchor" href="#7-metric-based-approach"></a> 7. Metric-based approach</h4><p>把训练、测试数据都喂给数据集，看看学习效果如何。</p><p>一般元学习用在few shot learning上，N way k shot。</p><p>当下MAML也用到NLP、语音识别、<strong>知识图谱</strong>（<a href="http://speech.ee.ntu.edu.tw/~tlkagk/meta_learning_table.pdf%EF%BC%89%E7%AD%89%E8%BE%83%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E4%B8%8A%E3%80%82" target="_blank" rel="noopener">http://speech.ee.ntu.edu.tw/~tlkagk/meta_learning_table.pdf）等较复杂任务上。</a></p><h2 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> 结语</h2><h2 id="相关研究"><a class="markdownIt-Anchor" href="#相关研究"></a> 相关研究</h2><h3 id="1-终身学习b_i设置"><a class="markdownIt-Anchor" href="#1-终身学习b_i设置"></a> 1. 终身学习<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>设置</h3><p>EWC, <a href="https://arxiv.org/abs/1612.00796" target="_blank" rel="noopener">https://arxiv.org/abs/1612.00796</a></p><p>SI, <a href="https://arxiv.org/abs/1703.04200" target="_blank" rel="noopener">https://arxiv.org/abs/1703.04200</a></p><p>MAS, <a href="https://arxiv.org/abs/1711.09601" target="_blank" rel="noopener">https://arxiv.org/abs/1711.09601</a></p><p>RWalk, <a href="https://arxiv.org/abs/1801.10112" target="_blank" rel="noopener">https://arxiv.org/abs/1801.10112</a></p><p>SCP, <a href="https://openreview.net/forum?id=BJge3TNKwH" target="_blank" rel="noopener">https://openreview.net/forum?id=BJge3TNKwH</a></p><h3 id="2-如何处理增加了标签类别"><a class="markdownIt-Anchor" href="#2-如何处理增加了标签类别"></a> 2. 如何处理增加了标签类别</h3><p>Learning without forgetting（LwF）</p><p>iCARL：Incremental Classifier and Representation Learning</p><p>Life long learning的三个情景，<a href="https://arxiv.org/abs/1904.07734" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07734</a></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅《机器/深度学习》2021笔记（二）</title>
    <link href="/2021/06/16/lhy-2021-b/"/>
    <url>/2021/06/16/lhy-2021-b/</url>
    
    <content type="html"><![CDATA[<h2 id="自督导式学习"><a class="markdownIt-Anchor" href="#自督导式学习"></a> 自督导式学习</h2><p>从资料中抽取两部分，一部分作为模型输入，一部分作为目标输出。Self-supervised可视为无监督学习方法的一种，这个概念最早于2019年提出。</p><p>相关模型有：ELMo（94M参数），BERT（340M参数），ERNIE，Big Bird，GPT-2（1500M参数），Megatron（ 8B），T5（11B），Turing NLG（17B），GPT-3（10倍图灵），Switch Transformer（1.6T）。</p><h3 id="1-bert"><a class="markdownIt-Anchor" href="#1-bert"></a> 1. Bert</h3><p>主要结构是<strong>Transformer Encoder</strong>。训练是主要进行两个任务：1）随机mask输入的token，可以替换为特殊符号或者随机值；2）下一句子预测（进阶版ALBERT使用的是SOP，sentence order prediction）。</p><p>使用上述“填空题”训练BERT，在经过Fine-tune之后，可应用到下游任务中。而之前的BERT训练过程称为Pre-train。整个过程合起来算是一种semi-supervised方法。</p><p>评估自监督模型的常用任务集，GLUE（General Language Understanding Evaluation），包含九个任务。</p><p>下游任务包括：情感分析（Linear模型部分随机初始化，BERT部分在之前参数基础上更新）、词性标注、自然语言推理（前提与假设）、问答系统。</p><p>BERT可以视为Deep版的CBOW，而且可以根据上下文情况得到多义字不同的表征向量，Contextualized word embedding。</p><h3 id="2-扩展工作"><a class="markdownIt-Anchor" href="#2-扩展工作"></a> 2. 扩展工作</h3><p>BERT训练过程需要耗费非常多的资源，所以大家致力于研究&quot;BERT胚胎学&quot;，研究训练中模型的发展过程。</p><p>Pre-train seq2seq模型，将一些故意“弄坏”的输入给Encoder，要求Decoder输出原本完好的数据。</p><p>论文BART研究使用了很多种“弄坏”输入的训练方法，表明组合起来效果比较好。</p><p>论文T5，在C4数据集上训练。</p><p>多语言Bert，multi-bert，可以进行zero-shot阅读理解。</p><h3 id="3-gpt系列模型"><a class="markdownIt-Anchor" href="#3-gpt系列模型"></a> 3. GPT系列模型</h3><p>预测下一个token，类似于Transformer的Decoder。所以GPT有生成能力（独角兽形象，因为写了一个有关独角兽的假新闻）。</p><p>直接有Few-shot learning、one-shot learning、zero-shot learning。</p><p>另外还有很多类型的自监督学习模型，应用在更多不同领域中，如下图所示：</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/self.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="自编码器"><a class="markdownIt-Anchor" href="#自编码器"></a> 自编码器</h2><p>李老师认为autoencoder可以视为self-supervised中pre-train的一种，因为同样也是没有应用到标签数据。</p><p>Autoencoder流程和之前提到的Cycle GAN类似，常见作用是降维。</p><p>De-noising Autoencoder，设计思想和Bert中的mask很类似。Bert可以视为一种De-noising Autoencoder。</p><p>Feature Disentangle技术，即有可能知道autoencoder中每个维度代表了什么信息。可以应用到语者转换（变声器）等领域。比如可以支持双方语者间不需要相同语料。最代表性的工作是<strong>VQVAE</strong>，可以学习到最基本的发音单位，也可以做到文章摘要。</p><p>Decoder可以视为一个生成器，比如VAE</p><p>Autoencoder可以用来做压缩与解压，lossy compression。</p><p>重点应用场景还有<strong>异常检测</strong>，比如欺诈检测（<a href="https://www.kaggle.com/mlg-ulb/creditcardfraud/home%EF%BC%89%EF%BC%8C%E7%BD%91%E7%BB%9C%E5%85%A5%E4%BE%B5%E6%A3%80%E6%B5%8B%EF%BC%88http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html%EF%BC%89%EF%BC%8C%E7%99%8C%E7%97%87%E7%BB%86%E8%83%9E%E6%A3%80%E6%B5%8B%E7%AD%89%E3%80%82%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E5%A4%A7%E5%A4%9A%E6%95%B0%E6%95%B0%E6%8D%AE%E9%83%BD%E6%98%AF%E6%AD%A3%E5%B8%B8%E6%95%B0%E6%8D%AE%E3%80%82" target="_blank" rel="noopener">https://www.kaggle.com/mlg-ulb/creditcardfraud/home），网络入侵检测（http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html），癌症细胞检测等。数据集中大多数数据都是正常数据。</a></p><h2 id="对抗攻击"><a class="markdownIt-Anchor" href="#对抗攻击"></a> 对抗攻击</h2><h3 id="1-攻击方式"><a class="markdownIt-Anchor" href="#1-攻击方式"></a> 1. 攻击方式</h3><p>超出范围的噪声值直接拉回范围内fix就好。</p><p>代表方法FGSM，只使用一次迭代，在这次迭代中并不直接使用梯度进行更新，而是取更新方向（一击必杀）。也有iterative FGSM。</p><p>白盒攻击</p><p>黑盒攻击，可以训练一个proxy模型，模仿原本的攻击对象。如果连训练资料都没有的话，就自备数据，获取其检测结果。实验中黑箱攻击效果还不错，target attack会相对比较难一点。</p><p><strong>Ensemble Attack</strong>技巧</p><p>为什么攻击很容易成功？《Adverserial Examples are not bugs, they are features》部分人认为可能问题不是出现在模型上而是数据上。</p><p>One pixel attack，只修改图片里的一个像素点就完成攻击。</p><p>Universal attack，所有的图片都可以用着一个噪声完成攻击。</p><p>Adversarial Reprogramming，操控模型去做本来不是他想做的事情。</p><p>Backdoor，在模型中开后门，从训练阶段就开始攻击</p><h3 id="2-防御方式"><a class="markdownIt-Anchor" href="#2-防御方式"></a> 2. 防御方式</h3><p><strong>被动防御</strong>，比如smoothing（模糊化），compression（压缩后解压），Generator（重新产生输入数据）、Randomization等。</p><p>但攻击者了解这些防御措施后，这些措施就会失效。</p><p><strong>主动防御</strong>，adversarial training（可以视为一种data augmentation），这是一种比较吃运算资源的方法，提升效率是一个研究点。</p><h2 id="可解释性"><a class="markdownIt-Anchor" href="#可解释性"></a> 可解释性</h2><p>explainable vs interpretable</p><p>是否有解释性又高，能力又强的模型呢？比如决策树。</p><p>可解释性ML的目标如何设定？</p><p><strong>Local explaination</strong>，比如“为什么认为这张图片是一只猫？”</p><p>使用SmoothGrad，随机加入噪声，生成saliency map。但是这种方法会有Gradient Saturation的问题，为解决这一问题提出Integrated gradient分析。</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/noise.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>可以取中间层向量可视化，也可以在中间层加入probe。</p><p><strong>Global explainable</strong>，比如“什么样的图片叫做一只猫？”</p><p>寻找使经过卷积层后产生的feature map的元素加和最大的输入X。但这样通常会找到一堆噪声，所以还要加上一些约束条件，比如限制X中的非零元素个数。如果想要产生清晰地图片，可以加入一个生成器。</p><p>目前explainable AI其实更多的是倾向于产生人类比较喜欢的结果。</p><p><strong>使用比较简单的模型</strong>，模仿复杂深度学习模型。比如LIME（Local Interpretable Model-Agnostic Explanations）。</p><h2 id="领域自适应domain-adaptation学习一下助教课"><a class="markdownIt-Anchor" href="#领域自适应domain-adaptation学习一下助教课"></a> 领域自适应（Domain Adaptation）&lt;学习一下助教课&gt;</h2><p><strong>Domain Shift（概念漂移）</strong>，训练集和测试集上的数据分布不同，比如输入数据的变化，输出的分布有变化（即标签分布的变化）以及输入和输出关系的变化。[ 课程专注于输入数据的变化部分 ]</p><p><strong>领域自适应</strong>可以视为迁移学习其中的环节，根据目标领域中的数据分为以下几种情况：</p><ul><li>有少量带标签数据；使用原始数据训练模型后利用目标领域数据进行微调，但注意不要过拟合。</li><li>有大量无标签数据；训练一个模型提取器，以及一个领域分类器；Feature Extractor的目标是骗过Domain Classifier，思路和GAN类似，原始论文中的目标函数设计如下。另外还有<strong>DIRT-T，Maximum Classifier Discrepancy</strong>，它们使用的思想是，将Feature Extractor的训练目标设定为让目标数据最终提取出来的特征分布距离当前分类界限越远越好。</li></ul><div align="center">  <img src="/2021/06/16/lhy-2021-b/da.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><ul><li><p>只有少量无标签数据；使用<strong>Testing Time Training（TTT）</strong></p></li><li><p>对目标领域一无所知（<strong>Domain Generalization</strong>）<a href="https://ieeexplore.ieee.org/document/8578664%EF%BC%9Bhttps://arxiv.org/abs/2003.13216" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8578664；https://arxiv.org/abs/2003.13216</a></p></li></ul><h2 id="网络压缩"><a class="markdownIt-Anchor" href="#网络压缩"></a> 网络压缩</h2><p>因为需要考虑终端算力及隐私问题，所以需要进行模型压缩。本课程中只介绍了和软件相关的部分。</p><h3 id="1-network-prunning"><a class="markdownIt-Anchor" href="#1-network-prunning"></a> 1. Network Prunning</h3><p>首先训练一个大模型，之后将网络中的一些参数/神经元删减掉，之后经过fine-tune让效果回升一下。以上过程可以反复迭代。</p><p>以参数为单位（weight prunning）进行裁剪，会出现形状不规则的神经网络模型不太好实现，GPU加速也不容易的问题。所以很多人是直接把参数设为0，但这样做并没有真的把网络变小。而以神经元为单位（neuron pruning）做，比较容易编程实现。</p><p><strong>大乐透假说</strong>，但也有人反对这一假说。</p><p>后续有研究调研各种prunning策略的效果，并试图解释其中的原因。</p><p>《Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask》</p><p>《Weight Agnostic Neural Networks》</p><h3 id="2-knowledge-distillation"><a class="markdownIt-Anchor" href="#2-knowledge-distillation"></a> 2. Knowledge distillation</h3><p>知识蒸馏，大的模型为teacher network，之后据此学习一个小的模型 student network。</p><p>比如目前打比赛的时候用ensemble的方法比较好，但是实际应用中可以简化一个小的模型去学习多个模型的ensemble结果。</p><p>temperature for softmax</p><h4 id="3-parameter-quantization"><a class="markdownIt-Anchor" href="#3-parameter-quantization"></a> 3. Parameter Quantization</h4><p>Less bit represent: 可以用更少的bit存储参数。</p><p>Weight clustering，对参数进行分区，每个分区选取一个代表。</p><p>Binary weights，参数只有+1，-1两种可能。</p><h4 id="4-depthwise-separable-convolution"><a class="markdownIt-Anchor" href="#4-depthwise-separable-convolution"></a> 4. Depthwise Separable Convolution</h4><p>Depthwise convolution, filter数目和channel数目相同，每个filter只管一个channel，channel间没有任何交互。</p><p>Pointwise convolution，加入多个1x1的filter，作用是考虑不同channel之间的关系。</p><p>这个思路的来源是low rank approximation，矩阵分解的感觉，将一层拆成两层减少参数。</p><p>另外还有很多设计思路，SqueezeNet、MobileNet、ShuffleNet、Xception、GhostNet等等。</p><h4 id="5-dynamic-computation"><a class="markdownIt-Anchor" href="#5-dynamic-computation"></a> 5. Dynamic Computation</h4><p>希望模型可以自由调整其需要的运算资源。</p><p>可以在每一层额外加输出层，运算资源不够的时候就提前输出结果。</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/dd.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>也可以让调整网络模型宽度（Slimmable Neural Networks）。</p><p>或者可以让模型自己决定，调整深度和宽度，比如有些样本非常容易判断，不需要经过特别多层。</p><div align="center">  <img src="/2021/06/16/lhy-2021-b/cp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以上这些方法可以结合使用。</p><h2 id="相关研究方向"><a class="markdownIt-Anchor" href="#相关研究方向"></a> 相关研究方向</h2><h3 id="1-feature-disentangle关于autoencoder特征的解释性"><a class="markdownIt-Anchor" href="#1-feature-disentangle关于autoencoder特征的解释性"></a> 1. Feature Disentangle（关于Autoencoder特征的解释性）</h3><p><a href="https://arxiv.org/abs/1904.05742" target="_blank" rel="noopener">https://arxiv.org/abs/1904.05742</a></p><p><a href="https://arxiv.org/abs/1804.02812" target="_blank" rel="noopener">https://arxiv.org/abs/1804.02812</a></p><p><a href="https://arxiv.org/abs/1905.05879" target="_blank" rel="noopener">https://arxiv.org/abs/1905.05879</a></p><h3 id="2-tree-as-embedding"><a class="markdownIt-Anchor" href="#2-tree-as-embedding"></a> 2. Tree as Embedding</h3><p><a href="https://arxiv.org/abs/1806.07832" target="_blank" rel="noopener">https://arxiv.org/abs/1806.07832</a></p><p><a href="https://arxiv.org/abs/1904.03746" target="_blank" rel="noopener">https://arxiv.org/abs/1904.03746</a></p><h3 id="3-模型攻击"><a class="markdownIt-Anchor" href="#3-模型攻击"></a> 3. 模型攻击</h3><p>《Adverserial Examples are not bugs, they are features》</p><p>文字领域对抗攻击，加入一个短句后，所有问答都失效，<a href="https://arxiv.org/abs/1908.07125" target="_blank" rel="noopener">https://arxiv.org/abs/1908.07125</a></p><p>CCS’16 考虑物理世界特性的CV模型攻击，Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition</p><p>Adversarial Reprogramming，<a href="https://arxiv.org/abs/1806.11146" target="_blank" rel="noopener">https://arxiv.org/abs/1806.11146</a></p><p>Backdoor，<a href="https://arxiv.org/abs/1804.00792" target="_blank" rel="noopener">https://arxiv.org/abs/1804.00792</a></p><p>Adversarial Training for free，<a href="https://arxiv.org/abs/1904.12843" target="_blank" rel="noopener">https://arxiv.org/abs/1904.12843</a></p><h3 id="4-概念漂移比较重要"><a class="markdownIt-Anchor" href="#4-概念漂移比较重要"></a> 4. 概念漂移（比较重要）</h3>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习</title>
    <link href="/2021/05/22/pytorch/"/>
    <url>/2021/05/22/pytorch/</url>
    
    <content type="html"><![CDATA[<p>感觉相较于Tensorflow，Pytorch更适合科研人员使用，这边是一些关于Pytorch学习的资源与笔记。</p><h2 id="五小时官网教程"><a class="markdownIt-Anchor" href="#五小时官网教程"></a> 五小时官网教程</h2><p>教程中从numpy开始一步步自动化至调用pytorch模型的教学方式非常好。</p><p>B站资源，<a href="https://www.bilibili.com/video/BV1MU4y1p74U" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1MU4y1p74U</a></p><p>名词解释：epoch、batch、iteration</p><p>如何构造数据集，Dataset、DataLoader，可以直接分batch，直接shuffle，可以设置n_workers提升速度。</p><p>Transformers，ToTensor、MulTransform，__call__函数</p><p>Pytorch中的CrossEntropy层已经加入了softmax，接受的输入，Y是标签而非one-hot编码，预测值是logits而不是softmax之后的。</p><p>BCELoss函数，需要自己在神经网络设计中加入sigmoid层。</p><p>激活函数：step function、Sigmoid（常用于二分类最后一层）、TanH、ReLU（常用于隐藏层）、Leaky ReLU（尝试解决梯度消失问题）、softmax（常用于多分类最后一层）。</p><p>迁移学习，只在新任务数据上微调最后几层。pretrained = True</p><pre><code class="hljs python">model = models.resnet18(pretrained = <span class="hljs-literal">True</span>)num_frs = model.fc.in_featuresmodel.fc = nn.Linear(num_frs, <span class="hljs-number">2</span>)model.to(device)<span class="hljs-comment"># scheduler</span>step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size =<span class="hljs-number">7</span>, gamma=<span class="hljs-number">0.1</span>)model = train_model(model, criterion, optimizer, scheduler, num_epoches=<span class="hljs-number">20</span>)<span class="hljs-comment">#for epoch in range(100):</span><span class="hljs-comment">#    train()</span><span class="hljs-comment">#    evaluate()</span><span class="hljs-comment">#    scheduler.step()</span><span class="hljs-comment"># freeze all the previous parameters</span>model = models.resnet18(pretrained = <span class="hljs-literal">True</span>)<span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model.parameters():    param.requires_grad = <span class="hljs-literal">False</span></code></pre><p>TensorBoard，可以支持pytorch，使用torch.utils.tensorboard中的SummaryWriter即可，官方材料有（<a href="https://pytorch.org/docs/stable/tensorboard.html%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://pytorch.org/docs/stable/tensorboard.html）。</a></p><p>保存和加载模型推荐使用下面的方式：</p><pre><code class="hljs python">FILE = <span class="hljs-string">'model.pth'</span>torch.save(model.state_dict(), FILE)loaded_model = Model(n_input_features = <span class="hljs-number">6</span>)loaded_model.load_state_dict(torch.load(FILE))loaded_model.eval()<span class="hljs-comment">#checkpoint</span>checkpoint = &#123;    <span class="hljs-string">"epoch"</span>:<span class="hljs-number">90</span>,    <span class="hljs-string">"model_state"</span>:model.state_dict(),    <span class="hljs-string">"optim_state"</span>:optimizer.state_dict()&#125;torch.save(checkpoint, <span class="hljs-string">'checkpoint.pth'</span>)loaded_checkpoint = torch.load(<span class="hljs-string">'checkpoint.pth'</span>)epoch = loaded_checkpoint[<span class="hljs-string">'epoch'</span>]</code></pre><p>将GPU上训练的模型加载到CPU上需要进行一点调整，写map_location参数。</p><h2 id="相关资源"><a class="markdownIt-Anchor" href="#相关资源"></a> 相关资源</h2><p>官网教程：</p><p>《Dive into deeplearning》pytorch版本：</p>]]></content>
    
    
    <categories>
      
      <category>知识梳理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>李宏毅《机器/深度学习》2021笔记（一）</title>
    <link href="/2021/05/12/lhy-2021/"/>
    <url>/2021/05/12/lhy-2021/</url>
    
    <content type="html"><![CDATA[<h1 id="背景介绍"><a class="markdownIt-Anchor" href="#背景介绍"></a> 背景介绍</h1><p>官网 <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html" target="_blank" rel="noopener">https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html</a></p><p>数学基础能力：微积分、线性代数和概率论。使用Python和Google Colab，也使用Kaggle。</p><p>这门课程主要是针对深度学习并且涉及到比较前端的技术。如果希望了解机器学习基础可以尝试林轩田《机器学习基石与技法》。</p><p>[ 这类在线课程重点还是要把作业好好完成哦~ ]</p><p>本篇包括深度学习基础、CNN、注意力机制和生成对抗网络部分。</p><div align="center">  <img src="/2021/05/12/lhy-2021/good.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h1 id="深度学习"><a class="markdownIt-Anchor" href="#深度学习"></a> 深度学习</h1><p>机器学习可大约视为一个“找方程的过程”。</p><p>在分类和回归两类问题之外，还有很大一部分内容，叫做<strong>Structed Learning</strong>，即机器生成某些结构型数据。</p><p>Error Surface，不同参数下的损失值等高线图。</p><p>Model Bias，来自于模型自身表达能力的限制，比如线性模型无法分类“异或问题”</p><p>这次课中引入Sigmoid的、Hard Sigmoid以及神经元、神经网络的思路有点意思~</p><p>两个ReLU叠起来就可以合成一个Hard Sigmoid。</p><div align="center">  <img src="/2021/05/12/lhy-2021/sigmoid.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>深度神经网络最开始应用在CV领域，2015年Residual Net有152层（训练有特殊操作，skip connection）。<p>为什么选择深而不是选择“胖”呢？Fat network 哈哈哈。</p><h2 id="一-神经网络训练问题"><a class="markdownIt-Anchor" href="#一-神经网络训练问题"></a> 一. 神经网络训练问题</h2><p>通用的模型训练方法如下：</p><div align="center">  <img src="/2021/05/12/lhy-2021/road.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>训练集上loss偏大到底是因为1）model bias还是因为2）优化策略不合适呢？</p><p>可以通过比较不同的模型解决这一问题，如果深层模型在训练集上的效果没有浅层模型好，那么一定是优化策略的问题。</p><p>测试集结果比训练集结果差才可能是<strong>过拟合（overfitting）</strong>，还有一种可能是mismatch，由于训练资料和测试资料的分布不同导致。</p><p>最根本解决overfitting的办法其实是<strong>增加训练资料，或者叫Data Augmentation</strong>。比如CV领域将图片翻转、放大等。但Augmentation不要乱做，要有比较说得过去的理由，比如CV领域很少会把图片上下颠倒…</p><p>另一种方式就是限制模型复杂度，比如加入专家经验限制模型只能是二次函数、减少DNN的神经元、共享参数、减少特征、早停、正则化、Dropout等。</p><div align="center">  <img src="/2021/05/12/lhy-2021/over.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="1-局部最小值local-minima与鞍点saddle-point"><a class="markdownIt-Anchor" href="#1-局部最小值local-minima与鞍点saddle-point"></a> 1. 局部最小值（local minima）与鞍点（saddle point）</h3><p>这两种梯度为0的位置统称为“critical point”，如何区分二者呢？</p><p>使用泰勒级数展开近似当前位置的损失函数形状，具体来说，在以上两种位置一阶项均为零，可以根据二阶项区分。如果二阶项都大于零则当前为局部最小，如果二阶项都大于零则当前为局部最大，否则是鞍点。使用线性代数里的技巧，我们不需要将当前点周围的点都带入去判断二阶项是否大于零，<strong>只需要关注H即可</strong>。如果H是正定矩阵，即所有特征值都是正数，则假设1成立。</p><div align="center">  <img src="/2021/05/12/lhy-2021/critical.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如果处于鞍点可以沿着特征值小于零的特征向量方向更新参数，可以继续减小损失值。</p><p>[ 但上述方法<strong>实际很少使用</strong>，因为二次微分计算量很大，而且还要算出特征向量 ]</p><p>是否维度越高可以走的路越多呢？定性来解释也许局部最小值位置是很少的。</p><h3 id="2-批次batch与动量momentum"><a class="markdownIt-Anchor" href="#2-批次batch与动量momentum"></a> 2. 批次（Batch）与动量（Momentum）</h3><p>为什么要区分批次？（Minibatch和batch通用）</p><p>批量更新概念厘清，epoch，batch，update的含义：</p><div align="center">  <img src="/2021/05/12/lhy-2021/batch.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GPU可以实现batch内资料的并行运算，但如果batch过大的话计算时间还是会增加的。正是由于有并行运算的能力，如果batch size设定过小的话，计算完所有数据所用的时间反而会较大些batch设置下的长。</p><p>但有时正是由于small batch的更新过程比较noisy，反而会在最后有更好的效果。</p><p><strong>有很多paper研究如何均衡batch size从而结合两方面的优势，加快模型训练过程。</strong></p><div align="center">  <img src="/2021/05/12/lhy-2021/smallb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>缓解SGD陷入局部最小值问题，加入momentum向量。动量有两种解读方法：1）前一步方向减去当前梯度方向；2）之前所有梯度方向的加权和。</p><h3 id="3-adaptive-learning-rate自适应学习率"><a class="markdownIt-Anchor" href="#3-adaptive-learning-rate自适应学习率"></a> 3. Adaptive learning rate（自适应学习率）</h3><p>训练卡住不一定是局部最小/全局最小/鞍点等critical points，也可能是学习率不合适一直在震荡。</p><p><strong>Adagrad</strong>使用了RMS（Root Mean Square），计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>参数记录之前梯度值，直观上理解，针对梯度较大的参数采取小的学习率，反之亦然。</p><p><strong>RMSProp</strong>支持（没有论文，直接在讲课时推导出来），第一步与Adagrad相似，但后面计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>的时候调整当前梯度权重。带有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>参数。更多强调针对同一参数，当其处于大梯度阶段会使用小学习率。</p><p>当前最常用的优化器为<strong>Adam</strong>，即RMSProp+Momentum。</p><p>当前还使用Learning Rate Scheduling技术，比如Learning rate decay（随着训练进行学习率要减小），Warm up（学习率要先变大后逐渐变小，现在训练Bert的时候常用，之前在Residual network，transformer等都有提及）等。</p><p>为什么需要Warm up？给出的一个解释是，自适应的学习率调节是以以往梯度值统计信息为基础的，而刚开始训练的时候数据量不足，所以让学习率小一些。</p><div align="center">  <img src="/2021/05/12/lhy-2021/schedule.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>自适应学习率总结如下图所示。直观上理解，momentum是直接对以往梯度的累加，而<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>只关注大小，所以即便它们分别处于分子和分母的位置，也不会将效果抵消。</p><div align="center">  <img src="/2021/05/12/lhy-2021/adaptive.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="4-分类问题-损失函数"><a class="markdownIt-Anchor" href="#4-分类问题-损失函数"></a> 4. 分类问题 &amp; 损失函数</h3><p>由回归问题修改引入分类问题。</p><p><strong>softmax函数</strong>处理结果后得到预测标签。<strong>为什么要使用softmax？</strong></p><p>softmax的效果：1）将结果归一化，0、1之间加和为1；2）扩大值之间的差距</p><p>softmax的输入称为logit。</p><p>二分类问题直接去<strong>sigmoid</strong>就可以，二者是等价的。</p><p>损失函数选择：1）均方误差，MSE；<strong>2）cross-entropy，交叉熵</strong>。最小化交叉熵等价于最大似然。</p><p>Pytorch里面将cross-entropy和softmax内建在一起，不如需要在设计网络时手动添加。</p><h3 id="5-批次标准化batch-normalization"><a class="markdownIt-Anchor" href="#5-批次标准化batch-normalization"></a> 5. 批次标准化（Batch Normalization）</h3><p>帮助训练时使梯度下降更快收敛。因为这样使得error surface没有那么崎岖。</p><p>在深度学习模型中，对隐藏层的输入（下图中的z向量）也进行标准化，而这个过程使得训练样本间不再独立。再考虑到GPU资源限制，我们在一个batch中进行这样的归一化。</p><div align="center">  <img src="/2021/05/12/lhy-2021/batchn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>但在测试阶段不存在上述batch概念，Pytroch会基于训练阶段不同batch计算出的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>，计算moving average直接应用在测试样本中。</p><p>Batch Normalization的作者提出了internal covariate shift概念，但之后又有一篇文章《How dose batch normalization help optimization》打脸了这个观点…但他从实验和理论上解释了为什么BatchNorm效果会比较好，但同时还存在其它方式也能达到相同效果。</p><p>此外还有很多归一化方法比如，Batch Renormalization，Layer Normalization，Instance Normalization，Group Normalization，Weight Normalization， Spectrum Normalization等。</p><h2 id="二-神经网络架构设计cnn为例"><a class="markdownIt-Anchor" href="#二-神经网络架构设计cnn为例"></a> 二. 神经网络架构设计（CNN为例）</h2><p>图片分类问题几点观察和对应的简化方法：</p><ul><li>识别关键模式，而并不需要看整张完整图片。因此，设置<strong>receptive field</strong>，限制每个神经元关注的范围；不同神经元的receptive field可以相同，可以重叠，可以调整不同大小，或者限制channel，可以设置成长方形（但通常不用）。一般会设计为，会看全部channel，所以只用设置kernel size即可（比如3 X 3, 7 X 7等），同一个receptive field会对应有一组神经元，不同receptive field之间会有重叠（步长 stride），在边缘处会补全（padding）。</li><li>同样的关键模式可能出现在图片的不同区域。因此，提出<strong>共享参数</strong>思想，让不同神经元具有同样的权重参数。</li><li>对一张图片进行subsampling并不影响识别。因此，提出pooling操作，其中没有需要学习的参数。而随着运算力的增强，大多数网络架构设计中不再使用pooling层而追求更高的效果。</li></ul><p>简化过程如下图所示，将CNN用在图像外的其它领域中时需要<strong>仔细分析一下是否具有上述特性</strong>。比如alpha go并没有使用pooling。</p><div align="center">  <img src="/2021/05/12/lhy-2021/cnn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外一种思路是由X个不同的fiter扫过整个图片（是参数共享的另一种面向），得到X channels的feature map（可以视为另外一张新的图片，只不过channel不再代表RGB）。</p><div align="center">  <img src="/2021/05/12/lhy-2021/cnnl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>经过CNN之后的结果要经过Flatten，拉直成向量，再加一层全连接层，加softmax之后得到最终的结果。</p><p>CNN自己并不能够处理影像的放大、缩小或者旋转的问题，我们<strong>需要数据增强</strong>。<strong>Spatial Transformer layer</strong>可以处理这个问题。</p><p>CNN当前也应用在语音识别、图像识别等领域。</p><h2 id="三-自注意力机制"><a class="markdownIt-Anchor" href="#三-自注意力机制"></a> 三. 自注意力机制</h2><h3 id="1-self-attention"><a class="markdownIt-Anchor" href="#1-self-attention"></a> 1. Self-attention</h3><p>假设模型输入是一个<strong>长度可变</strong>的向量集合会如何？比如一段声音信号，社交网络等。</p><p>以输出向量数量可以区分三类任务：1）N个输入N个输出，比如词性标定；2）N个输入1个输出，比如情感分析；3）N个输入不确定个数的输出，seq2seq，比如机器翻译。</p><p>Self-attention会接收一整个sequence的信息（N个输入），并返回N个输出，它们都是考虑一整个sequence的内容而得到的，之后再经过FC得到最终结果。如下图所示的结构可以叠加多层，self-attention和FC交替使用。最知名的相关文章为《Attention is all you need》。</p><div align="center">  <img src="/2021/05/12/lhy-2021/self.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>计算输出向量的步骤（针对每个输入并行进行如下操作）：</p><ol><li>找出其他向量与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>a</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">a^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>的关联程度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>；这里有不同的做法，常见的是<strong>dot-product</strong>，additive等。所有结果会经过一个类似softmax的非线性层；</li><li>将输入向量乘上<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>v</mi></msup></mrow><annotation encoding="application/x-tex">W^v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span></span></span></span></span></span></span>，得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>向量；</li><li>b向量即为各v向量带<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>加权。</li></ol><p>相关术语：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span></span>为query，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>为key，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span></span></span></span>为attention score。</p><div align="center">  <img src="/2021/05/12/lhy-2021/dot.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外可以从矩阵运算角度总结如下，self-attention中需要学的参数是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>q</mi></msup><mo separator="true">,</mo><msup><mi>W</mi><mi>k</mi></msup><mo separator="true">,</mo><msup><mi>W</mi><mi>v</mi></msup></mrow><annotation encoding="application/x-tex">W^q, W^k,W^v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span></span></span></span></span></span></span>。</p><div align="center">  <img src="/2021/05/12/lhy-2021/selfm.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>进阶版本为多头自注意机制（<strong>Multi-head</strong> self attention），让不同的q负责不同种类的相关性，计算过程与上述类似，不同的head种类分别计算。</p><p>目前来讲，self-attention完全<strong>没有用到位置信息</strong>。可以使用<strong>positional encoding技术</strong>加入位置信息。最早是使用人工设定的位置信息，目前可以根据资料学习。</p><p>Self-attention应用在Transformer，Bert以及其它如语音辨识（Truncated self-attention，不要看一整句话，数据量太大了）、图像识别（可以将每个位置的pixel视为一个3维向量，比如self-attention GAN、DETR等）、图数据等任务中。</p><p><strong>Self-attention vs CNN</strong>：CNN可以视为简化版的self-attention，后者理解为receptive field是自动被学出来的而非人为划定的。《On the relationship between self-attention and convolutional layers》</p><p>**Self-attention vs RNN：**当前用到RNN的任务大多可以用self-attention取代。self-attention可以很轻易地利用远距离信息，而且可以平行处理。《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》</p><p><strong>self-attention for graph：</strong> 可以选择以图中的边确定节点向量间的关联性。也是GNN的一种</p><p>self-attention的最大问题在于运算量非常大，目前有很多变体<strong>致力于提升效率</strong>。《Long Range Arena: A Benchmark for efficient transformers》，《Efficient Transformers：A survey》。</p><h3 id="2-transformer"><a class="markdownIt-Anchor" href="#2-transformer"></a> 2. Transformer</h3><p>Seq2seq应用案例：1）是语法解析也可以视为是seq2seq，可以直接把语法的树状结构转换为sequence。《Grammar as a Foreign Language》；2）Multi-label分类；3）Object detection。</p><p>Transformer一个seq2seq的模型，分为Encoder和Decoder两个部分，每个部分有多个block，block中有self-attention层和FC层，且遵循residual架构，而且使用了layer normalization。Encoder架构如下图所示：</p><p>（但原始的Transformer的架构不一定是最优设计，有其它研究工作进行调整）</p><p>《On Layer Normalization in the Transformer Architecture》、《PowerNorm: Rethinking Batch Normalization in Transformers》</p><div align="center">  <img src="/2021/05/12/lhy-2021/trans.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Decoder部分有两种，比较常见的是<strong>Autoregressive</strong>的形式。基础架构和encoder相似，但需要使用masked self-attention，还需要设置BEGINE和END符号。</p><p>Non-autoregressive模型（NAT），一次产生所有输出，可以控制输出长度，平行化是它的优势。由于self-attention与次序无关，所以现在NAT的decoder也是一个热门研究方向。</p><p>两种模型异同如下图所示，当前NAT的效果基本赶不上AT，主要是由于Multi-modality的问题，这也是一个热门研究方向。</p><div align="center">  <img src="/2021/05/12/lhy-2021/NAT.Jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Decoder和Encoder连接的桥梁是<strong>cross-attention</strong>，Decoder提供q向量，k和v向量来自Encoder的输出最终生成v向量提供给FC。</p><p>训练过程中，Decoder的输入是正确答案，<strong>Teacher Forcing</strong>。</p><p>可以直接复制一些内容作为回答的模型，包括Pointer Network，Copying Network。（科研方向）</p><p><strong>Guided Attention</strong>，强迫模型的attention有固定的样子，比如语音合成中要求attention必须由左向右。（科研方向）</p><p><strong>Beam Search</strong>（偶尔有用），而非Greedy Decoding，</p><p>如果需要一些创造力的话，可以在训练decoder的时候加入一些杂讯。TTS领域，测试的时候也会加入一些杂讯。</p><p><strong>Scheduled Sampling</strong>，解决exposure bias，直接在训练时给Decoder一些错误的输入。原本的scheduled sampling会伤害到模型的平行化，所以针对transformer有所调整。</p><p>[ Accept that nothing is perfect. True beauty lies in the cracks of imperfection. ]</p><p>When you don’t know how to optimize, just use reinforcement learning (RL)！比如无法给出一个可微的损失函数。</p><h2 id="生成对抗网络"><a class="markdownIt-Anchor" href="#生成对抗网络"></a> 生成对抗网络</h2><h3 id="1-gan基础"><a class="markdownIt-Anchor" href="#1-gan基础"></a> 1. GAN基础</h3><p>Generator接收一个输入X和一个分布Z，输出一个分布Y。</p><p>训练时可以把生成器和判别器视为一个大网络，一部分fix，反复训练。</p><div align="center">  <img src="/2021/05/12/lhy-2021/gan.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>最终Generator可以根据不同的输入生成一些中间状态，比如从严肃到大笑。</p><p>本质上，GAN是突破了我们无法计算生成数据与真实数据分布差异的问题，divergence between distributions of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。有了GAN以后，只要我们知道如何在G和data中采样样本，就可以<strong>完成divergence计算</strong>，即使用Discrimincriminator。</p><p>原始的GAN论文中关于判别器的训练貌似是从二分类任务中发散过来，比如训练目标为最大化负交叉熵，也即最小化交叉熵。经过推导发现其训练目标<strong>与JS divergence有关</strong>。</p><div align="center">  <img src="/2021/05/12/lhy-2021/js.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>所以可以把生成器的目标函数中的divergence项替换为判别器中的目标函数项，形成min-max形式。</p><div align="center">  <img src="/2021/05/12/lhy-2021/theory.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>其实也可以设计不同的目标函数得到不同的divergence计算，相关论文（<a href="https://arxiv.org/abs/1606.00709%EF%BC%89" target="_blank" rel="noopener">https://arxiv.org/abs/1606.00709）</a></p><h3 id="2-gan训练技巧"><a class="markdownIt-Anchor" href="#2-gan训练技巧"></a> 2. GAN训练技巧</h3><p>GAN以难训练而闻名。经常出现如果使用简单的二分类模型做判别器的话，出现百分百分辨出来的情况。</p><p>实际中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>间可重叠/相交的部分非常少，可从以下两个角度解释：1）<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>都是低维数据在高维空间的映射，重叠/相交的部分基本可以忽略；2）我们对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mi>G</mi></msub></mrow><annotation encoding="application/x-tex">P_G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{data}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的了解仅来自于采样样本，可能没有采样到重叠区域。</p><p>所以JS divergence可能并不是很合适，因为对于两个没有重叠的分布，JS divergence永远都是log2，而没有中间值。</p><p><strong>WGAN（使用Wasserstein distance）</strong>，可以解决上述问题。其实计算W distance还是比较麻烦的，可以简单的视为解下面的优化问题。</p><div align="center">  <img src="/2021/05/12/lhy-2021/wgan.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>注意，这边要求判别器是平滑的。最开始是通过直接截断实现的，即限制D的输出值范围。之后有了<strong>Improved WGAN（使用Gradient Penalty）</strong>。在之后还有**SNGAN（Spectral Normalization）**真的做了对判别器的限制。</p><p>[ 实际训练过程和理论上有所出入，比如可能不会每轮都将判别器训到收敛 ]</p><p>另外，由于GAN中判别器和生成器是“相爱相杀，相辅相成”的，一旦其中某一个出问题，另一个也会无法训练。</p><p>相对而言，用GAN生成一段文字是最困难的，有很长一段时间没有人可以做到这一点，直到<strong>ScratchGAN</strong>出现。</p><p>其它有关生成模型的还有<strong>VAE</strong>，<strong>flow-based model</strong>等，但一般来讲，GAN生成的效果会比较好。</p><h3 id="3-gan评估"><a class="markdownIt-Anchor" href="#3-gan评估"></a> 3. GAN评估</h3><p>最直接的是直接找人来看…</p><p>如今的GAN也许还有mode dropping问题，即生成的图片多样性（diversity）不足。常用**Inception Score（IS）**定义生成结果的多样性。</p><p>也有使用**FID（Frechet Inception Distance）**衡量。</p><h3 id="4-cgan"><a class="markdownIt-Anchor" href="#4-cgan"></a> 4. CGAN</h3><p>Conditional GAN，比如输入“红头发、绿眼睛”产生相应的图片。</p><p>重点在于判别器还要接受条件输入，判断生成图片和条件是否匹配。</p><h3 id="5-在无成对数据条件下学习"><a class="markdownIt-Anchor" href="#5-在无成对数据条件下学习"></a> 5. 在无成对数据条件下学习</h3><p>Cycle GAN，比如用于图像风格迁移。网络可以设置为双向。类似的还有Disco GAN, Dual GAN都是这样的想法。</p><div align="center">  <img src="/2021/05/12/lhy-2021/cyclegan.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外还有进阶版StarGAN，可以在多种风格间进行转换。</p><p>另外的应用还有进行<strong>文字风格转换</strong>，长文转摘要，无监督翻译，无监督语音辨识等。</p><h2 id="相关研究点整理"><a class="markdownIt-Anchor" href="#相关研究点整理"></a> 相关研究点整理</h2><h3 id="1-均衡batch-size加速模型训练"><a class="markdownIt-Anchor" href="#1-均衡batch-size加速模型训练"></a> 1. 均衡batch size加速模型训练</h3><p>《Large Batch Optimization for Deep Learning: Training BERT in 76 minutes》</p><p>《Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes》</p><p>《Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well 》</p><p>《Large Batch Training of Convolutional Networks》</p><p>《Accurate, large minibatch sgd: Training imagenet in 1 hour》</p><h3 id="2-学习率warm-up"><a class="markdownIt-Anchor" href="#2-学习率warm-up"></a> 2. 学习率Warm Up</h3><p>RAdam，<a href="https://arxiv.org/abs/1908.03265" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03265</a></p><h3 id="3-spatial-transformer-layer"><a class="markdownIt-Anchor" href="#3-spatial-transformer-layer"></a> 3. Spatial Transformer layer</h3><h3 id="4-positional-encoding"><a class="markdownIt-Anchor" href="#4-positional-encoding"></a> 4. Positional encoding</h3><p><a href="https://arxiv.org/abs/2003.09229" target="_blank" rel="noopener">https://arxiv.org/abs/2003.09229</a></p><h3 id="5-transformer-nat"><a class="markdownIt-Anchor" href="#5-transformer-nat"></a> 5. Transformer NAT</h3><h3 id="6-gan-training一些训练技巧和论文"><a class="markdownIt-Anchor" href="#6-gan-training一些训练技巧和论文"></a> 6. GAN training（一些训练技巧和论文）</h3><p><a href="https://github.com/soumith/ganhacks" target="_blank" rel="noopener">https://github.com/soumith/ganhacks</a></p><p><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">https://arxiv.org/abs/1511.06434</a></p><p><a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">https://arxiv.org/abs/1606.03498</a></p><p><a href="https://arxiv.org/abs/1809.11096" target="_blank" rel="noopener">https://arxiv.org/abs/1809.11096</a></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（三）知识图谱</title>
    <link href="/2021/05/10/cs224w3/"/>
    <url>/2021/05/10/cs224w3/</url>
    
    <content type="html"><![CDATA[<h2 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h2><p>RotateE，《RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space 》</p><h2 id="知识图谱"><a class="markdownIt-Anchor" href="#知识图谱"></a> 知识图谱</h2><p>常用的知识图谱如Google knowledge Graph、Amazon Product Graph、Facebook Graph API、IBM Watson、Microsoft Satori、Project Hanover/Literome、LinkeIn Knowledge Graph、Yandex Object Answer等。</p><p>开放的知识图谱有FreeBase、Wikidata、Dbpedia、YAGO、NELL等。它们的共同特征是1）节点、边数量巨大；2）不完整性，即很多边是缺失的。比如下图所指示的Freebase的情况：</p><div align="center">  <img src="/2021/05/10/cs224w3/freebase.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>知识图谱的应用场景主要在于，比如推荐系统、问答对话系统等。</p><h2 id="图谱补全"><a class="markdownIt-Anchor" href="#图谱补全"></a> 图谱补全</h2><p>给定头结点、关系类型信息，预测尾结点。[ 与之前介绍的链路预测任务略有不同 ]</p><p>使用shollow embeddings联合entities和relations，而不是训练一个GNN。</p><blockquote><p>Given a true triple (h, r, t), the goal is that the embedding of (h,r) should be close to the embedding of t.</p></blockquote><h4 id="1-transe"><a class="markdownIt-Anchor" href="#1-transe"></a> 1. TransE</h4><p>希望头结点和关系的表征向量加和与尾结点表征向量接近。</p><div align="center">  <img src="/2021/05/10/cs224w3/transe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/10/cs224w3/transe_d.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>知识图谱中的关系类型：</p><ul><li>对称（symmetry）或antisymmetric，比如室友关系、上位词关系</li><li>逆（inverse），比如顾问与被顾问</li><li>传递（composition/Transitive），比如朋友关系，比如“我妈妈的丈夫是我爸爸”</li><li>一对多（1-to-N），比如“student of”</li></ul><p><strong>TransE是否可以有效表达以上这些关系？</strong></p><p>可以自然捕获反对称关系、逆关系、传递关系，但无法表达对称关系和1-to-N关系。</p><div align="center">  <img src="/2021/05/10/cs224w3/sy.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/10/cs224w3/oneN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-transr"><a class="markdownIt-Anchor" href="#2-transr"></a> 2. TransR</h4><p>将实体和边映射到不同的向量空间，实体映射到空间<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>，关系映射到空间<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">R^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span>， 同时计算一个投影矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi>R</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>×</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">M\in R^{(k \times d)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>，计算时投影到空间<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">R^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>，之后再按照TransE思路进行。</p><p>这样的操作下，TransR可以进一步支持对称关系、1-to-N关系（来源于投影矩阵M带来的灵活性）。但<strong>TransR不能支持传递关系</strong>，因为每种关系有自己的向量空间，而我们只有它和节点空间的投影关系M，而没有它们之间的映射关系。</p><h4 id="3-双线性建模bilinear-modeling"><a class="markdownIt-Anchor" href="#3-双线性建模bilinear-modeling"></a> 3. 双线性建模（Bilinear Modeling）</h4><p>TransE和TransR中都简单的使用预测值与目标值在嵌入空间的（L1/L2）距离作为代价函数。</p><p><strong>DistMult</strong>，使用映射后坐标乘积作为代价函数，如果存在此三元组则最终得分很高。这个方法也可以视为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo>⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">h\cdot r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>与t的余弦相似度。</p><div align="center">  <img src="/2021/05/10/cs224w3/distmult.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>DistMult建模可以表达1-to-N、对称关系，但是不可以表示非对称关系、逆关系、传递关系。</p><h4 id="4-complex"><a class="markdownIt-Anchor" href="#4-complex"></a> 4. ComplEx</h4><p>基于DisMult，ComplEx将实体和关系映射到复数向量空间（Complex vector space），而只用实数部分作为评分函数。</p><div align="center">  <img src="/2021/05/10/cs224w3/complex.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>ComplEx可以支持对称、非对称、逆关系、1-to-N，但无法表达传递关系。</p><p>四种方法效果总结如下，需要依据不同的建模任务加以区分。一般来说用TransR和ComplEx的比较多。经常先用TransE快速建模看效果，之后使用更具表达性的模型比如ComplEx，RotatE（TransE in complex space）等。</p><div align="center">  <img src="/2021/05/10/cs224w3/com.jpg" srcset="/img/loading.gif" width="50%" height="40%" alt="oauth"></div><h2 id="知识推理"><a class="markdownIt-Anchor" href="#知识推理"></a> 知识推理</h2><p>不同类型的查询方式如下图，既可以表示为自然语言也可以通过图结构表达：</p><div align="center">  <img src="/2021/05/10/cs224w3/query.jpg" srcset="/img/loading.gif" width="50%" height="40%" alt="oauth"></div><p>知识图谱补全任务可以视为回答one-hop query任务。</p><p>在path query中有锚节点anchor node，比如上图中蓝色的节点。</p><p>注意，做这些查询的知识图谱可能是<strong>不完整的</strong>，需要我们自己补全，而不只是简单地遍历查询。</p><p>**那么我们是否可以先进行图谱补全，之后再遍历查询呢？不可以！**因为通过图谱补全的步骤我们会得到几乎全部三元组是否存在的概率，而几乎所有的三元组都会有非零几率，这会得到一个非常密集的图谱，而使遍历也非常困难，复杂度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msubsup><mi>d</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mi>L</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d_{max}^L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, L为path的长度。</p><p>我们希望进行<strong>一种“泛化的链路预测任务”</strong>，希望可以回答任意查询同时隐式估算丢失的信息。</p><h3 id="一-多跳查询"><a class="markdownIt-Anchor" href="#一-多跳查询"></a> 一. 多跳查询</h3><p>如何在一个知识谱图中预测出给定查询的答案？</p><p>我们需要<strong>嵌入查询</strong>，将TransE扩展到多跳推理任务中。因为TransE本身就可以支持传递关系，所以天然可以处理path queries，通过将各关系向量相加完成。</p><div align="center">  <img src="/2021/05/10/cs224w3/qe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="二-query2box"><a class="markdownIt-Anchor" href="#二-query2box"></a> 二. Query2box</h3><p>如何完成连接查询？ （我们希望可以得到嵌入空间中两个实体集合的交集）使用box embeddings推理知识图谱。</p><div align="center">  <img src="/2021/05/10/cs224w3/box.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>龙虾教授《人格及其转变》笔记（上）</title>
    <link href="/2021/05/07/lobster/"/>
    <url>/2021/05/07/lobster/</url>
    
    <content type="html"><![CDATA[<p>乔治*皮特森是加拿大多伦多大学教授，因其在《人生十二法则》一书中提出人与人的交往关系与龙虾间有相似之处，而被网友们亲切地称呼为“龙虾教授”。</p><p>我在2020年下半年学习了《人格及其转变》课程，收获颇丰。比如，我们应生活在阴（混沌）与阳（秩序）的交界、各种神话故事中包含的“英雄出走”的内核、人与父母间的关系等。</p><p>有关大五人格的部分没有放在这边介绍，感兴趣可以听一下课程。</p><p><strong>请把自己想象成人类灵魂工程师，我们在同时塑造自己和他人的人格，我们一同努力想要达成的是什么？</strong></p><p><strong>Be what you can be; Be yourself !</strong></p><p>另外，作为土生土长的中国人，我认为在了解西方哲学思想的同时还需要学习我国传统思想文化。理性至上无法提供安身立命之精神，如<strong>王德峰</strong>教授所说，即“感性大我之重建”。</p><h2 id="英雄-神话与萨满启蒙"><a class="markdownIt-Anchor" href="#英雄-神话与萨满启蒙"></a> 英雄、神话与萨满启蒙</h2><h3 id="一-元故事-meta-story"><a class="markdownIt-Anchor" href="#一-元故事-meta-story"></a> 一. 元故事 meta-story</h3><p>各种圣经故事、神话、小说、电影等作品其实包含了统一的精神内核，即“元故事”，它们在告诉人们如何在这个世界上生存。</p><p>人类的主旨故事之一，是走出去探索前人未知的领域，直面未知带来的恐惧，获得宝物凯旋而归。比如约翰从鲸鱼肚子里出来、哈利波特与密室、狮子王、漫威系列等，总是经过<strong>乐园、失乐园、复乐园</strong>（paradise, paradise lost, paradise regained）的过程。这个过程大多不顺利，因为life is hard，我们需要有意义的东西支撑下去，履行自己的使命。</p><div align="center">  <img src="/2021/05/07/lobster/whale.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p><strong>一旦掉入“地下城”该怎么办？</strong></p><ul><li>意识到也许自己正是一切不幸的源头；</li><li>审视自己的假设和行为方式，看清楚它们是怎样阻拦着你，搞清楚要怎么改变。你在担心什么？逃避什么？你无法发展出的是什么？你要怎么重塑自己？</li><li>摊开分析，弄懂它们，然后彻底放下；</li><li>决定做该做的事，重新塑造自己，从而以一种更正确的方式生活</li></ul><p>以“哈利波特与密室”为例具体说明：要直面未知的恐惧，虽然可能会死掉，但这是最好的选择和出路；其中有一段邓布利多的凤凰重生（transformation），学习是<strong>寻找自己正在做错的事，同时失去自己的这一部分</strong>（即使是错误的），这会很痛苦，你需要让自己很大一部人人格死去，像凤凰一样。另外，哈利波特包括很多其它故事也都体现出“拥有<strong>至善人格</strong>的人被施以残酷的惩罚”的原型。</p><div align="center">  <img src="/2021/05/07/lobster/hp.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>扩展开来，人类之间也有很多共同点。</p><ul><li><p>人类的自我意识也和自我局限有关，大猩猩视频实验表示，人类无法关注到世界上的所有事。人类其实是目光狭隘的，只是通过快速移动点亮每个区域，所能看到的世界其实很小。我们活在某种感知框架里，这个框架由我们的认知决定。正如佛家思想所讲，你的价值观决定了你的认知方向。我们的注意力很大程度上是自主的，即使你认为可以掌控自己，但并不一定（比如潜意识）；</p></li><li><p>人类天生有共同的恐惧和脆弱，比如死亡焦虑，害怕蛇、黑暗、混沌等。致力于一些有意义的事可以在一定程度上抵挡脆弱；</p></li><li><p>使用了致幻剂之后，被试们描绘着相同的超体验或濒死体验；</p></li><li><p>各地古老的关于世界的描述十分相似；</p></li><li><p>我们处于社群之中是会受到血清素系统影响的（与龙虾们相似），一个理想的人是爬到某个优势等级顶端的人。我们的环境并不是自然，而是文化，是其他人。人有本性，根深蒂固，我们只能依据本性构建社会（荣格），否则这类文化就会瓦解。</p></li></ul><h3 id="二-隐喻"><a class="markdownIt-Anchor" href="#二-隐喻"></a> 二. 隐喻</h3><h4 id="1已知隐喻"><a class="markdownIt-Anchor" href="#1已知隐喻"></a> 1.已知隐喻</h4><p>一般以父亲、太阳等形象隐喻已知，它代表了文化与社会秩序等，它具体、明确、确定，会给人一定的安全感。但同样也会以为这极权、暴君，比如斯大林、纳粹党等。</p><p>彼得潘的故事告诉我们，我们无法一直缩在已知的舒适区中。如果你不在成熟的过程（自然岁月）中使用潜能的话，潜能也会自我消耗。你会自然长大，所以最好将潜能塑造成某个东西。</p><p>让某人做某事的最好方法，是禁止他做这件事而且不告诉他原因。这里也体现出人类会打破秩序、探索未知的本能。</p><h4 id="2未知隐喻"><a class="markdownIt-Anchor" href="#2未知隐喻"></a> 2.未知隐喻</h4><p>通常以黑暗之地、禁果、潘多拉魔盒、蛇等形式展现，<strong>表现为女性或阴性</strong>。它代表了潜意识、带着酒神力量的本我（弗洛伊德）、潜伏的怪兽、万物的来源与归宿。还包括伟大母亲、女王、基体（the matrix）、容器、深处、山谷、月亮等符号，是<strong>超越性的</strong>。</p><p>以迦利神像为例，它是毁灭或恐惧的具象符号。我们需要给“社会/自然/母亲”她想要的东西，与未来做交易，做出正确的牺牲，以今天的愉悦为筹码，交换明日的优势。</p><div align="center">  <img src="/2021/05/07/lobster/jl.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>我们需要主动探索未知，如果知道外面正酝酿着极大的危险，主动前往比不小心掉入要好很多。因为主动挑战威胁，身体会被激活，否则你会进入猎物模式，所以<strong>最好睁大双眼，警惕萌生的威胁，及早行动</strong>。而每次你学到人生中的一课，学习到的瞬间，你的世界都地动山摇。</p><h4 id="3-二者结合"><a class="markdownIt-Anchor" href="#3-二者结合"></a> 3. 二者结合</h4><p>我们存在其中的世界充满了动机和感情，充斥着恐怖、痛苦、喜悦与挫败，以及其他人，包含一切已知和未知，你希望在可预料当中有一些不可预料。<strong>未知中蕴含着宇宙的混沌与未来的潜能，我参与其中，将可能转变为现实</strong>。</p><p>借助<strong>太极图</strong>，我们要理解到，已知的部分可能在一瞬间变为未知，一生中可能经历无数次这样的转折。我们注定会经历混沌（chaos），也许会在某次混沌中一蹶不振，但<strong>一定要坚信混沌中蕴含着向秩序的转变</strong>。</p><div align="center">  <img src="/2021/05/07/lobster/taiji.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>你要尽力活在<strong>秩序和混沌的边界（meta-place）</strong>，主动探索未知，强化秩序。现实中，有些人可以容忍大范围的混沌，比如自由派，但有些人更喜欢现存结构的稳定性，如保守派，环境瞬息万变，没有永远的对错，这就需要<strong>双方的沟通交流</strong>。当你处于混沌与秩序的最佳边界时，你的大脑会告诉你，它会制造出一种全情投入、富含意义的感受；<strong>你足够稳定，也有充足的兴趣，达到这种完美的平衡</strong>。</p><p><strong>要如何在混沌与秩序之间斡旋呢？</strong></p><p>1）Heaven is in the unknown。有些人带着幻想面对未知，未来，梦境，创造力从这里萌发，具有很高的经验开放性。但很多患者也这样，他们具有强烈的自我批判，他们聪明，具有创造力，通过梦境与艺术探索未知。许多19、20世纪被普遍认为很伟大的人（如尼采、达尔文、陀思妥耶夫斯基、托尔斯泰、弗洛伊德、荣格等）都经历过这种创造类疾病，那是长期、深刻的、心理上的不安与不确定。<strong>通往更高智慧的道路便是要经过恐怖的地域之门。</strong></p><p>2）文化、艺术、幻想、音乐、戏剧、故事，会起到缓冲作用。我们被文化和幻想包围着，也被保卫着，我们应当尊重秩序。对历史了解的太少才会更偏爱混沌，而不是秩序。应心存感激，感谢明君，但同时也要足够聪明，知道他的另一面是邪恶暴君，这才是<strong>对世界的完整认知</strong>。必须对人性有了解，并不是作恶者在一边而受害者在另一边，<strong>人性之邪恶，人性之崇高都是你的中心元素</strong>。</p><p>3）在发展初期（比如事不过三，你要记得人们会狡辩，你要坚持住自己的观点），你最好把自己从压迫中释放出来，<strong>把真实想法讲出来</strong>（不一定正确），你要与敌人或者不同观点的人<strong>真诚交流，探索未知</strong>。</p><p>4）<strong>怨恨是一种有毒的情绪</strong>。如果你怨恨某事这意味着你应该有所成长，承担责任，不再四处抱怨，哭哭啼啼；或者这意味着正有人压迫你、欺辱你，你有东西要说出来或做出来，但你没有（因为那可能在短期内让你惹上危险）。日复一日，虽然逃避会在短期内可以保护你，但这会把你挤压变形，随着怨恨积累，疯狂滋长，成为复仇的欲望。你会成为压迫者，你在背后说坏话，他们让你做什么你会很马虎或者很勉强地去做。如同陀思妥耶夫斯基在《地下室手记》中描写的主人公一样，住在<strong>心理地下室</strong>里，想着这个世界多么邪恶，别人如何针对你、拒绝你，那么你会觉得活着本身就是有毒的，你会想要走出去做尽极恶之事。</p><p>在此也可以引出<strong>科学技术与人文艺术的关系</strong>：</p><ul><li><p>科学用来描述“世界是什么”，而对于【真实】的定义，即使是科学也只是一种工具，是一种人类用来探索世界的工具；</p></li><li><p>神话、戏剧、梦境、潜意识以及其他美学的、艺术的、幻想方面的文学作品是在告诉我们事情应该是怎样的，告诉我们如何行动，传授为人处事之道。经过多少个世纪，提炼出<strong>至善、至恶</strong>让我们理解，它们就<strong>像圣经故事中的两兄弟，也在我们体内时常斗争</strong>；</p></li><li><p>人文类学科像魔法，让人意识到，我不只是父母的孩子，还是自然的孩子，人类文化的孩子；由于人生艰难，你必须做一些真的有意义的事，你肩负着道德责任，做正确的事；<strong>真正意义上的好人，单纯遵守纪律是不够的，需要理解恶，并经受住恶</strong>；我们必须要理解自己身上的恶和阴暗面，这些可以通过读史来实现，比如《古拉格群岛》、《南京大屠杀》、《奥斯维辛》等，读的时候想象自己是集中营的守卫、是卡波而不是所谓解放人民的大英雄。<strong>只有认识到自己是个魔鬼之后，你才能对自己有足够的尊重，会对自己的行为有所控制</strong>。</p></li></ul><h3 id="三-狮子王中的隐喻"><a class="markdownIt-Anchor" href="#三-狮子王中的隐喻"></a> 三. 《狮子王》中的隐喻</h3><h4 id="1-性欲与侵犯欲"><a class="markdownIt-Anchor" href="#1-性欲与侵犯欲"></a> 1. 性欲与侵犯欲</h4><p>与饥饿和口渴不同，性欲与侵犯欲这两种驱动力通常被人类社会排除在外，需要个体有意识地自我整合。</p><p><strong>整合好自身的攻击性</strong>是一件非常重要的成长课题。</p><p>早期的辛巴像一只瞪大了眼睛的无辜的鹿，它软弱而幼稚，具有天真的脸庞。它接受一切信息，但没有反应和产出，牺牲了自己而成为环境的出气筒。</p><p>牺牲自己迎合别人，永远不制造冲突，这并不会让你更受欢迎或成为一个好人。因为没有能力伤害别人不代表你就是道德的，<strong>你需要长出獠牙，这样会保证你更少用到它们</strong>。背后的愤恨或怨恨其实变相地展示了你的不成熟。另外，如果无法拒绝加入群众病态的（某些情况下）行动中，恶意的企图会把你的邪恶（你一直逃避、摒弃、压抑的邪恶）勾引出来。</p><p>荣格认为，<strong>应该将恐怖的一面整合进自身而不是一直摒弃它</strong>，鄙视它，妄图彻底清除它，因为你做不到。而且即使做到了，你只能得到一个软弱的自己。圣人不是纯白的，是阴阳统一的。整合好你的阴影和侵犯性，你的面庞会变得更加坚定，如同成年辛巴。</p><p>通过<strong>查验是否还存在比较持久的（18个月以上）活跃的负面记忆</strong>，可以判断自己的感知价值结构中是否还存在漏洞。具体可以通过<strong>自我写作</strong>的形式来做。在你的自传中，仔细考虑过去发生在你身上的那些不好的事，弄清楚到底发生了什么，如何避免未来再次发生，梳理并表述自己的负面情绪。</p><h4 id="2-自性"><a class="markdownIt-Anchor" href="#2-自性"></a> 2. 自性</h4><p>《狮子王》中的狒狒象征了<strong>自性，也即潜能，未来可能成为的你</strong>，如果你以获取最大化信息的方式与世界交互，那个你能成为的你。辛巴受到召唤，在地下深水泉（潜意识）中看到自己未来的模样。</p><p>你会受到睿智的召唤，重新发现孩童时期与太阳相连的部分，并且相信它。当然这会需要一个长期的过程：</p><ul><li>第一阶段，开始意识到自己的无用，伴随着对自己的辩解。生气而毫无力量，爱发牢骚，满怀愤恨，拥有可悲而欠揍的面容。刚刚唤醒自我意识，伴随着非常痛苦的自我反思；</li><li>第二阶段，意识到自身价值结构的不足，希望找到“父亲”，也即权威。但“父亲”已经死了，我该如何转化自己，我该如何变好？但是没有人能够给出答案。</li><li>第三阶段，真诚希望改变。一旦你意识到自己错了，并且打开了潜在改变的大门，你的一部分自我会做出回应。你的深层自我还保存着潜在的可能性，它会改变你对事情的看法，改变你的回应方式。</li></ul><div align="center">  <img src="/2021/05/07/lobster/feifei.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>如果你真的想让情况好转，让自己振作，并且你承认现阶段的不足，你考虑当前的问题，想弄明白下一步究竟应该怎么办，<strong>那么你会找到内心中有某种东西在你发展的过程中，指引着你，那就是自性，是更高阶的自我，是转变中的不变</strong>。<strong>心理治疗是可以被高境界的道德努力取代的</strong>，一定要在内心深处，深层潜意识中，看到自己可能成为的人。</p><p><strong>如何解决心理问题？</strong></p><p>1）陷入混乱，如何改善当下？即<strong>自我认知模型的重建</strong>。你如今的生活不如意，那么你期待一年（或三五年）以后的生活（或者只是某些方面）是什么样的？确定下来，作为目标，筹划一下（诚实地），考虑会遇到哪些阻碍（实际的，心理的）。制定策略，试着朝向理想状态迈步。<strong>一定要对自己真诚，协调自我，记住，自己和自己是一伙的</strong>。</p><p>2）<strong>在多个维度照顾好自己</strong>，避免陷入混沌。每个人都需要<strong>日常惯例（routine）<strong>比如固定的作息，在物理上照顾好自己；你需要</strong>一份事业（career）</strong>，生产你认为有意义的东西，制定职业计划；需要家庭、朋友、<strong>亲密关系</strong>；<strong>学会如何谈判，学会表达需求</strong>；学会<strong>讲真话</strong>；学会<strong>倾听</strong>；<strong>运动是最好的阻止智力下滑的方法</strong>。</p><p>3）其实，很多去找心理治疗的来访并不是心理问题，而是生活真的出了问题亟待解决。心理治疗是一段真诚的关系。而本质上，<strong>自愿接触你不愿面对的东西是有疗愈性的</strong>，注意这必须是自愿的面对的。</p><p>4）<strong>存在式焦虑</strong>；正常人在安全的地方是冷静的，是把自己整合的很好的。但是现实中常见的地方并不安全，所以<strong>感到焦虑、抑郁是正常的，并不需要理由</strong>，我们需要关注的是你如何达到安全协调。我们可以通过待在自己的领地，维持稳定与安全感。但我们<strong>必须在意外界</strong>。你确实需要<strong>整合好自己的思想、精神，将每一种构成要素转换成功能性的存在</strong>，并且让这个<strong>功能性存在与外界图景具有一致性，即融入社会</strong>。内在、外在整合兼备才能真正调整好你的情绪。</p><h4 id="3-其它隐喻元素"><a class="markdownIt-Anchor" href="#3-其它隐喻元素"></a> 3. 其它隐喻元素</h4><ul><li><p><strong>阳光照耀</strong>；如果真正想在某处感到舒适，想主导并融入这个地方，你需要<strong>花心思让每个角落都被（你的）光照耀到</strong>，主动探索而非恐惧地缩在一角。这里的“某处”包括自身、亲密关系、陌生环境等；</p></li><li><p><strong>将特权与能力混为一谈</strong>；在大学里、在和平国度里应当心存感激，而不是认为这一切理所当然；</p></li><li><p>要<strong>允许自己是傻瓜</strong>；如果不允许自己时这种状态，而且一味完美主义，你会觉得自己是个“冒名顶替者”。尝试新事物的时候，你确实会像个傻瓜，但如果因此不去尝试，你就是个更糟糕的傻瓜。<strong>允许自己犯错才能进步</strong>。</p></li><li><p><strong>刀疤的帝国</strong>；二战时期的德国，极具条理性和极权主义，是过度的文明。人们信奉一套理论体系，甚至不惜欺骗自己，整个系统中充满了谎言（比如德国、苏联、1984）。</p></li><li><p><strong>辛巴逃离到了一篇沙漠</strong>；当你离开一个国度（不管那里是怎样的残暴），你都会<strong>陷入混乱中</strong>。如戒烟戒酒的时候，你抛弃了旧的价值体系（也许是因为它不够好），但<strong>并不是马上迎来提升</strong>。</p></li><li><p><strong>国王在国外培养</strong>；哈利波特、亚瑟王、狮子王等都会包含这类元素。你会在一定程度上疏离于你的文化，可能是因为原本的文化滞后腐朽，也可能由于你并没有发挥出你的潜力践行价值，在现有的评价体系中，你并不成功。比如在电影中辛巴被陷害了，但它并不是完全无辜的。</p></li><li><p>睿智的部分<strong>没有对邪恶的部分有足够的戒心</strong>；辛巴的父亲被弟弟打败的场景表明，明君有一个邪恶的兄弟，而没有足够留意，它选择逃避或视而不见。人生的洪水也是如此，这些灾难有上天的随机因素，也有一些部分是你会责备自己过去的，<strong>因为当时的你短视或选择视而不见</strong>。</p></li></ul><h2 id="皮亚杰与构成主义"><a class="markdownIt-Anchor" href="#皮亚杰与构成主义"></a> 皮亚杰与构成主义</h2><p>皮亚杰，知识巨匠，一位杰出的发展心理学家，知识渊博。</p><p>经典经典科学观会假设知识本身是事实而不是过程，但当代科学更多认识到<strong>知识是过程而不是静态事实</strong>。</p><h4 id="1-同化与顺应"><a class="markdownIt-Anchor" href="#1-同化与顺应"></a> 1. 同化与顺应</h4><p>皮亚杰理论中重点有<strong>同化与顺应</strong>的概念。我们需要将知识视为工具，而不是客观独立的现实，<strong>世界不是所有等待发现的客观事实的合集</strong>，世界是更为复杂的。不随时间改变的才可以称为事实，比如人们产出事实的方式、知识（世界观）的获取与转换过程等，而不是事实本身。<strong>终极现实，是经历这些阶段的过程</strong>。</p><p>皮亚杰理论与萨满启蒙有共通之处，也即起初存在有序状态，之后意外发生，陷入混沌，原有体系瓦解，最终重组新生，形成一个更加完整的状态。根据这个理论可以解释<strong>孩子在商场中与家长分开会恐慌</strong>的现象。因为，原本家长将外界的复杂性与孩子隔绝，而家长离开，混沌和不确定性向孩子涌来，在新世界中学习速度过快，会带来巨大痛苦。而如果学习速度适中的话，你会从可能性中获益，而不是被不确定因素淹没。</p><div align="center">  <img src="/2021/05/07/lobster/pyj.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h4 id="2-程序记忆与表现记忆"><a class="markdownIt-Anchor" href="#2-程序记忆与表现记忆"></a> 2. 程序记忆与表现记忆</h4><p>皮亚杰理论整体在研究人们如何表现世界并学习。他在<strong>试图填补科学与价值之间的裂缝</strong>。</p><blockquote><p>大卫休谟提出，你无法从“是什么”中推理出“应该是什么”，你知道这件事不代表你可以毫无差错地处理这件事；</p><p>哈里斯相信价值可以嵌于科学中；</p></blockquote><p>提出<strong>程序记忆与表现记忆</strong>，人类在婴儿时期还没有表现记忆，而社会结构隐形地嵌于程序记忆的系统结构中，你所出生于的这个社会结构会被编入你的行为，而你并不知道规则。</p><h4 id="3人生游戏"><a class="markdownIt-Anchor" href="#3人生游戏"></a> 3.人生游戏</h4><p>人类间的社会互动来源于一个有限制的空间，我们永远都在玩游戏。而通过研究哺乳动物的脑回路研究，我们发现公平游戏的感觉是生物天生的。</p><p>小孩子的游戏嵌在大人游戏中（所以，在2-4岁时，要让孩子学会如何和其他人，尤其是孩子，玩耍）；之后，人们会随着发展，更加有意识地玩游戏，并开始在行为上表现游戏，开始学习显示游戏规则；最终，在道德发展的最高阶段，人们意识到自己不仅仅是游戏的玩家，还是规则的制定者，也可以发明游戏。</p><p>如何毁掉一个孩子：</p><ul><li>在他做好事或尝试做好事时，惩罚他或忽略他；忽略比惩罚更可怕，因为惩罚他的时候至少你的注意力还在他身上</li><li>玩游戏时不要越过让孩子筋疲力尽的那个点[ 其实“培养”自己也是同理]</li></ul><p>在人生游戏中，克服痛苦的方式之一是创造意义：</p><ul><li>在技术层面，从挫折中学习到什么，真的会改变的微观生物构造，可以类比于冲浪；</li><li>我们在死亡（稳定或混沌都是死亡状态，你生活在阴阳交界）和生存之间保持着精妙的平衡。学习的过程也是这样，你需要杀死部分已知（生物结构），才能学到未知，虽然过程会带来痛苦；</li><li>在这个过程中，我们学习到游戏的“元玩法”</li><li>意义的重要性之一就在于，它会改变你看待世界、回应世界的方式</li></ul><h4 id="4-高级抽象认知"><a class="markdownIt-Anchor" href="#4-高级抽象认知"></a> 4. 高级抽象认知</h4><p>如图所示，要从小事做起、从多方面做起，从底层的行为感知序列开始，从实际的微观行为到高层抽象。</p><div align="center">  <img src="/2021/05/07/lobster/pyj_high.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h2 id="弗洛伊德与潜意识"><a class="markdownIt-Anchor" href="#弗洛伊德与潜意识"></a> 弗洛伊德与潜意识</h2><p>潜意识的概念表明了，人们可以在无法解释的时候做出行动，比如孩子无法描述游戏规则，但他可以玩这个游戏。</p><p>人类深层的潜意识和其他哺乳动物、爬行动物等也有很多共通之处。</p><p>我们并不是完全掌握这些“后台运行的程序”，人格越没有整合好，就越容易失去对这类意识的掌控。</p><p>弗洛伊德提出我们的防御机制包括：压抑、否认、反向、转移、认同、合理化、理智化、升华、投射。</p><p><strong>JP教授对弗洛伊德的理论有几处不认同的地方</strong>：</p><p>首先是<strong>学习成长模型</strong>，教授更倾向认为在健康状态下是皮亚杰模型。病态发展中，你并没有把攻击冲动和性冲动整合到你的人格中去，而是让超我直接压抑了它们，没能称为活跃的一部分。你不去展示它们，所以成为了所谓的“好人”。你对自己非常专制，成为一个超严厉超我的受害者。你把父母、祖父母及内化的“父母”集合成了一个超级严厉的法官，一直在注视你，评判你。</p><p>其次是有关<strong>记忆与遗忘</strong>，弗洛伊德理论认为人类有连续录影带式的记忆，某些由于太过痛苦或其他原因而被压抑，导致遗忘。而教授则认为，人的记忆并不是这样有条理的，过去的信息是杂乱的，待清除的，所以人们没有注意到，而不是有意识（自我意识或潜意识）地去压抑。</p><p>最终有关<strong>梦境解析</strong>，弗洛伊德认为梦展现了被压抑的东西，梦尽力地像掩盖所要表达的内容。而教授更倾向于荣格的观点，即认为梦在尽可能清晰地表达，它不属于语义记忆系统，而更像是探索未知的触角，所以使用象征符号，而并不是想要向做梦者隐藏不愉悦的内容，而是用它唯一可以使用的语言体系。</p><h2 id="卡尔罗杰斯与现象学"><a class="markdownIt-Anchor" href="#卡尔罗杰斯与现象学"></a> 卡尔罗杰斯与现象学</h2><p>科学将主观剥离世界，它将任何主观视为偏见或错误，希望尽可能摆脱。但问题在于每个人都是一种主观。</p><p>在科学发展的当今，人成为了冷漠的客观事实中一种孤立的存在，容易导致存在的虚无。</p><p>尼采提出，上帝已死，带入虚无主义。海德格尔则希望从头构建体系，重新思考现实，解决这一问题。将现实看作我们经历/体验到的一切，抛弃主观和客观的划分，但这类研究并没有实质性进展。</p><p>荣格后半生转向现象学，提出如下三个必要层面，也可视为扩展了皮亚杰的道德模型：</p><ul><li><strong>将思想与情绪结合 [男女结合]</strong>；可以通过<strong>未来自我写作训练，让焦虑成为你的助手</strong>；想象如果你不去处理某个你目前尽量逃避的问题，会发生什么；也可以<strong>进行坚定性训练</strong>，想象你具体想要什么，不要因为当下得不到就不去想象，想象你不去这个做的代价，整合攻击性（如果你没有力量，你就无法谈判；而愤怒是一种有毒的情绪，会加剧身体负担，长期下来会影响健康）。</li><li>将思想与情绪的结合体，<strong>再与身体结合 [行动起来]</strong>；一定要知行合一，保证认知与行动的连贯性。从皮亚杰高层抽象模型的最底层做起，从多个方面的小事做起。没有肩负起存在的重任会导致神经质的愧疚和恐惧，所以从清扫你的房间做起。</li><li>最终消除<strong>自己与世界的区分[天人合一]</strong>；世界会随着你的目标而改变。</li></ul><p>人与人之间的沟通要做到<strong>真正的倾听</strong>，我在这里是为了一起达到更好的你的一部分；我希望了解你的观点，而不是输出我的然后只希望你赞同。人们很难找到真正倾听自己的人，真正的倾听是我们给别人的非常宝贵的礼物。</p><h2 id="存在主义"><a class="markdownIt-Anchor" href="#存在主义"></a> 存在主义</h2><p>存在主义相关人物包括尼采、陀思妥耶夫斯基、克尔凯郭尔等。</p><p>存在主义产生的背景为“上帝已死”。现代生活中不可避免地承担着很多焦虑，这可能是人们对科技的提升与意识觉醒付出的代价。人类暴露于一种无意义且痛苦的存在，如果你的价值系统瓦解了，那么你就会没有目标，没有积极情绪（虚无），这种情况下，人们也许会飞奔至极权主义的怀抱，牺牲理智与智力换取秩序与确定性。</p><blockquote><p>虚无主义(物质世界观中隐含)，陷入其中，你会一无所有，无法应对生活中的挑战或苦难</p><p>理性极权主义(激进意识形态)，过度信奉，你变成了一堆理论抽象中的提线木偶，走向毁灭</p></blockquote><p>存在主义<strong>悲观又极度乐观</strong>，它承认人是脆弱的、有限的，但一旦你<strong>直面恐惧</strong>，又会激发出我们无法估量的力量。这力量就是自性或称为内在潜能。不要低估自己的内在潜能，对于你自己，你还有很多不知道的事，<strong>去新的环境</strong>，会改变你的微观生物结构。如果你<strong>恰当地把自己推进世界</strong>，你会开启新的能力，并在探索过程中获得信息。<strong>自愿地以更多方式挑战自己</strong>，可以促进这种转变。不要针对死亡焦虑构建虚假的抵抗，而是积极学会如何应对这个世界。</p><p>存在主义包括三个基本理念：</p><ul><li><strong>行动比语言更有力</strong>。知行合一，身心协调，行胜于言。如果想了解别人或自己的信念，最好去看做了什么，而非说了什么。</li><li><strong>麻烦和痛苦是人类经历的固有元素</strong>。人难以摆脱苦难，即便这个人本身没什么问题。很多人生中的苦难没有原因，没有由来，并不只是因为童年/经历的锅，并不是你这个人哪里出了问题，也不要认为这些问题仅限于你自己或这一小部分人。<strong>人的生活本身就有问题，你要做的是面对和解决</strong>。合理程度的痛苦是正常的，类似圣经故事中所说，人们从美好的天堂掉落，总是处于残缺的状态，认为自己哪里出了问题，需要被纠正。</li><li><strong>存在主义包含一些浪漫主义</strong>。反对理性与智力的至高地位，理性并不是指引人们的根本原则。<strong>理性是需要与其它主观因素相辅相成的</strong>，而非如20世纪以来所呈现出的一家独大。科学是一种需要被合理利用的工具，而不是描述存在的方式。<strong>关注个体</strong>，个体才是心理分析的恰当层面，将每个患者视为独特的个体，有独特的问题，而不仅是使用精神分析那种成体系的框架。</li></ul><p>由此JP引到恐旷症的逐步治疗：</p><ul><li>四处嗅探的小白鼠；</li><li>早期阶段的回归与固化，可以看到患者退行回小孩子阶段，认为自己没有能力；</li><li>对于权威的态度，自己总处于次级、奴隶的状态。家庭成员对心理治疗的抵制（包括沉默的抵制），你真的希望伴侣变得更好吗，那意味着她会更坚定，而难以掌控；</li><li>一些心理分析师不赞成类似行为主义的治疗手段，认为对于电梯的恐惧并不针对电梯，而是象征着其它东西，如果治好了电梯，这恐惧还会从其它地方冒出来；JP认为不然，治好电梯，患者会自行挑战出租车，这是在教患者学会勇敢，而<strong>勇敢会扩大化</strong>；</li><li><strong>表现得弱小无用会成为一种武器</strong>；</li><li>[ 题外话 ] 联想到刚开始做科研时，自己对自己的批判，导致了自己的罢工，所以很喜欢DDL，因为那时候你必须专注，没心思再批判什么。发论文也是如此，直面恐惧，真的做到之后，就觉得没有那么困难。</li></ul><p>另外，人并不是理性的，这也是存在主义对乌托邦主义的批判。陀思妥耶夫斯基的作品也表明，<strong>人具有自由意志</strong>，人所做的事情，是为了时时刻刻证明他是人，不是钢琴键，不是可以计算、推理的事物，就算这可能会损害他自己的皮肤，就算要以自相残杀为代价。尼采也表示，也许感到不满足其实也是一种满足，也许你必须受限，这是你想要的，也许这些才会给你生活的意义。</p><h2 id="现象学"><a class="markdownIt-Anchor" href="#现象学"></a> 现象学</h2><p>相关人物，马丁海德格尔（哲学家）。</p><p>现象学认为人们生活在一个自我定义的感知框架之中。</p><p>JP认为<strong>临床心理学是有价值导向的，并不是纯粹的科学学科</strong>。</p><p>客体是非常复杂的，比如波粒二象性。即便以科学方法定义某个客体，也并非真正在定义，你只能说，这是个多维度物体，如果我以这样的方式接触它，也就是采取这样的过程或方法，它就会显现出那样的特质，但还有很多种其它的可能。</p><p>我们需要限制自己的感知范围，达到一个可以处理的范围，<strong>在这个收窄的现实中</strong>生活。这就意味着，你<strong>需要有一个目标（视野的焦点）</strong>，这代表了你的价值体系，以及世界将如何在你面前展开。目标启示着你的世界，组织着你的情感，并让你准备好做行动。目标包含了很多，比如内驱力、目的、动机等等，它是你人格的一部分。但同时也要了解到，我们正<strong>处于收窄的视野范围内</strong>。</p><p>人们为什么对某些事物好奇？要去追寻某些意义？</p><p><strong>宾斯旺格</strong>认为我们最先感知到的，不是味道、声调或触感印象，也不是物体或客体，而是意义；<strong>美丽是主观的</strong>，由于你的感知‘滤镜’产生的。<strong>鲍斯</strong>则认为，<strong>美丽固有于客体本身</strong>，显现了自身，向外发光，你追求那些向外发光的东西（比如《哈利波特》中的金色飞贼隐喻）。JP认为是这两种观点的结合，你无法完全掌控你的好奇心，但它也并不是完全随机的，因为没有主体能脱离结构去感知。同时，被感知的那个客体也带着它自己的潜能向外发光。</p><p>我们面前不是一个固定的客观世界，而是<strong>一个充满潜能的世界</strong>。你能和这种潜能的任何方面进行互动，在互动中，你将一些以前不存在的东西拉进了现实。这些潜能并不是无限的，因为你本身就是有限的，但不管你有什么目标打算，它都足够了，因为<strong>它永远比你所需的潜能更多</strong>。</p><p>你探索着某个新的东西，你从这次探索中生成了什么呢？首先，会产生一个新的你，物质的也是精神的，因为探索时你在学习，这个过程会改变你。而同样从你的探索中也生成了一个世界。</p><p>开放的想象力：</p><ul><li>梦境处于思考的前沿，走在你的前面</li><li><strong>艺术超越了语言能表达的东西，否则它就不是艺术，而是宣传</strong></li><li>发源于未知世界，并提供给你一些信息</li></ul><h2 id="索尔仁尼琴"><a class="markdownIt-Anchor" href="#索尔仁尼琴"></a> 索尔仁尼琴</h2><p>他是一位二战上了前线的苏联士兵，是存在主义作家，相关人物还包括撰写《活出生命的意义》的维克克多弗兰克、哈维尔等。</p><p>其著作《伊凡德尼索维奇的一天》是苏联时期第一本公开描述集中营的书，他还凭借《古拉格群岛》获得诺贝尔奖。</p><h4 id="1古拉格群岛"><a class="markdownIt-Anchor" href="#1古拉格群岛"></a> 1.《古拉格群岛》</h4><p>《古拉格群岛》于1973年出版，之后Samizdat地下传阅，1989年再次公开出版。古拉格，即纠正性劳动营主管部门，因为当时认为人们会犯罪是由于沙皇俄国体制的压迫，所以让一些罪犯（强奸犯、抢劫犯、小偷等）管理集中营，而集中营中关押着的是与“特权”有关的人士，带着基于阶级和种族的罪名。<strong>这本书书例证劳动人民的乌托邦可以实现的想法的幻灭</strong>。《昨日的世界》中也描绘了苏联邀请西方知识分子去参观的情景。1930年开始，《1984》、《动物农场》也都开始揭露一些现象。</p><p>这本书包括了压迫性苏联体制的产生，斯大林统治下全面展开和共产系统。当时人们把苏联解体归咎于斯大林的个人崇拜扭曲了最初准确的主义，认为如果列宁活得就一些，乌托邦就可以实现了。索尔仁尼琴从根本上反驳的了个观点。他梳理了主义与列宁定制的某些法律之间的问题，比如清除异己、个人崇拜、专制权力、无处不在的监控，KGB等。当时的情况下，即使是坚定的党员也无法幸免。毫无缘由地，即便他们没有对D犯下任何错误。<strong>人类的心无法承受被心爱的斧头所伤，却还要证明那把斧头是智慧的。</strong></p><p><strong>帕累托分布</strong>掌管了金钱分布、公司关系等情况，支配了几乎所有创造性生产的领域。这是<strong>一个根本性原则</strong>，而目前没人知道该怎么有效且持续地把资源从几乎掌握一切的人手里撒到下层几乎什么都没有的人那里（虽然顶端的人会变化）。<strong>即使是通过抛硬币决胜负</strong>，财富也会最终集中到少数人手里，时间足够长后，甚至会集中到一个人手里。</p><p>社会的病态和个体的病态间根本联系在于，<strong>个体倾向于欺骗自己</strong>，从而无法以真实真诚的方式行事。最终个体变为虚无主义者，或由于品格被逐渐削弱，不真诚称为了生活的一部分，转向意识形态和极权主义的解决方案，放弃恰当生活，放弃个体责任。而无法<strong>真诚真实地行事会导致走向虚无主义或极权主义</strong>。</p><p>如何分辨一个被意识形态控制住的人？其实一旦你掌握了他们意识形态底层结构的五六个公理，你<strong>甚至可以预测</strong>他会说什么（比如休蒙格斯采访视频的例子）。人们选择被意识形态控制，因为这减少了他们思考的负担，也让他们相信自己完全掌握了世界上所有的知识，而且相信自己不需要思考就可以分辨出谁在善的一边。</p><h4 id="2圣经故事"><a class="markdownIt-Anchor" href="#2圣经故事"></a> 2.圣经故事</h4><p>《巴别塔》，极权主义大厦或乌托邦，越建越好，要容纳更多的因素，更多不同的人，最终会成为一盘散沙。洪水一般隐喻是来自神的惩罚，包括事物瓦解的趋势、人类对罪的趋向、熵增原理等。</p><p>《失乐园》，理性思维产生的政治的、意识形态的理性建构与引导着人类组织的超然神话间有一种紧张的关系。<strong>上帝的最高天使撒旦就是这种经理性思维的拟人化象征</strong>。这类思维倾向于产生极权系统，并爱上极权系统，系统之外的东西都不允许存在，而最终将自己投入地狱。</p><p>艺术家、诗人、哲学家先后了解到未知。</p><h4 id="3箴言"><a class="markdownIt-Anchor" href="#3箴言"></a> 3.箴言</h4><p><strong>苦难是存在的一部分</strong>，这是存在主义的基础观点之一，大部分伟大宗教体系也拥抱这一观点。</p><p><strong>苦难的三大来源</strong>如下：</p><ul><li>自然界的人性，人生而有限</li><li>社会结构的武断审判（无论你处于哪个社会中都会有不同程度的专断）但你需要和他人生活在一起，你要找到你适合的地方</li><li>我们自身也对某些不必要的痛苦负有责任。你本可以做某些事，让你的生活以及别人的生活得到改善；你有未承担起来的责任，而你的良知明明知道。<strong>人心真的有良知，听一听来自良知的劝告（conscience）</strong>。</li></ul><p>正确度过人生的方式是：<strong>真诚地存在，拒绝参与说谎和欺骗，让你的语言/行动尽可能真实，为你的生活（也许还有其它人的生活）负起责任</strong>。这样做是有意义的、负责任的、高尚的。这样做有助于减轻痛苦，否则痛苦会带来虚无主义，或让人逃入极权主义的怀抱。</p><p>你需要一些东西来抵抗你自己的脆弱性，你可以采纳<strong>别人给你制定好的</strong>对现实的综合描述，有种描述把世界简洁地分为天真的无辜受害者和犯了罪的苦难制造者，而且他们都和你无关，这不是评估世界的合理方法，<strong>苦难是与生俱来的。即使你精神/心理没有出现问题</strong>，事情也向糟糕的方向发展了，但我们仍然有前进的方向，选择活得高尚一些，让你可以忍受你自己，甚至可能尊敬你自己，因为你能直面那可怕的脆弱和痛苦。</p><p>[ 避免欺骗、承担责任、试图改善 ]</p><p>什么是真实？ 你看到什么，听到什么，做什么，和谁在一起。有一种从心灵深处满溢出来的不懊悔，也不羞耻的和平与喜悦。爱你所爱，行你所行，听从你心，无问西东。</p><p><strong>成为你自己</strong></p><p>你很清楚你没有完全实现自己的潜能，你造成了部分的苦难，也许你可以换一种看待世界的方式，换一种行动的方式，不要再浪费眼前的机会！<strong>人心真的有良知</strong>，我们并不知道那是什么，但你要听从良心的劝告。以你真实自我建立起来的亲密关系会更强健、更愉快，一个完整的你，通过和伴侣协商，活出真实的生活，这是也是养育孩子的基础。</p><p>实际上人们不仅不做他们应该做的、让情况变好的事，他们还积极地把事情搞糟，因为他们**（我目前是他们的一员）**怀恨在心、怨气冲天、狂傲自满、尔虞我诈甚至杀人如麻，所有这些病态都纠缠在一起。</p><p>你振作起来，多大程度上活出真实的自我（不要误解为放浪形骸，而是存在主义中恰当生活的三个步骤）不仅关乎你一个人的命运，而是关乎所有与你产生联系的人的命运：</p><ul><li>不要低估自己的影响力，它如同涟漪</li><li>你的所作所为非常重要，大多数事情都是有意义的，这同时意味着你需要承当由此而来的责任</li><li>如果你过着病态的生活，你病态化了整个社会，如果有足够多的人这样做，社会会变成什么样子？</li><li>虚无主义者很痛苦，<strong>但他们的优势在于不用承担责任</strong>，是你自己你放手让你的价值体系崩溃了，于是你无需承担责任，代价是时常痛苦，但你可以一直哭哭啼啼，人们会为你感到难过</li></ul><p><strong>人们需要在生命中的某阶段时间全身心参与某项游戏</strong>，从某时刻起，你得在某一方面有所成就，即使你牺牲掉了其他所有的可能，但你必须做出选择（某种职业、某种价值观），否则会徒增年岁而依旧混乱。</p><h2 id="大脑边缘系统"><a class="markdownIt-Anchor" href="#大脑边缘系统"></a> 大脑边缘系统</h2><h3 id="一-人格神经科学相关人物"><a class="markdownIt-Anchor" href="#一-人格神经科学相关人物"></a> 一. 人格神经科学相关人物</h3><p><strong>汉斯艾森克</strong></p><p><strong>格雷</strong>，对动物行为学、神经解剖学和神经精神药理学都非常熟悉，极大拓展了我们对至少两个人格特质（外向性和情绪不稳定性）的生物学和演化学基础，著有《焦虑神经心理学》。与之相关的还有<strong>控制论</strong>，MIT的诺布特维纳，是早期人工智能科学家。智能个体是目标导向型的，在达成目标的过程中，以减少与目标的偏差来组织自己的行为。</p><p><strong>勒杜</strong>是情感神经科学家，给格雷（多集中于海马体）的研究做了很多补充。</p><p><strong>斯万森</strong>研究发育解剖学，对大脑在胚胎发育以及之后的阶段如何逐渐成熟感兴趣。大脑是一个动态、发育、不断变化的系统，心理学家一般只关注成熟的大脑，与皮亚杰理论有偶然性的相符。</p><h3 id="二-人类的目标范围有限"><a class="markdownIt-Anchor" href="#二-人类的目标范围有限"></a> 二. 人类的目标范围有限</h3><p>现实异常复杂，大脑也是非常复杂的。即使在做及其简单的事情，我们都需要屏蔽掉几乎所有的其它信息。作为人类感知能力非常受限，基本对任何事物都是“瞎的”。比如以猩猩实验为例，表明如果专注于目标的话，我们的注意力并不容易被打扰。</p><p>在实际生活中，我们需要首先构建框架，之后才能和世界互动。我们有很多恐惧管理的要素，比如爱国主义、文化认同、宗教信仰，死亡焦虑、等级制度，社会体制等。社会制度帮我们阻挡了复杂性，我们的身体也帮忙屏蔽了一些事情。</p><p>弗洛伊德提出人类有几项基本动机，比如口渴、饥饿、痛苦（心理上的抑郁与孤独、生理痛苦等）、愤怒（攻击性、掠夺攻击性、防御攻击性 &lt;与情绪不稳定性有关&gt;）、体温调节、恐慌、归属感和关怀、性冲动、探索与玩耍（大部分哺乳动物需要通过玩耍来社会化、父亲与孩子的打闹游戏、人们天生有很强的探索性，受下丘脑管控）。</p><p>这些动机的作用在于，有了动机才方便设定目标，情感会根据目标来引导你，让你准备行动并建立与世界互动的感知框架。</p><p>多个动机组成整个人格，可以将动机视为“微人格”，这和精神分析学派相符。<strong>大脑皮质是无法战胜下丘脑的</strong>，俗称“恼羞成怒”。下丘脑是一个古老的区域，是赖以生存的根本，是处理紧急状况的主宰，只有当你不受任何（在各种维度上）干扰的时候，大脑皮质才是掌管者。</p><h3 id="三-你不只在大脑里"><a class="markdownIt-Anchor" href="#三-你不只在大脑里"></a> 三. &quot;你&quot;不只在大脑里</h3><p>你的神经系统遍布全身，相关实验比如盲人实验表明你不仅仅是用眼睛看的，你的大多数感觉会映射到神经系统的不同层面，思考的过程太慢了，有一些过程并不经过“思考”这个中间解释过程。</p><p>“大脑”是一个连续体，所以要在身体各部分层面都注意照顾好自己。</p><p>皮亚杰认为从婴儿开始，人的意识是从下到上发展的，首先是脊柱相关的意识，最后才到大脑皮层的高级意识，设计机器人的科学家也是这样做的。</p><h3 id="四-大脑"><a class="markdownIt-Anchor" href="#四-大脑"></a> 四. 大脑</h3><p>两个半脑中，左脑更多的负责已知、秩序和集中注意力，右脑则负责感知未知。大概在做梦的时候右脑将新知识传递给左脑，所以<strong>一定要保证睡眠！！</strong></p><p><strong>下丘脑</strong>：整个感官-运动-感知-行动的等级系统的构建方式，需要让你的基础动机状态维持满足，所以<strong>你必须管理自己的行为</strong>。在基础动机被满足的限制下，恰当地排列组合。同时你要注意到社会环境！！！下丘脑等级组织需要满足许多参数条件，考虑到他人，同时考虑到未来。意识可能存在于丘脑和大脑皮层之间（正反馈过程）的<strong>交互过程中</strong>，而不是仅仅产生于某个区域。</p><p><strong>海马体和杏仁体</strong>：海马体会监视那些根据你的理解，你希望发生的事（动机状态），同时它会观察世界（你自己诠释后的世界），评估这两者是否一致，如果匹配（即你知道你在干什么、你在哪儿）则会抑制杏仁体和下丘脑，让网状激活系统保持冷静状态。如果出现偏差，会激活网状回路，启动情绪，特别是消极情绪，同时也会准备积极情绪系统以及下丘脑的一些神经回路。</p><p>我们并没有熟悉大脑结构，大脑是未知区域。<strong>我们对大脑部位的命名并不一一对应，这只是为了解剖学上的方便</strong>。人类是一种极其有趣的动物，极其发达的大脑皮层，复杂的运动输出系统。你不能直接将动物视为一种简单的“刺激-响应”机械系统，这是简单的行为主义，但大脑还有其它层次，比如行为主义、认知心理、心理动力等学科研究的都是大脑不同的层面（盲人摸象？）。随着进化与发展，人类既是简单机器（古老的反射），也是有些复杂的机器，同时也是及其复杂的机器，<strong>以及另外一些你根本不知道的东西</strong>。（谦卑虔诚一些，不要自大）</p><h3 id="五-基础情绪"><a class="markdownIt-Anchor" href="#五-基础情绪"></a> 五. 基础情绪</h3><h4 id="1-积极情绪"><a class="markdownIt-Anchor" href="#1-积极情绪"></a> 1. 积极情绪</h4><p><strong>不是所有的激励奖赏机制都是后天习得的</strong>，这也是行为主义的盲点，它忽略了古老的无条件奖赏回路。比如躲避蛇、血、牙齿、低频大声嚎叫等不需要后天习得，而是自动反应。</p><p>第一种积极情绪，比如吃了三明治后你不再饿了的饱足感（西西弗推石），无条件反应，无需后天学习，完成性奖赏。这与血清素含量相关，满足感，没有过多的积极或消极情绪，没有过多的担心和兴奋，就是满足。</p><p>第二种积极情绪，包括好奇、期盼、希望等，激励奖赏或条件性奖赏，那种朝着所向往的奖赏前进的动力。它们根植于下丘脑的多巴胺能系统，“那儿有好东西，我要去拿到它！”。这可以后天养成与激活，但也有部分是天生的。一半下丘脑制定具体目标，另一半进行探索。多巴胺系统激活，会产生探索、外向、开心、玩耍、热情，会自信，新奇事物积极的一面。</p><p>通常人们对即将到来的完成性奖赏所产生的情绪比真正获得奖赏时更强烈。我们的探索欲非常强，新奇事物本身跟现象学主要关心的内容相关，<strong>即意义的揭示</strong>。积极情绪推动你向理想目标迈进，消极情绪保护你，如果发生负面情况，你会停下来，重新理清你的行为或行为框架（这里有漏洞），越过障碍（有的时候你直接抛弃了问题）。<strong>一个情绪稳定、非神经质的人</strong>会很快将烦恼抛之脑后，重新开启探索回路，尤其是外向开放性还很高的时候。而<strong>也有人会产生奇怪的联想，并且认为自己是个“坏人”，掉入漏洞后，焦虑害怕，难以走出来</strong>。</p><h4 id="2-消极情绪"><a class="markdownIt-Anchor" href="#2-消极情绪"></a> 2. 消极情绪</h4><p>与防御和回避相关，痛苦、悲伤、沮丧、失望还有恐惧，恐惧、厌恶等都有自己独立的回路。血清素可以抑制消极情绪，血清素能系统是古老的系统，就像大脑中的管弦乐队指挥。GABA缓解焦虑，内生的，安定和巴比妥药物、酒精等激活gaba。对于某些人来说，酒精是非常容易成瘾的，它影响gaba，多巴胺系统，奖惩机制。</p><h4 id="3综合"><a class="markdownIt-Anchor" href="#3综合"></a> 3.综合</h4><p>两个情绪系统是独立的系统，哪一个力量更强大一些会造成情绪稳定性和外向性的不同表现，而<strong>这种倾向性大多是天生的</strong>。</p><p>女性相对男性更加情绪不稳定（半个标准差），而且越是男女平等的地区越明显，社会越平等，两性之间的差异不是减小，而是增大，所以并不是所有的特征都是社会化的结果。可能的原因在于，女性体型较小，上身较弱，在争夺优势地位的冲突中，体力不如男生；性方面更容易受伤；需要照顾婴儿，可能是为了适应女性+婴儿的系统，让女性的消极情绪系统更强，保护婴儿。</p><blockquote><p><strong>意料之外的事情发生后，你需要多大程度的担心？</strong></p><p>可能是你没有正确认知/诠释世界；</p><p>可能由于你不切实际的计划；</p><p>这里受情绪的影响很大，不同的人有不同的情绪稳定程度，没有绝对的优势，都是视情况而定</p></blockquote><p><strong>人们不喜欢承认自己对世界的高级抽象出了问题</strong>，因为整个复杂性会涌上来。所以在恋爱中不要这么做。抽象化和范畴化是一种高级能力。</p><p><strong>为什么人会受精神创伤？</strong></p><p>首先二元抽象也带来危险，过度泛化，在某件事情上容易得出，“我不是一个好人，所以我就是一个坏人”，但这其实是在价值结构中逐步抽象，环环相扣的过程。</p><p>另外，抑郁症患者容易从一个很小的，难以参数化的所谓“错误”中，迅速归纳出最高级别的抽象意义（虽然在另一个角度抽象是个很棒的能力），比如我犯了错误A，所以我无用/坏人，这种思维很不好，忽略了差异性。</p><h2 id="大五人格"><a class="markdownIt-Anchor" href="#大五人格"></a> 大五人格</h2><h2 id="相关资源"><a class="markdownIt-Anchor" href="#相关资源"></a> 相关资源</h2><p>教授中文网站，<a href="https://cn.jordanbpeterson.com/" target="_blank" rel="noopener">https://cn.jordanbpeterson.com/</a></p><p>《人生十二法则》、《人格及其转变》课程 [B站，<a href="https://www.bilibili.com/video/BV1AW411M7vL" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1AW411M7vL</a>]</p><p>《哈佛幸福课》</p><p>王德峰教授讲座系列 [B站可搜索到，但被屏蔽了好多，喜马拉雅有音频资源]，包括王教授讲的《坛经》、《传习录》、《资本论》等</p><p>欧丽娟老师讲《红楼梦》[喜马拉雅有完整音频，B站有一些精彩节选]</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>哈佛《正义课》笔记</title>
    <link href="/2021/05/07/fair-havard/"/>
    <url>/2021/05/07/fair-havard/</url>
    
    <content type="html"><![CDATA[<h2 id="背景材料"><a class="markdownIt-Anchor" href="#背景材料"></a> 背景材料</h2><p>迈克尔.桑德尔教授是哈佛最受欢迎的教授之一，他说过“我的目标不是试图用什么理念去说服学生，而是把他们训练成有头脑的公民”。他在29岁时就成为社群主义向自由主义发起挑战的标志性人物。刘擎教授在《刘擎西方现代思想讲义》中有专门章节介绍。</p><p>课程视频：<a href="https://www.bilibili.com/video/BV1ct4y167fM%EF%BC%88B%E7%AB%99%EF%BC%89%E5%8F%A6%E5%A4%96%E5%8C%85%E6%8B%AC%E8%AE%B2%E5%BA%A7%E3%80%8A%E9%87%91%E9%92%B1%E4%B8%8D%E5%8F%AF%E4%B9%B0%E7%9A%84%E4%B8%9C%E8%A5%BF%E3%80%8B" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1ct4y167fM（B站）另外包括讲座《金钱不可买的东西》</a></p><p>课程讲稿发表为书籍《公正》，相关书籍还推荐《洞穴奇案》。</p><p>[ 但个人阅读体会觉得富勒写的第一部分比较精彩，萨伯补充的后九个观点略显逊色些 ]</p><p><em>对于哲学问题没有一劳永逸的解决方式，也许我们会产生怀疑论的回避，但永远无法平息内心渴望理性思考的不安</em></p><h2 id="功利主义与道德主义"><a class="markdownIt-Anchor" href="#功利主义与道德主义"></a> 功利主义与道德主义</h2><h3 id="一-引入"><a class="markdownIt-Anchor" href="#一-引入"></a> 一. 引入</h3><p>以电车难题为例引入。</p><p>最初版本中<strong>我是一辆失控列车上的司机</strong>，大多数人选择正方观点即会转动方向盘撞向一个人。正方理由是当只需要牺牲一个人的时候，不应该选择牺牲五个人。而反方同学提出，认为这和极权主义和种族主义类似；（这上升的好高）</p><p>第二个版本中<strong>我是一个站在桥上的旁观者，身边站着一个如果被推下桥之后可以挡住火车的胖子</strong>，我是否选择推他？现在大多数人选择不会把他推下去。理由是：1）因为这是主动选择把一个完全没有关系的人牵涉到灾难中（但其实情景一种的人原本也没有在灾难当中）；2）第一个场景中我只能选择撞死一个还是死五个，而第二个场景中，作为我本身来讲，我是个<strong>旁观者</strong>，我可以选择和胖子一起置身事外；进阶的第二个版本中，我不需要直接推胖子，而是有一个和火车上一样的方向盘，转动之后他就会掉下去。</p><p>另一个例子是急救医生和六个病人的故事。</p><p>第一个版本，有一个病人受重伤，而另外五个人伤势较轻，你可以选择救治重伤的那个但同时剩余五个会死，或者你可以选择放弃重伤的人而救治其它五人。大部分人选择了救治轻伤的五人。</p><p>第二版本中，我是一个器官移植医生，我有五个病人每个都需要立马进行不同的器官移植手术，但我没有器官捐赠者，但隔壁有一个过来体检的健康人，你还会选择“牺牲一个而救五个”吗？大多数人不会。</p><p>整个讨论中涉及到一些道德原则：</p><ol><li><strong>后果主义道德推理（Consequentialist）</strong>：判断一个选择是否是道德的和正确，取决于你的行为所导致的后果。所以“牺牲一人而救治五个”是对的。</li><li><strong>绝对主义道德推理（Categorical）</strong>：判断行为本身的动机而非行为的后果，所以即便可以救五个人，但杀掉一个无辜的人是错误的。</li></ol><p>后果主义道德推理中最具有影响力的就是边沁提出的<strong>功利主义</strong>，而绝对主义道德推理中最著名是<strong>康德</strong>。</p><p>哲学就是让我们面对自己熟知的事物，然后引导我们动摇原有的认知。将我们熟知的、毋庸置疑的事情变得陌生。而一旦熟悉的事物变得陌生，它将再也无法回到从前。<strong>自我认知就像逝去的童年</strong>。</p><p>另外，通过讨论课程中的议题，可以拥有更敏锐的政治嗅觉，更有效地参与公共事务。但同时也可能变成一个“更糟”的公民。因为<strong>哲学会使人脱离现实，甚至可能弱化其行动力</strong>。</p><p>而面对这些风险其实有一种非常典型的回避方式，那就是<strong>怀疑论</strong>，怀疑一切，认为凡事都没有定论，而且我们也解决不了。但是！虽然我们解决不了，但这些问题是无法避免的，我们需要在日常生活中，一次又一次地回答这些问题。</p><blockquote><p>怀疑论是人类理性暂时休憩的场所，但绝非理性的永久居住地。简单地默许怀疑论，永远无法平息内心渴望理性思考之不安。</p></blockquote><h3 id="二-功利主义"><a class="markdownIt-Anchor" href="#二-功利主义"></a> 二. 功利主义</h3><p>边沁的核心观点，公正的选择最是最大功利化。<strong>功利</strong>即快乐减去痛苦，幸福减去苦难。人的本性就是趋乐避苦的，所以要全方位地最大化地提升幸福。<strong>“为最多的人谋求最大的幸福”</strong>。</p><p>19世纪英国的一则法律案例：女王诉讼德利和斯蒂芬斯案件（海难的幸存者在过程中吃掉了一个将死的人）。和《洞穴奇案》大致相似。</p><p>——他们是否有罪？</p><p>有大概20%的人认为这个行为是道德上允许的。</p><p>一部分人认为不可以，因为这没有征得派克（被杀死并吃掉的孤儿）的同意。如果抽签之后，被抽到的人后悔了，是否还可以继续？</p><p>——如果过程征得了同意，采用了抽签方法，而最终派克也答应了，那么杀死并吃掉他是否有罪？</p><p>有一种观点认为，谋杀就是谋杀，是为了改善自身状况而犯下的罪行。</p><p>—— 如果杀死一个人可以救30人呢？300呢?更多人呢？</p><p>总结上述讨论可以<strong>引发以下三个问题</strong>：</p><ul><li>为什么谋杀就是绝对错误，我们作为人的一些基本权利是从哪里来的呢？</li><li>为什么只要经过了特定的公平程序（比如抽签）之后，不管任何结果都变得正当了呢？</li><li>“征得同意”有什么样的道德作用？</li></ul><h2 id="边沁功利主义的价值衡量"><a class="markdownIt-Anchor" href="#边沁功利主义的价值衡量"></a> 边沁功利主义的价值衡量</h2><p>功利最大化原则不只针对个人，也适用于共同体（成员的集合）和立法者。</p><p>功利主义的逻辑也通常被称为成本效益分析，而被政府和企业使用。</p><h3 id="一-统一的衡量方式"><a class="markdownIt-Anchor" href="#一-统一的衡量方式"></a> 一. 统一的衡量方式</h3><p>【案例】捷克的烟草公司研究结果表明，让捷克人吸烟能让政府获利。具体来讲，虽然会增加患病率和相应的医疗支出，但人们的早逝也节省了政府的医疗支出、养老金、老人住房支出等。那么其中被忽略的生命的价值呢？该如何计算？</p><p>【案例】假设在911前逮捕了一名恐怖分子，你坚信他掌握了恐怖活动的信息，是否可以选择对他进行严刑拷打，或者你有尊重个人权利的绝对道德责任？</p><p>【案例】福特平拖车，邮箱会在追尾时爆炸而造成严重伤亡。福特公司早就知道油箱的缺陷，但分析出给每辆车装一个隔板（仅11美元）的代价，比出事之后赔钱的代价（每人20万美元）要高，所以没有采用。而人命是否可以用金钱衡量？</p><p>【案例】女校中有规定不允许男生留宿，并声称这是因为会增加学校的开销（比如热水、床垫等），后来改为每个女生每周可以最多留男士过夜三次，而且过夜男士每次要付50便士补贴学校。于是有新闻头条“圣安妮女生，五十便士一夜”。</p><p><strong>对功利主义的怀疑</strong>：</p><ol><li>是否充分尊重个体和少数的权利与尊严；</li><li>所有价值都可能用金钱衡量吗？是否所有价值都可以完好无损地转换为功利主义的形式？高级快乐和低级快乐之间需要区分吗？</li></ol><p>【案例】桑代克的调查，清单上写着比如“如果给你__元，你就可以放弃一颗牙/吃一条蚯蚓/掐死一只猫”之类的问题。</p><p><strong>如果我们珍视的东西不能够用统一方式衡量，那么功利主义有什么存在的意义？</strong></p><h3 id="二-高级快乐-vs-低级快乐"><a class="markdownIt-Anchor" href="#二-高级快乐-vs-低级快乐"></a> 二. 高级快乐 vs 低级快乐</h3><p>边沁认为只要快乐的总量相等，针戏（一种游戏）和诗一样好。谁可以说这些快乐中，哪些更高级？</p><p>近代功利主义者<strong>约翰 斯图尔特 穆勒</strong> 试图将功利主义人性化，将其中的演算放宽和修正。其著作《功利主义》明确指出功利是道德的唯一标准，我们的实际欲望是道德的唯一基础。</p><p>但他认为功利主义者可以区分高级和低级的快乐。“只要你体验过两者，你就会自然一直偏好更高级的那一种”，“当痛苦的人胜过作快乐的猪”。**真的吗？**现实中的调研结果是《辛普森一家》远比莎士比亚更受人们喜爱，虽然大多数人觉得莎士比亚更高级。为什么？</p><ul><li>一种观点认为，我们被老师和周围环境教导成这样；</li><li>有同学提出，莎士比亚是可以反复品味的作品，所以是更高级的快乐</li><li>也许人们需要受教育才能理解更高级的快乐</li></ul><p>穆勒认为，即便我们在二者中选择了低级快乐，我们也知道另一种才是更高级的。</p><p>而针对上一节中功利主义不能做到充分尊重个体权利的质疑，穆勒认为个人权利公正神圣、至高，但它还是在功利主义的范畴中，当你考虑到社会整体的长远发展时，就会选择公正和尊重权利。但这也似乎带来了更多对于功利主义的质疑。</p><p>边沁身体力行地践行着功利主义，将自己的遗体捐献展览，供人们观看学习。</p><blockquote><p>道德就是计算生命、权衡得失呢，还是某些道德责任和人权是根本性的，以至于它们超越于这样的计算之上？如果某些权利是自然的、神圣的、不可剥夺的和无条件的，我们该如何甄别它们？而它们的根本性又来自哪里呢？</p></blockquote><h2 id="自由主义与社会主义"><a class="markdownIt-Anchor" href="#自由主义与社会主义"></a> 自由主义与社会主义</h2><p>带着对穆勒提出的两个观点的进一步质疑，我们思考是否存在“良善生活”的理论，能为快乐的价值，提供独立的道德标准？</p><p>一些权力理论认为，个人是独立的存在，而不仅仅是用来实现功利最大化的工具，所以不应该以偏好和价值的总和来考虑公正。</p><h3 id="一-自由主义"><a class="markdownIt-Anchor" href="#一-自由主义"></a> 一. 自由主义</h3><p>个人的基本权利是自由权，我们是独立的存在，不能被利用去满足社会需要。</p><p>基本理念：只要我们尊重他人同等的权利，我们就有权自由选择自己喜欢的生活。</p><p>在自由主义之下，政府是什么角色呢？该理论认为，</p><ul><li>反对家长式立法，制定保护人们免受自身行为伤害的法律是不合理的；比如不能强迫人们系安全带。</li><li>反对道德式立法，不要试图提升公民道德，弘扬道德，这违反了自由权；比如进制同性恋行为。</li><li>任何为了劫富济贫，进行收入或财富再分配的政策都是不合理的。</li></ul><p>诺齐克和其它自由主义者主张“小政府”角色，政府税收仅用来提供所有人都需要的服务，比如治安、国防、强制履约和保护产权的司法系统等。</p><h3 id="二-财富分配"><a class="markdownIt-Anchor" href="#二-财富分配"></a> 二. 财富分配</h3><p>自由主义者认为我们仅仅属于我们自己，而非整个社会，人人自主。<strong>有关财富分配，诺奇克有两方面观点</strong>：1）人们是否公平地获得生产资料；2）财富分配是否是基于自由选择而达成的交易。</p><p>对富人征收更高的税这个政策其实是符合功利主义观点的，所以不要被名字中的“功利”二字误导。相反的，自由主义强调尊重个人，认为如果符合上述两个条件就不应该向他多收税。</p><p>所以，思考**向富人征税救济穷人是否是正确合理的？**是否有真正的机会均等？征税是否等同于盗窃或强迫劳动/奴役？</p><p>而关于公共产品也有类似的问题，比如私人的消防公司，只负责为其会员灭火的例子。</p><p>以下列出一些反对自由主义的理由：</p><ul><li>对于等量的金钱，穷人更需要；</li><li>征税算不上奴隶，这是合法的；</li><li>想比尔盖茨这样的任务难道不对社会有亏欠（比如社会安定、交易自由等因素）而需要回报社会吗？</li><li><strong>我们选择生活在社会中，是无法完全自我拥有的</strong>（攻击自由主义的最根本主张）</li></ul><h3 id="三-私有财产权"><a class="markdownIt-Anchor" href="#三-私有财产权"></a> 三. 私有财产权</h3><p><strong>约翰洛克</strong>解释了<strong>私有财产权</strong>的兴起，认为财产权是一种<strong>先于政治</strong>的自然权利，依附于个体而存在。</p><p>我们不但拥有自己，更进一步地，我们拥有自己的劳动，因此，任何施加了我们劳动的无主物品都是我们的财产。比如我们种地的话，不仅作物属于我们，连土地也是我们的。私有财产无需他人同意，不需要政府或其它契约的认定。（这部分观点与自由主义者类似）</p><p>在<strong>自然状态</strong>下，每个人都是自由平等的，没有等级制度。但这和放纵状态不同，自由状态下遵循<strong>自然法，其中的唯一制约</strong>是我们<strong>不能放弃</strong>自己的自然权利，<strong>也不能剥夺</strong>他人的。生命、自由和追求幸福的权利都是不可剥夺的权利。（这是和自由主义有所不同的地方）</p><blockquote><p>自由绝不是让我们为所欲为。</p></blockquote><p>洛克认为有两点理由：1）因为人是上帝的产物，而不仅仅是自己的；2）不可剥夺的权利一方面使我们不够完全自由，另一方面又使我们更加深入地拥有；</p><p>【举例】西方国家，尤其是美国宣称他们有研发新药的机构，希望世界各国都遵守药品专利，但疾病在南非爆发时，来自美国的治疗药物是天价，所以南非政府低价购买了非专利转录药品。（我不是药神）</p><p>而一旦进入社会，自然权利会发生怎样的改变？洛克的主义中将会如何定义政府的角色？但“怎样才算我们的财产？”,&quot;<strong>怎样才算</strong>尊重我们的生命与自由？&quot;这些是由这个“有限的”政府来界定的。所以，洛克是在自相矛盾吗？</p><h3 id="四-合法政府"><a class="markdownIt-Anchor" href="#四-合法政府"></a> 四. 合法政府</h3><p>**在洛克所谓的自然状态下为什么要建立政府呢？**洛克认为是为了方便解决一些不便之处，每个人都是自然法的执行者，拥有处罚权。但当人们审理和自己有关的案件时会丧失理智。</p><p>而<strong>同意行为</strong>就是脱离自然状态的手段，我们同意<strong>放弃执行权</strong>而建立一个政府或共同体，同意遵守大多数人的决议。但注意，我们只是放弃了执行权，多数人依然不能侵犯我们的自然权利（生命、自由、财产权）。</p><h2 id="个人权利与政府权力"><a class="markdownIt-Anchor" href="#个人权利与政府权力"></a> 个人权利与政府权力</h2><h2 id="公民义务及生殖权利"><a class="markdownIt-Anchor" href="#公民义务及生殖权利"></a> 公民义务及生殖权利</h2><h2 id="康德的道德主义"><a class="markdownIt-Anchor" href="#康德的道德主义"></a> 康德的道德主义</h2><h2 id="善意谎言与相对正义"><a class="markdownIt-Anchor" href="#善意谎言与相对正义"></a> 善意谎言与相对正义</h2><h2 id="公正起源于社会分配"><a class="markdownIt-Anchor" href="#公正起源于社会分配"></a> 公正起源于社会分配</h2><h2 id="平权运动与种族歧视"><a class="markdownIt-Anchor" href="#平权运动与种族歧视"></a> 平权运动与种族歧视</h2><h2 id="公民自觉与选择权利"><a class="markdownIt-Anchor" href="#公民自觉与选择权利"></a> 公民自觉与选择权利</h2><h2 id="忠诚义务与爱国主义"><a class="markdownIt-Anchor" href="#忠诚义务与爱国主义"></a> 忠诚义务与爱国主义</h2><h2 id="同性婚姻与堕胎权利"><a class="markdownIt-Anchor" href="#同性婚姻与堕胎权利"></a> 同性婚姻与堕胎权利</h2><h2 id="金钱不可买到的东西"><a class="markdownIt-Anchor" href="#金钱不可买到的东西"></a> 金钱不可买到的东西</h2><p>市场具备两个优点：1）市场经济为经济增长做贡献，可以协调管理商品生产，有效组织生产活动；2）提供了一定程度上对自由的体现。</p><p>我们尝到了市场经济的甜头，而被一种假设所蛊惑，即认为市场、市场逻辑、金钱、经济逻辑可以为人类界定公共利益，演绎公正社会。</p><p>【市场社会】是把一切人类活动都与价格挂钩，金钱和价格统治着生活。<strong>我们是否想生活在这样一个一切皆生意的社会？</strong></p><p>从17岁少年卖肾买iphone的故事引入，大家是否同意在这个过程中肾是可以自由买卖的，这过程中是否存在不公平的地方。</p><p>花钱雇人去排队买iphone是否可以？以双倍价格倒卖iphone可以吗？如果是过年回家的火车票又如何？应该就是黄牛的意思。火车票只是一种单纯的商品吗？</p><p>单纯的商品和关于道德价值观、家庭、传统以及节庆的商品是否有本质上的不同？</p><p>一所优秀大学的录取通知书是否可以出售？比如为了学校的运营资金，所以拿10%的学位出来拍卖给有钱人家的孩子。比如有钱人给学校捐了一栋楼或者图书馆，然后子女可以去上学。另外，如果这样的交易不是地下的，而是公开宣布出来的，是否合理？也许，一旦在学位上也加入金钱的因素，不论最初的动机是什么，都有可能损害或贬低原本大学之道中对于学术或道德的价值评估。</p><blockquote><p>be deminished or corrupted if dominated by considerations of money and markets</p></blockquote><p>光靠经济学和市场逻辑来思考公共利益是不够的，还要进行到的层次的斟酌，而这时非常复杂的。这个时候人们很容易就会倒向市场逻辑。我们需要加入个人反思、集体反思，考量不同的价值观。这关乎到我们想要怎样一起生活，以及怎样在物质之上把握价值、寻获意义。</p><h2 id="公正该如何是好书籍笔记"><a class="markdownIt-Anchor" href="#公正该如何是好书籍笔记"></a> 《公正该如何是好》书籍笔记</h2><h3 id="一-总引"><a class="markdownIt-Anchor" href="#一-总引"></a> 一. 总引</h3><p>一个公正的社会应当努力推进其公民的德性吗？或者法律是否应当在各种德性观念保持中立，以使公民们能够自由地为自己选择最佳的生活方式？什么样的德性值得尊重和奖赏，以及什么样的生活方式是一个良好的社会应当推进的？</p><p>福利、自由与德性。</p><p>【案例】飓风中的价格欺诈、紫心勋章的获取资格、对华尔街的金融救助计划（奖励贪婪？奖励失败）、电车难题、美军与阿富汗农民。</p><p>道德反思并不是个体的追求，而是公共的努力，它需要一个对话者，超越偏见和日常惯例。</p><p>总结出三种道路：</p><ul><li>功利主义道路，界定公正的方法在于询问什么将会使福利或者社会总体幸福最大化；</li><li>自由至上主义道路，关于收入和财富的正当分配就是任何一个在不受约束的市场中自由交换商品和服务所产生的分配；</li><li>第三种道路，公正就是基于人们在道德上所应得的，以分配物品来奖励和促进道德。</li></ul><h3 id="二-功利主义-2"><a class="markdownIt-Anchor" href="#二-功利主义-2"></a> 二. 功利主义</h3><p>最大幸福原则：</p><ul><li>边沁：私人承包商的环形监狱、乞丐管理、缺乏对个体权利的尊重（罗马竞技场中的基督徒、《离开欧麦拉的人》）、通用价值货币（公民吸烟的益处、福特汽车的油箱、老年人的价格，许多政策的设定都暗含着对人类生命的一个标价、有偿受苦、圣安妮的女生）</li><li>穆勒：《论自由》、长远角度、强制性顺从、关于品格和人类繁荣的理想、高级快乐与低级快乐（莎士比亚与《辛普森一家》）、</li></ul><h3 id="三-自由至上主义"><a class="markdownIt-Anchor" href="#三-自由至上主义"></a> 三. 自由至上主义</h3><p>最小政府、以人类自由的名义，支持不受约束的时常，并反对政府管制。反对家长式立法、道德立法与财富再分配。</p><ul><li>诺齐克：分配公正取决于初始拥有的正当性和财产转移的正当性（篮球明星的钱）、向劳动所得征税就等于强迫人劳动、自我所有权（出售器官、辅助性自杀、经双方同意的吃人、）</li></ul><p>自由市场：1）征兵与雇佣兵和代偿金（黑水公司、外籍军团）。一个民主社会通过市场手段招募士兵是否正当？当代志愿军与雇佣军之间真正的区别是什么？2）代孕、有偿怀孕、有偿精子捐献、试管婴儿、出售婴儿（婴儿M案件）</p><p>不管是基于自由主义还是功利主义，都可以支持代孕。而相应的也有很多反驳理由：是否全面掌握信息、有些东西不能用金钱购买、</p><ul><li>卢梭：涉及到公民善的地方，相较于市场自由交换，国家及其有约束力的法律和规则可能意味着更多的自由；</li><li>安德森：根据功利来评价所有事物，贬低了更适合用更高规范加以评价的各种事物和社会行为。</li><li>康德：道德的最高原则是什么？什么是自由？提出了更为苛刻、崇高的自由观念。</li></ul><p>我们是<strong>理性的存在</strong>，所以值得拥有尊严和尊重。自由不是服从偏好/欲望。自律与他律。当我们自律地行动，也即根据自己所立的法则而行动时，我们做某件事就是为了这件事本身，它自己就是目的（而非因为它有用或便利）。尊重人的尊严就意味着将人当做目的本身来对待。同一种行为背后可能有各种不同的动机。但这个理论最难论证的是例子是如何看待帮助他人这一义务。将义务的动机孤立开，确保它不被怜悯和同情遮蔽。</p><p>义务与倾向，自律与他律，假言命令与绝对命令，理智与感性。</p><p>如何区分绝对命令和假言命令：1）使你的准则普遍化；2）将人看做目的（自杀与谋杀等同）。</p><blockquote><p>科学穷尽其所有的力量与洞见都不能触及道德问题，因为它是在感知领域起作用。</p></blockquote><p><strong>性、谎言与政治</strong></p><p>只有当伴侣双方相互分享他们“无论好坏以及各方面的人、身体和灵魂”时，他们的性行为才能导致“一种人与人的联合”。</p><p>康德理论认为对纳粹分子撒谎阁楼上没有藏着犹太人是违反道德的。教授提出，可以说出<strong>一个真实的、但具有误导性的信息</strong>。从道德上来讲，一个策略性的、真实而具有误导性的表达与一个直率的谎言之间，到底有什么区别呢？一种精心设计的措辞，以某种方式对说实话的这一义务心存敬意，而一个直率的谎言则并不如此。</p><ul><li>约翰罗尔斯：契约/同意的道德局限（1787年的美国宪法、老人的漏水马桶、休谟的房子）、《正义论》、无知之幕</li></ul><p><strong>差异原则</strong>：只有当社会和经济的不平等能够有利于社会最不利者的利益时，它们才是可允许的。</p><p>绝对平等主义的噩梦。</p><p>四种不同的分配公正理论：1）封建制度或种姓制度，基于出生的固定等级制；2）自由至上主义：拥有形式上的机会均等的自由市场；3）精英统治制度：拥有公平机会均等的自由市场；4）平等主义：罗尔斯的差异原则。</p><p>我们的成就至少在部分程度上都取决于那些我们并不能心安理得地拥有的各种自然才能。我们鼓励那些有天赋的人发展并锻炼自己的才能，不过同时也要认识到，这些才能再市场中所获得的回报不仅属于他们，还属于作为整体的共同体。<strong>事情的实然与应然</strong>。</p><p>我们应当反对这样一种论点：制度的安排总是有缺点的，因为自然才能的分配和社会环境的偶然性因素是不公正的，而这种不公正不可避免地必然要转移到人类的制度安排之中。这种思想有时候被用来作为对不公正熟视无睹的借口，仿佛拒绝默认不公正的存在和不能接受死亡一样。</p><p>我们越多地将自己的成功看做自己的行为结果，那么我们就越少地感觉到自己对那些落后者所负有的责任。</p><h3 id="四-亚里士多德伦理学与政治学"><a class="markdownIt-Anchor" href="#四-亚里士多德伦理学与政治学"></a> 四. 亚里士多德伦理学与政治学</h3><p>公正是目的论的，公正是荣誉性的。</p><p>“那些最好的的长笛吹奏者应当获得最好的长笛。”因为亚里士多德认为这就是长笛存在的目的。</p><p>在古代自然更被视为一种拥有某种意义的秩序，理解自然就要抓住其意图和本质性的意义（目的论）。而随着现代科技的来临，自然开始被机械地理解，受制于物理法则。</p><p>大学目的是什么？哈佛不是沃尔玛、不是布鲁明戴尔百货公司。</p><p>如今我们将政治视为一种程序，使每个人都可以选择自己的目标。但亚里士多德则认为政治的目的在于塑造好的公民，培养好品质。如今我们将政治视为一种必要性的恶。</p><p>他批判了寡头制和民主制（多数主义）。</p><p>学成于行。亚里士多德认为道德德性是某种从行动中学到的东西，我们首先需要养成正当的习惯，而这就是法则/政治存在的目的。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（二）图神经网络</title>
    <link href="/2021/05/06/cs224w2/"/>
    <url>/2021/05/06/cs224w2/</url>
    
    <content type="html"><![CDATA[<h2 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h2><p>GCN《<a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">Semi-supervised classification with graph convolutional networks</a>》</p><p>GraphSAGE 《<a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">Inductive representation learning on large graphs</a>》</p><p>GAT《<a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener"><strong>Graph attention networks</strong></a>》</p><p>GNN with skip connection_1 《<a href="https://arxiv.org/abs/1605.06431" target="_blank" rel="noopener"><strong>Residual networks behave like ensembles</strong> of <strong>relatively shallow networks</strong></a>》</p><p>GNN with skip connection_2 《<a href="http://proceedings.mlr.press/v80/xu18c.html" target="_blank" rel="noopener">Representation learning on graphs with jumping knowledge networks</a>》</p><p>DiffPool 《<a href="https://arxiv.org/abs/1806.08804" target="_blank" rel="noopener">Hierarchical graph representation learning with differentiable pooling</a>》</p><p>GIN《<a href="https://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">How powerful are graph neural networks?</a>》</p><h2 id="信息传播与节点分类"><a class="markdownIt-Anchor" href="#信息传播与节点分类"></a> 信息传播与节点分类</h2><p>半监督节点分类问题（<strong>semi-supervised</strong> node classification）</p><p>消息传递框架（message passing framework），关键思想在于，<strong>同类/同标签的节点间倾向于有连接</strong>，也即correlations。</p><p>集体分类（collective classification），节点根据其邻居的标签更新其自身标签。</p><p>相关场景包括比如恶意网页检测、垃圾邮件、欺诈用户检测等等。</p><h3 id="一-基础概念"><a class="markdownIt-Anchor" href="#一-基础概念"></a> 一. 基础概念</h3><p>相关性（correlation）具体体现在以下两个方面：</p><ul><li><p><strong>同构/同质性（Homophily）</strong>：以社交网络为例，具有相似特征的人倾向于相互联系（社会学同质性概念）。具体定义为“The tendency of individuals to associate and bond with similar others”。</p></li><li><p>影响力（Influence）：以社交网络为例，社会关系会影响我们自己的特征或行为；</p></li></ul><p>考虑节点的属性及其邻居节点的标签和属性，确定某节点v标签，方法可以表达为Guilt-by-association。</p><p>可以使用概率框架，依照一阶<strong>马尔科夫假设（Markov Assumption）</strong>，即节点v的标签只取决于其邻居节点们的标签。这里的“一阶”表示我们只考虑当前节点的一跳邻居。</p><p>集体迭代分类包括三个步骤：</p><ul><li>局部分类器（Local Classifier）：为节点<strong>分配初始标签</strong>；这是一个基于节点属性预测标签的标准分类器，与网络结构信息无关。</li><li>关系分类器（Relational Classifier）：捕获节点时间的<strong>相互关系</strong>；此分类器应用到网络结构信息，基于邻居节点属性/标签预测当前节点标签。</li><li>集体推理（Collective Inference）：在网络中<strong>传递相关性/信念（belief）</strong>，直到出现标签并收敛；迭代地将关系分类器应用于每个节点知道相邻节点间的预测结果趋向一致。</li></ul><h3 id="二-经典方法"><a class="markdownIt-Anchor" href="#二-经典方法"></a> 二. 经典方法</h3><h4 id="1关系分类-relational-classification"><a class="markdownIt-Anchor" href="#1关系分类-relational-classification"></a> 1.关系分类 Relational Classification</h4><p>基本思想：节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>的标签概率是其<strong>邻居节点</strong>概率的平均值/边权重加权。有标记节点使用其真实标签，未标记节点，标签初始化为0.5。以随机的顺序更新全部节点概率直到到达迭代最大次数或结果收敛。</p><p>[ 这个方法没有应用到节点属性，也不保证收敛。 ]</p><h4 id="2迭代分类-iterative-classification"><a class="markdownIt-Anchor" href="#2迭代分类-iterative-classification"></a> 2.迭代分类 Iterative Classification</h4><p>基本思想：针对节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>，基于其属性和其节点集合的标签确定它的标签。</p><p>组成：训练两个分类器：1）基础分类器，基于节点属性预测其标签；2）两个输入的分类器：基于节点属性和邻居节点标签（a label summary vector z）预测节点v标签。</p><p>方法：1）训练阶段，完成上述两个分类器的训练；2）在测试集合（unlabeled nodes）中迭代直到收敛，首先使用基础分类器得到初始标签，计算出邻居向量z，使用分类器二得到预测结果；之后进入分类器二的迭代。</p><div align="center">  <img src="/2021/05/06/cs224w2/iteration.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>[ 这个方法不保证收敛。]</p><h4 id="3循环置信传播-loopy-belief-propagation"><a class="markdownIt-Anchor" href="#3循环置信传播-loopy-belief-propagation"></a> 3.循环置信传播 Loopy Belief propagation</h4><p>Loopy表示其适用的图数据中可能有环（cycles）。核心在于potential function/matrix。</p><div align="center">  <img src="/2021/05/06/cs224w2/notion.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>收集下游节点信息，然后结合自己的标签本性，决定其向节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span>传递的信息。</p><div align="center">  <img src="/2021/05/06/cs224w2/bp.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>即便图数据中有环（子图中的各节点不再独立，而是相互影响），我们也可以随意挑选起始节点，然后沿着边传递信息。</p><p>但是有环的时候可能无法收敛，但是实践中效果依然很好，<strong>环不是问题</strong>。最糟糕的情况是下面这样，但在实际中不常见：</p><div align="center">  <img src="/2021/05/06/cs224w2/bpw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>总结一下有关信念传播方法：</p><ul><li>容易代码实现，也容易并行化</li><li>易于应用于各种图模型，可以基于各种潜在矩阵（potential matrix）不一定是上述的label-label矩阵；不只考虑了同构性而是加入了更复杂的关系</li><li>这个方法也是不保证收敛的</li><li>potential matrix需要一定的估算</li><li>是一种非常强大、有效的半监督节点分类方法</li></ul><h2 id="gnn基础"><a class="markdownIt-Anchor" href="#gnn基础"></a> GNN基础</h2><p>之前学习的浅层encoder-decoder节点嵌入方法的局限性在于：</p><ul><li>每个节点有自己的embedding向量，即需要训练<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(|V|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span>个参数</li><li>属于转导学习（transductive），无法应对在训练阶段未出现的节点</li><li>没有考虑节点属性</li></ul><p>Deep Graph Encoders，encoders是基于图结构信息的多层非线性转换。这些编码器可以和之前学到的所有节点相似度计算方法结合。</p><p>这些方法可以应用于节点分类、链路预测、社区发现、网络相似度计算等任务。</p><h3 id="一-深度学习基础"><a class="markdownIt-Anchor" href="#一-深度学习基础"></a> 一. 深度学习基础</h3><div align="center">  <img src="/2021/05/06/cs224w2/sgd.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>常用优化器包括Adam、Adagrad、Adadelta、RMSprop等</p><p>使用深度学习框架使得训练时的梯度计算变得十分容易！</p><p><strong>批量归一化（Batch Normalization）</strong>，用于稳定模型训练过程。给定一批数据，通过变换将其移到均值为0，方差为1的情况。</p><div align="center">  <img src="/2021/05/06/cs224w2/nor.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>Dropout</strong>，防止过拟合。在GNN中，dropout被应用于消息传递即message部分的线性层。</p><div align="center">  <img src="/2021/05/06/cs224w2/dropout.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>激活函数</strong>，非线性化，常用ReLU、Sigmoid、Parametric ReLU（实际场景中基本效果比ReLU要好）等。</p><h3 id="二-图的深度学习"><a class="markdownIt-Anchor" href="#二-图的深度学习"></a> 二. 图的深度学习</h3><p>当图数据中没有节点属性时，一般会选择使用1）指示向量，即节点的one-hot编码或 2）全1向量。</p><p>最简单的方案是直接将邻接矩阵和节点属性矩阵拼接起来，直接加入一个深度神经网络中，类似视为一张图片加入CNN中。但这样做的问题在于：</p><ul><li>需要训练的参数非常多</li><li>无法使用与各种大小的图数据</li><li>对节点次序信息敏感</li></ul><p>将CNN的思想扩展到GNN上，将图片上的邻域扩展成图数据中的<strong>节点邻居集合</strong>。GNN变为两个关键步骤：1）确定某个节点的计算（子）图；2）在其上进行信息传递。</p><p>每个节点都会基于其邻居节点得到它的计算图。</p><div align="center">  <img src="/2021/05/06/cs224w2/graph.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>模型可以有多层，每一层都会有节点的表征信息，第0层节点向量是其输入特征向量，第k层的嵌入向量中考虑了K跳之外的邻居节点信息。</p><p>不同模型的邻居节点<strong>特征汇总方式</strong>会有所不同。注意，邻居聚合函数需要<strong>与节点次序无关</strong>。一般常用Average、sum等。重要的是该模型中所有的节点共享参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span>。</p><div align="center">  <img src="/2021/05/06/cs224w2/gnn.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以上模型也可以写为矩阵分解的形式，最终的矩阵是比较稀疏的。[ 当聚合方式比较复杂时，这种GNN无法写为矩阵形式]</p><div align="center">  <img src="/2021/05/06/cs224w2/matrix.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/06/cs224w2/rewrite.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>可以基于节点标签进行有监督训练<strong>或者基于图结构信息进行无监督学习</strong>。无监督学习中关于节点相似性的定义可以使用之前介绍过的随机游走、矩阵分解等形式。</p><div align="center">  <img src="/2021/05/06/cs224w2/unsupervised.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>使用GNN模型获取节点表征向量的总结：</p><ul><li>定义邻域聚合函数，确定当前节点邻居节点集合及聚合方式 [key distinctions are in how different approaches aggregate information across the layers]</li><li>定义表征向量损失函数</li><li>在一个集合上完成模型训练</li><li>所有节点共享相同的聚合参数，所以模型参数规模与网络规模是<strong>次线性</strong>的（sublinear），而且模型<strong>具有归纳能力</strong>（inductive），可以扩展到训练集中未出现的节点上，所以可以应用到新的图数据或新发展出的节点上 [注意一下<strong>动态图模型</strong>的研究主题]。</li></ul><h2 id="gnn模型设计"><a class="markdownIt-Anchor" href="#gnn模型设计"></a> GNN模型设计</h2><p>GNN层 = <strong>Message + Aggregation</strong>，如何定义（1）消息和（2）聚合时区分不同GNN模型的关键。另外还有（3）<strong>如何进行层堆叠、（4）如何确定计算图</strong>，以及（5）如何进行模型学习。</p><div align="center">  <img src="/2021/05/06/cs224w2/framework.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="一-单层gnn"><a class="markdownIt-Anchor" href="#一-单层gnn"></a> 一. 单层GNN</h3><p>首先进行“消息计算”，定义一个message function，生成的消息会传递给其它节点。</p><p>之后是聚合，每个节点会聚合来自其邻居的消息，比如使用Sum、Mean、Max等，注意要与节点次序无关（order invariant）。将来自邻居的信息和来自上一层的节点自身的信息结合起来。</p><div align="center">  <img src="/2021/05/06/cs224w2/single.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>最后一般加一个激活函数，比如ReLU，sigmoid等，用来提升（针对节点属性的）表达能力。<div align="center">  <img src="/2021/05/06/cs224w2/gcn_layer.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="1-gcn"><a class="markdownIt-Anchor" href="#1-gcn"></a> 1. GCN</h4><p>节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>在第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span>层的表征是其邻居节点表征的均值。按照message+aggregation的形式可以写成下面的样子：</p><div align="center">  <img src="/2021/05/06/cs224w2/GCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2graphsage"><a class="markdownIt-Anchor" href="#2graphsage"></a> 2.GraphSAGE</h4><p>扩展了GCN的聚合函数形式（比如Mean、Pooling、LSTM（需要特殊操作消除次序信息的影响）等），而且使用<strong>CONCAT</strong>的方式把自身消息和邻居消息结合起来。</p><div align="center">  <img src="/2021/05/06/cs224w2/graphsage.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>另外，GraphSAGE在每一层都<strong>加入了L2归一化</strong>，如果没有的话节点向量取值范围不同，加入L2归一化可以提升效果。</p><h4 id="3-gat"><a class="markdownIt-Anchor" href="#3-gat"></a> 3. GAT</h4><p>对于每个邻居加入一个权重，体现针对当前节点，其不同邻居的不同重要性。</p><p>GCN和GraphSage直接使用了平均即权重为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>N</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1/N(v)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span>，即每个邻居节点都同样重要。</p><p><strong>Attention</strong>是受认知注意力启发而来，注意力系数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>v</mi><mi>u</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{vu}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">u</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>强调输入数据中重要的部分而忽略其他内容。在模型训练过程中学习到哪部分数据是重要的。</p><div align="center">  <img src="/2021/05/06/cs224w2/GAT.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>注意力机制</strong></p><p>在节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u,v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>之间计算注意力系数（attention coefficients <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>），表示来自节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">u</span></span></span></span>的信息对节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>的重要程度。之后对<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>进行归一化（比如softmax）得到最终的权重系数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">a_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。最终这个参数和其它参数如权重矩阵W一起训练，模型是一个端到端的效果。</p><div align="center">  <img src="/2021/05/06/cs224w2/attention.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>加入注意力机制后的模型可能很难训练，很难收敛。通过使用多头注意力机制（multi-head attention）稳定其学习过程。本质思想是设计多个注意力函数，聚合之后一起计算，增加模型鲁棒性。每一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>u</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">a_{uv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>使用不同的函数预测，而且每个函数都随机其初始值。</p><div align="center">  <img src="/2021/05/06/cs224w2/multi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>加入注意力机制的好处：</p><ul><li>区分了不同邻居节点对当前节点的重要程度</li><li>计算高效，参数的计算可以并行。注意力系数可以按不同边并行；聚合操作可以在节点间并行</li><li>存储高效，稀疏矩阵运算符，参数数目固定（与图大小无关）</li><li>局部化，只关注局部网络邻居信息</li><li>可扩展，具有归纳能力，是一种shared edge-wise机制，不依赖全局图结构</li></ul><h3 id="二-gnn堆叠"><a class="markdownIt-Anchor" href="#二-gnn堆叠"></a> 二. GNN堆叠</h3><h4 id="1-经典方式"><a class="markdownIt-Anchor" href="#1-经典方式"></a> 1. 经典方式</h4><p>直接将GNN层sequentially叠加起来。</p><p>GNN的深度表示我们在得到某一节点表征的时候考虑了它的几跳邻居，而GNN模型的表达能力/复杂度是取决于单层GNN层的设计。</p><p>直接堆叠起来会带来<strong>over-smoothing</strong>的问题，无法构造比较深层的GNN模型。到达一定深度，所有节点的表征向量会收敛到一起。</p><p>可以通过<strong>Receptive field</strong>（感受野）解释这个问题，GNN中可定义为为获取某节点表征向量而需要考虑的节点集合。在一个K层GNN模型中，每个节点的感受野就是它的K跳（以内的）邻居。</p><p>堆叠多层GNN层 ——&gt; 各节点的感受野重合程度过大 ——&gt; 得到的节点表征向量过于近似  ——&gt; 造成过平滑问题</p><p>最直接的解决办法是在构造模型是注意堆叠的GNN层数。首先分析解决任务必要的感受野（比如<strong>首先计算一下图数据的半径</strong>），GNN的层数可以比这个感受野稍大一些。</p><p><strong>如何让浅层GNN模型更具表达力？</strong></p><p><strong>解决方法1，提升每层GNN的表达能力</strong>；之前介绍的模型中每层里的聚合或变换都只使用了线性层，我们可以改成使用深度学习网络，比如换成3层MLP。[ 这里也可以看出，GNN中的层概念和DNN中有所不同 ]</p><p><strong>解决方法2，添加不进行消息传递的层</strong>；GNN模型中不一定只包含GNN层，在GNN层之前/之后都可以加入MLP层进行预处理或后处理。</p><div align="center">  <img src="/2021/05/06/cs224w2/MLP.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-skip-connection"><a class="markdownIt-Anchor" href="#2-skip-connection"></a> 2. Skip connection</h4><p>如何构建深层GNN模型？可以在GNN层之间加入一些跳过连接（skip connection）。</p><p>研究发现有时候比较前面的GNN层得到的节点表征结果更有利于区分节点，所以可以在最终结果中提升这些层的重要性。skip connection可以构建mixture of models，即上一层和当前层信息的加权组合。由此我们得到了一个深度GNN模型和一些浅层GNN模型（如果有N个跳过连接，会得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>N</mi></msup><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2^N-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.924661em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>个浅层模型）的混合模型。</p><p>跳过的操作可以如下图所示，也可以直接跳到最后一层（如ICML 2018中写到的），有很多种方式。</p><div align="center">  <img src="/2021/05/06/cs224w2/skip.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="三-图征增强-graph-augmentation"><a class="markdownIt-Anchor" href="#三-图征增强-graph-augmentation"></a> 三. 图征增强 Graph Augmentation</h3><p>很多情况下原始的图数据不适合直接作为计算图数据，而是需要一些改进。</p><h4 id="1特征增强"><a class="markdownIt-Anchor" href="#1特征增强"></a> 1.特征增强</h4><p>原始的图数据可能缺少某些特征信息  ——&gt; feature augmentation</p><p><strong>问题1，输入图数据中不含有节点特征</strong></p><p>方法1：直接赋予常量值作为特征</p><p>方法2：给每个节点赋予独特的ID，并且进行独热编码；但这个方法问题在于无法很好地扩展到其它图数据上</p><div align="center">  <img src="/2021/05/06/cs224w2/aug.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>问题2，有些结构对于GNN来说很难学</strong></p><p>比如，如下图所示，节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>处于某个环形结构中，GNN很难学习到这个结构的周长，区分这两个结构。</p><div align="center">  <img src="/2021/05/06/cs224w2/feature.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>方法，可以把这个结构信息编辑到节点特征中去，比如左图中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">v_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的独热编码在第三位（处于周长为3的环形中），右图中的在第四位。</p><p>其它方法还包括使用节点度、聚合系数、PageRank、中心度等之前介绍过的特征。</p><h4 id="2结构增强"><a class="markdownIt-Anchor" href="#2结构增强"></a> 2.结构增强</h4><p>原始图数据结构可能：</p><ul><li>过于稀疏，影响消息传递   ——&gt; 添加虚拟节点或边</li><li>过于密集，消息传递花费过大 ——&gt; 消息传递时进行邻居采样，降低计算成本</li><li>图太大，无法装载如显存 ——&gt; 计算时进行子图采样</li></ul><p><strong>添加虚拟边</strong></p><p>使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mo>+</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">A + A^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>进行计算，而非单独使用邻接矩阵A表示图，这样我们给2-hop的邻居加入了虚拟边。</p><p>比如在author-paper的二部图中，使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">A^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>就构造了一个共同作者网络。</p><p><strong>添加虚拟节点</strong></p><p>给稀疏图中加入一个节点，这个虚拟节点与图中所有的节点都相连。</p><p><strong>节点采样</strong></p><p>可以采样当前节点的邻居节点进行消息聚合，而非考虑其所有邻居的消息。</p><p>如何选取邻居子集呢？是一个可以优化研究的问题</p><h2 id="gnn模型训练"><a class="markdownIt-Anchor" href="#gnn模型训练"></a> GNN模型训练</h2><div align="center">  <img src="/2021/05/06/cs224w2/GNNt.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="一-预测任务"><a class="markdownIt-Anchor" href="#一-预测任务"></a> 一. 预测任务</h3><h4 id="1node-level-prediction"><a class="markdownIt-Anchor" href="#1node-level-prediction"></a> 1.Node-level prediction</h4><p>直接把GNN得到的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>维表征向量映射到为K类标签向量。</p><div align="center">  <img src="/2021/05/06/cs224w2/node.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-edge-level-prediction"><a class="markdownIt-Anchor" href="#2-edge-level-prediction"></a> 2. Edge-level prediction</h4><p>以节点对表征向量为输入映射到K类标签向量。</p><p>处理节点对向量常用方法有两类：1）Concat + Linear（GAT中使用过）；</p><div align="center">  <img src="/2021/05/06/cs224w2/2d.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>2）向量内积，一般会得到一个标量结果，如果想做成K向预测，可以使用类似多头注意力机制的处理方法。</p><div align="center">  <img src="/2021/05/06/cs224w2/kway.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-graph-level-prediction"><a class="markdownIt-Anchor" href="#3-graph-level-prediction"></a> 3. Graph-level prediction</h4><p>需要将节点的表征向量聚合来得到表示整个图的向量。可以使用很多pooling方法，比如Mean、Max、Sum等。</p><p>但有时候简单的全局池化会损失太多信息，尤其当图规模比较大的时候。</p><p>改善这个问题可以使用<strong>分层池化</strong>（hierarchical global pooling）。可以加入聚类方法确定每层中可以聚合的节点集合。这里的两个GNN模型是独立的，每层的模型可以并行训练。</p><div align="center">  <img src="/2021/05/06/cs224w2/diffpool.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h3 id="二-预测和标签"><a class="markdownIt-Anchor" href="#二-预测和标签"></a> 二. 预测和标签</h3><p>图上的监督学习：标签来自外部；图上的无监督学习：标签来自图数据自己，比如链路预测问题。但实际上二者之间的界限比较模糊。</p><p>在无监督学习中，可以使用图数据自身产生的标签信息：</p><ul><li>Node-level，使用节点统计数据，比如聚类系数、PageRank等</li><li>Edge-level，比如在链路预测任务中隐藏两节点间的边</li><li>Graph-level，使用图特征，比如是否两个图是同构的</li></ul><h3 id="三-损失函数与评估"><a class="markdownIt-Anchor" href="#三-损失函数与评估"></a> 三. 损失函数与评估</h3><p>GNN可以进行分类任务，也支持回归任务（标签<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span>具有连续值）</p><p>分类问题中常用交叉熵，重点强调了K向预测时的交叉熵。</p><div align="center">  <img src="/2021/05/06/cs224w2/entropy.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>回归问题一般使用MSE，K向回归计算如下：</p><div align="center">  <img src="/2021/05/06/cs224w2/kreg.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>评估GNN时一般也是使用sklearn中的常规指标。评估回归问题的时候常用RMSE、MAE等。</p><h3 id="四-数据切分"><a class="markdownIt-Anchor" href="#四-数据切分"></a> 四. 数据切分</h3><p>图数据中有的时候无法明确区分出测试集，因为图中的节点是相互连接的，彼此之间不独立。</p><p>方案1：transductive setting，只根据标签区分，训练阶段可以看到所有数据，我们只需要区分不同节点的标签</p><div align="center">  <img src="/2021/05/06/cs224w2/tran.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>方案2：inductive setting, 将图数据中的某些边截断，区分成多个子图</p><div align="center">  <img src="/2021/05/06/cs224w2/ind.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>两个方法的区别主要在于：</p><div align="center">  <img src="/2021/05/06/cs224w2/vs.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在各类任务中注意一下inductive setting的<strong>链路预测任务</strong>，一般说起链路预测的话是transductive setting的任务。</p><p><strong>链路预测任务</strong>中，训练时可以看到message edge而supervision edge不喂入GNN模型。</p><p>inductive setting设置如下：</p><div align="center">  <img src="/2021/05/06/cs224w2/indl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>transductive setting为：</p><div align="center">  <img src="/2021/05/06/cs224w2/tranl.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>【注意，以上是教授提出的认为正确的区分方法，不同论文中的划分方法有所不同】</p><h2 id="gnn表达能力"><a class="markdownIt-Anchor" href="#gnn表达能力"></a> GNN表达能力</h2><p>目前已经有很多GNN模型被提出来，大家的区分点主要是在消息处理和聚合时使用的网络不同。</p><p>例如GCN使用element-wise mean pooling+ Linear + ReLU。GraphSAGE使用MLP + max-pool。</p><div align="center">  <img src="/2021/05/06/cs224w2/model.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GNN模型的表达能力主要体现在它的计算图（或叫做subtree rooted around each node）上，如下图所示，GNN得到的映射要尽可能做到<strong>单射（injective）</strong>。如果每层GNN上的聚合都是单射的，那么这个GNN模型就可以完全区分不同的子树模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/emb.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GCN和GraphSAGE并不是单射的，这部分内容在《图神经网络的局限》博文中有总结和介绍。</p><p>由此比较，SUM pooling具有较强表达/判别能力，其次是Mean pooling，再次是Max pooling。</p><h3 id="一-设计具有强表达能力的gnn模型"><a class="markdownIt-Anchor" href="#一-设计具有强表达能力的gnn模型"></a> 一. 设计具有强表达能力的GNN模型</h3><p>通过设计单射的邻居聚合方法构建具有最佳表达能力的基于消息传递的GNN模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/inject.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>根据“含有一层足够多神经元隐藏层的MLP可以拟合任意函数”的定力，作者使用MLP设计<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>。实验表明，大概用100~500个神经元，可以训练出很好的单射函数。由此作者提出GIN模型。</p><p>[ GIN is the most expressive GNN n the class of message-passing GNNs! ]</p><p>[ The key is to use element-wise sum pooling. ]</p><h3 id="二-使用wl测试解释gin模型"><a class="markdownIt-Anchor" href="#二-使用wl测试解释gin模型"></a> 二. 使用WL测试解释GIN模型</h3><p>WL测试中可以使用color refine算法，如果两个图模型拥有相同颜色集合，则表示它们同构。</p><div align="center">  <img src="/2021/05/06/cs224w2/color.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>GIN模型使用MLP模仿color refine算法中的单射哈希方法.</p><div align="center">  <img src="/2021/05/06/cs224w2/gin.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>两种方法的对比如下，<strong>两模型的表达能力相当</strong>。GIN的优势在于：1）它可以得到低阶表征向量；2）方程的参数训练可以利用到下游任务中的信息。WL已于1992年被证实可以区分大多数实际图模型。</p><div align="center">  <img src="/2021/05/06/cs224w2/com.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div>### 三. 当前挑战<p>目前还有一些GNN无法区分的基础图结构，比如之前提到的不同节点数的环形。</p><p>已有工作在这方面开始努力。 [You et al. AAAI 2021, Li et al. NeurIPS 2020]</p><h2 id="异构图"><a class="markdownIt-Anchor" href="#异构图"></a> 异构图</h2><h3 id="一-关系型gcnrgcn"><a class="markdownIt-Anchor" href="#一-关系型gcnrgcn"></a> 一. 关系型GCN（RGCN）</h3><p>加入不同类型连接下的消息转换机制。</p><div align="center">  <img src="/2021/05/06/cs224w2/rGCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>可扩展性问题</strong>：随着关系类型数量的增多，RGCN的参数量上涨非常快，可能导致过拟合。可以采用以下两种技术减少模型参数量：</p><p><strong>方法一，块对角矩阵</strong></p><p>我们希望消息传递矩阵W是稀疏的，可以使用它的块对角矩阵，但这也造成只有相邻的节点间可以彼此传递信息，所以整个GCN模型可能要更深一些。</p><div align="center">  <img src="/2021/05/06/cs224w2/wGCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>方法二，基准/词典学习</strong></p><p>在不同类型的连接之间共享参数，所有的消息传递矩阵W都表示成不同基础矩阵V的组合。</p><div align="center">  <img src="/2021/05/06/cs224w2/bGCN.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>之后介绍了在RGCN下的节点分类与链路预测（重点介绍）问题。</p><p>基于RGCN的思想，可以很方便地得出RGraphSAGE、RGAT等。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cs224w《图机器学习》2021（一）经典方法</title>
    <link href="/2021/05/05/cs224w/"/>
    <url>/2021/05/05/cs224w/</url>
    
    <content type="html"><![CDATA[<h2 id="学习资料"><a class="markdownIt-Anchor" href="#学习资料"></a> 学习资料</h2><p>相比于2019年的课堂录播，本年度直接使用线上课程形式，更加方便理解学习。</p><div align="center">  <img src="/2021/05/05/cs224w/dagang.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>课程官方网站：<a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224w/</a></p><p>课程视频链接：Youtube（<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn%EF%BC%89%E3%80%81B%E7%AB%99%EF%BC%88https://www.bilibili.com/video/BV1RZ4y1c7Co%EF%BC%89" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn）、B站（https://www.bilibili.com/video/BV1RZ4y1c7Co）</a></p><p>参考书目：《Graph Representation Learning 》by Will Hamilton</p><p>编程工具：<a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html" target="_blank" rel="noopener">Pytorch Geometric（PyG）</a>、DeepSNAP、GraphGym、<a href="http://SNAP.PY" target="_blank" rel="noopener">SNAP.PY</a>、<a href="https://networkx.org/documentation/stable/tutorial.html" target="_blank" rel="noopener">NetworkX</a></p><h3 id="相关论文"><a class="markdownIt-Anchor" href="#相关论文"></a> 相关论文</h3><ul><li>PinSAGE，《Graph Convolutional Neural Networks for Web-Scale Recommender Systems》KDD，2018</li><li>DeepWalk, 《Online Learning of Social Representations》KDD 2014</li><li>node2vec，《node2vec: Scalable Feature Learning for Networks.》KDD 2016</li><li>Graph Embedding Survey，《Graph Embedding Techniques, Applications, and Performance: A Survey》2017</li><li>子图表征，引入虚拟节点，《Gated Graph Sequence Neural Networks》</li><li>匿名游走，《Anonymous Walk Embeddings》ICML 2018</li><li>矩阵分解与节点表征，《Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec》WSDM 2018</li></ul><h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2><p>为什么使用图模型？</p><p>不仅考虑数据本身，还要考虑实体间的复杂关系。How do we take advantage of relational structure for better prediction?</p><p>多种图模型样例：计算机网络、疾病传播、食物链网络、交通网络、社交网络、论文引用、神经元连接、知识图谱、代码、化学分子、3D建模等等。</p><p>当前深度学习模型一般应用于序列、图片等简单的数据结构。Graphs are the new frontier of deep learning.</p><p>图模型常见任务：</p><ul><li>节点分类：DeepMind提出AlphaFold解决生物学领域蛋白质折叠问题；蛋白质序列中的氨基酸为节点，氨基酸（残基）之间的接近度为边；</li><li>链路预测：推荐系统（users-items）、多种药物一起的副作用预测</li><li>图/子图分类：交通流量预测、药物发现</li><li>聚类</li><li>生成图：新分子发现</li><li>图进化：物理仿真</li></ul><p>选择节点和连接，构成何种图模型这一步非常重要。</p><p>图模型基础概念：</p><ul><li>有向、无向图  -&gt; 度、节点平均度（2E/N）</li><li>Bipartite Graph（二部图）-&gt; Folded network（映射图）</li><li>邻接矩阵（Adjacency Matrix），现实网络的邻接矩阵通常非常稀疏</li><li>边缘列表（Edge list），在深度学习工程实现时十分常用</li><li>邻接表（Adjacency list）</li><li>其它可用属性：边权重、排名、类型、标签以及其它与场景契合的属性等</li><li>自环（self-loops）</li><li>多图（multigraph）：一对节点间有多个边</li><li>连通性、强连接（有向图中每对节点可以相互访问，SCCs，Strongly connected components，强连接部分）、弱连接</li></ul><h2 id="经典图机器学习方法"><a class="markdownIt-Anchor" href="#经典图机器学习方法"></a> 经典图机器学习方法</h2><p>经典机器学习方法重点在于<strong>提取有效特征</strong>（hand-designed features）。</p><h3 id="一-特征提取"><a class="markdownIt-Anchor" href="#一-特征提取"></a> 一. 特征提取</h3><h4 id="1-node-features"><a class="markdownIt-Anchor" href="#1-node-features"></a> 1. Node features</h4><p><strong>节点度（node degree）</strong>[importance, structure]，相同度数的节点无法区分</p><p><strong>节点中心度（node centrality）</strong>[importance]，考虑了图中不同节点的重要程度，包括engienvector centrality，betweenness centrality, closeness centrality等。<strong>engienvector centrality</strong>，某节点的重要程度是其邻居节点重要程度的归一化和（递归计算）；<strong>betweenness centrality</strong>，存在于其它节点对间最短路径上的节点更重要；<strong>closeness centrality</strong>，与其它各节点间最短路径长度和越短的节点越重要。</p><div align="center">  <img src="/2021/05/05/cs224w/ec.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/bc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/cc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>聚类系数（clustering coefficient）</strong>[structure]，节点附近的局部结构，衡量某节点的邻居节点间的联通程度如何。基本上是在计算自网络（ego-network）中的<strong>三角形</strong>个数。</p><div align="center">  <img src="/2021/05/05/cs224w/coefficient.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>GDV(graphlet degree vector)</strong> [structure]，将上述三角形概念扩大，Rooted connected non-isomorphic subgraphs。以下概念中节点的位置也很重要。GDV即该节点在某种graphlet出现的次数。</p><div align="center">  <img src="/2021/05/05/cs224w/graphlets.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/gdv.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="2-link-prediction-features"><a class="markdownIt-Anchor" href="#2-link-prediction-features"></a> 2. Link Prediction features</h4><p>链路预测任务，比如随机丢失了当前图模型中的某些连接信息，或需要预测下一时间窗口中的节点连接信息。</p><p><strong>Distance-based features</strong>，如两节点之间的最短距离</p><p><strong>Local neighborhood overlap</strong>，两节点间的共有邻居数，Jaccard系数、Adamic-Adar index等</p><p><strong>Global Neighborhood Overlap</strong>，katz index计算一对节点间的全部路径数目（使用邻接矩阵A计算）；邻接矩阵的N次幂表示了每对节点间长度为N的路径的数量。</p><div align="center">  <img src="/2021/05/05/cs224w/katz.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="3-graph-level-features"><a class="markdownIt-Anchor" href="#3-graph-level-features"></a> 3. Graph-level features</h4><p>核方法（kernel），核心是设计一个kernel而不是使用特征向量。两个图模型之间的Kernel衡量的是图数据之间的相似度。内核矩阵是测量每对数据点之间的相似度，它一定是正定的，即只有正数特征值。</p><p>当前存在很多Graph kernels，比如graphlet kernel，Weisfeiler-Lehman Kernel, Random-walk kernel，shortest-path graph kernel等等。</p><p>Graph kernel背后的思想是给图模型做词袋（Bag-of-words），将图中的节点视为词，比如Bag of node degrees。而graphlet kernel，Weisfeiler-Lehman Kernel是这类方法的延伸，即使用了比节点度更复杂一些的表示方法。</p><p>Graphlet kernel使用不同graphlet数目，这边的graphlet定义与node-level features那一节中的有所不同。首先这类graphlet中允许存在孤立节点，而且它们不是rooted。注意，计算两个图模型相似性的时候可以把向量f归一化一下。这类方法的问题在于graphlets计数非常困难，类似的subgraph isomorphism test问题是NP难的。</p><div align="center">  <img src="/2021/05/05/cs224w/graphlet.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>Weisefeiler-Lehman Kernel</strong>旨在缓解这个问题，使用<strong>color refinement</strong>。其中确定属性使用到了哈希函数。这个方法计算效率较高，是线性于两个图的边数目的。Weisefeiler-Lehman Kernel是衡量图相似度的一种非常有效的方式（很难被击败），也是与GNN是息息相关的。</p><h2 id="节点表征"><a class="markdownIt-Anchor" href="#节点表征"></a> 节点表征</h2><p>重点包括三部分内容：1）编解码器框架，encoder是一个简单的表征向量查询，decoder是基于表征向量计算与定义的节点相似度匹配程度；2）节点相似度衡量方法：基于随机游走；3）图级别表征方法，可以直接将节点表征向量聚合或者利用匿名游走。</p><h3 id="一-encoder-decoder-框架"><a class="markdownIt-Anchor" href="#一-encoder-decoder-框架"></a> 一. Encoder + Decoder 框架</h3><p>相较于传统的图学习方法，图表征学习省去了特征工程的步骤，直接自动学习图特征，之后应用于不同下游任务。</p><p>Efficient task-independent feature learning for machine learning with graphs. Encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the graph.</p><p>重点在于定义：1）什么是图上的节点相似性；2）节点到向量的映射函数。</p><p>一个最简单的encoder（shallow encoder）就是对于embeddings的查询。我们可以去直接优化每个节点的表征向量。但如果是大图的话，这个方法会很慢，因为嵌入矩阵Z会有很多列，要针对每一个进行表征向量估计。</p><div align="center">  <img src="/2021/05/05/cs224w/embedding.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如何衡量图中节点相似度？这也是不同算法工作中比较大的差异点。大多流行方法会使用<strong>随机游走</strong>。</p><p>节点表征方法的特点：</p><ul><li>这类方法属于无监督或自监督学习，并没有利用到标签信息</li><li>没有利用节点属性，方法的目标是表征向量体现出图结构信息</li><li>表征结果是独立于下游任务的</li></ul><h3 id="二-基于random-walk的节点表征方法"><a class="markdownIt-Anchor" href="#二-基于random-walk的节点表征方法"></a> 二. 基于Random Walk的节点表征方法</h3><p>关于图中节点间的相似度衡量定义为两节点同时出现在图中统一随机游走记录中的概率。</p><p>使用随机游走的优势在于：1）可以同时考虑局部和高阶邻居信息；2）只需要考虑在随机游走中同时出现的节点对，而不是考虑图上的全部节点对。</p><div align="center">  <img src="/2021/05/05/cs224w/rw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>一般会在某些游走策略R下使用short fixed-length随机游走，收集某节点的邻居节点集合（此集合为multiset，即允许重复，因为某些节点可以被访问多次），之后调整嵌入向量z（参数）去优化最大似然概率。下图使用了softmax方法。</p><div align="center">  <img src="/2021/05/05/cs224w/rwe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>但直接进行这样的计算代价很大，复杂度是图中节点数的平方。我们同样使用<strong>负采样（negative sampling）<strong>进行优化，只在负样本集合上计算而非在全部数据上计算。那么</strong>如何选取负样本</strong>呢？采样概率依照每个不同节点的度设定，共采样K个。K值越高得到的模型越鲁棒，但同时对负样本的偏向也越高，实际中<strong>一般选取K=5-20</strong>。</p><p>最后使用梯度下降方法优化即可。</p><p>最后一个问题是<strong>如何设定随机游走策略</strong>？最简单的方法是直接进行fixed-length, unbiased random walks（DeepWalk使用）。而node2vec认为更灵活地定义邻居节点可以获取包含信息量更大的节点表征，所以提出了biased 2nd order随机游走来生成节点邻居集合，这个方法可以综合权衡局部（BFS）和全局（DFS）信息。</p><div align="center">  <img src="/2021/05/05/cs224w/node2vec.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>因此node2vec提出return参数p（返回到之前的节点）和in-out参数q（BFS和DFS的比例）。关键在于<strong>记录了上一节点信息</strong>，对于处于某个节点的随机游走，下一节点有三种选择：1）退回上一节点；2）去和上一节点距离相同的节点；3）去距离上一节点更远的节点。</p><div align="center">  <img src="/2021/05/05/cs224w/pq.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Node2vec算法总结如下，算法为<strong>线性复杂度</strong>，三个步骤可以并行处理。</p><ul><li>计算随机游走概率</li><li>对每个节点u模拟r次长度为l的随机游走</li><li>使用随机梯度下降法优化目标函数</li></ul><p>还有很多<strong>其它经典算法</strong>应用了不同的随机游走策略、优化策略和一些数据预处理技巧。</p><div align="center">  <img src="/2021/05/05/cs224w/others.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>截至目前，我们学习了三类图中<strong>节点相似度度量</strong>的方法：</p><ul><li>Naive：有连接的节点相似；</li><li>第二节中的Neighborhood overlap，即两节点间共有邻居情况</li><li>基于随机游走的节点向量表征</li></ul><p><strong>Must choose definition of node similarity that matches your application!</strong></p><h3 id="三-图表征"><a class="markdownIt-Anchor" href="#三-图表征"></a> 三. 图表征</h3><p>图或子图级别（即多个节点）的向量表征。</p><p>方法一：最简单是沿用节点向量表征方法，之后直接加和或平均，作为整个图的表征向量，虽然简单但是实际效果还不错。也可以使用层次聚类的方法逐步计算。</p><p>方法二：引入一个虚拟节点“virtual node”代表整个图/子图。</p><div align="center">  <img src="/2021/05/05/cs224w/subgraph.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p><strong>方法三</strong>：使用<strong>匿名游走（anonymous walks）</strong>，名字由来在于最终结果与具体图中节点信息无关。随着随机游走长度的增长，匿名游走数量呈指数型增长。可以按不同长度L匿名游走下不同游走类型的数量/概率作为表征向量。</p><div align="center">  <img src="/2021/05/05/cs224w/aw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>如何确定我们需要的匿名游走的数量？</p><div align="center">  <img src="/2021/05/05/cs224w/count_aw.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>以同时出现在时间窗口T中的匿名游走为样本进行训练。这也是和DeepWalk之间的一大差异，即没有用节点集合左右邻居域。</p><div align="center">  <img src="/2021/05/05/cs224w/awe.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2021/05/05/cs224w/awg.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h2 id="pagerank一些链路分析方法"><a class="markdownIt-Anchor" href="#pagerank一些链路分析方法"></a> PageRank（一些链路分析方法）</h2><p>从矩阵角度进行图数据分析，由此可以进行：1）基于随机游走衡量节点重要程度（PageRank）；2）通过矩阵分解（Matrix factorization，MF）获取节点向量表征；3）将其它节点向量表征视为MF。</p><p><strong>Random walk，matrix factorization and node embeddings are closely related!</strong></p><div align="center">  <img src="/2021/05/05/cs224w/summary.png" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>之前的假设是把互联网中的（静态）网页视为节点，超链接视为边，去<strong>衡量网页的重要性</strong>。[ 虽然目前随着互联网的发展有了很多<strong>动态页面</strong>以及一些<strong>无法访问的生成页面</strong>。之前的网站链接多为navigational，而如今的更多是transactional。]</p><h4 id="1-pagerank"><a class="markdownIt-Anchor" href="#1-pagerank"></a> 1. PageRank</h4><p>将网页链接视为投票，使用in-links权衡，但每个连接的重要程度又不同，从而形成一个递归问题。</p><div align="center">  <img src="/2021/05/05/cs224w/page.jpg" srcset="/img/loading.gif" width="20%" height="20%" alt="oauth"></div><p>计算使用<strong>列随机矩阵M</strong>，最终得到<strong>秩向量r</strong>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi><mo>=</mo><mi>M</mi><mo separator="true">⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">r = M · r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>。可以将r视作<strong>随机游走</strong>收敛到的平稳分布，或者<strong>视为M的特征值为1对应的特征向量</strong>，即主特征向量。它是随机游走方程、基于流的方程式 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo separator="true">⋅</mo><mi>r</mi><mo>=</mo><mi>M</mi><mo separator="true">⋅</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">1·r = M · r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span>和线性代数的特征向量、特征值的完美融合。</p><p>[ 这里与前文中介绍的节点中心度中的<strong>eignvector centrality</strong>（针对无向图）和<strong>Katz centrality</strong>有一些梦幻联动。]</p><div align="center">  <img src="/2021/05/05/cs224w/pc.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>使用幂迭代（power iteration）方法求解r，一般来讲迭代50次会大致收敛。[ 下图中左右两侧的迭代公式意义相同 ]</p><div align="center">  <img src="/2021/05/05/cs224w/pi.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>还有<strong>两个问题</strong>需要关注：</p><p>第一，死胡同问题（dead ends）[数学问题]，有的页面时没有out-link的。解决方法是，给没有out-link的页面所在的M列<strong>赋均值</strong>。</p><p>第二，蜘蛛陷阱（spider traps）[非数学问题]，有些页面相互连接，最终吸收了所有的“重要性”。解决方法是，引入参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>，表示继续沿着当前link游走的概率，而有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>的概率**随机传送（teleport）**到任意页面，<strong>一般取值在0.8-0.9之间</strong>。</p><p>所以，Google的做法如下，或者写成矩阵形式<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mo>=</mo><mi>β</mi><mi>M</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mo stretchy="false">⌈</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><msub><mo stretchy="false">⌉</mo><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msub><mo separator="true">,</mo><mi>r</mi><mo>=</mo><mi>G</mi><mo>∗</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">G = \beta M + (1-\beta) \lceil \dfrac{1}{N} \rceil_{N \times N},  r = G * r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">G</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mopen">⌈</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose"><span class="mclose">⌉</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">G</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span></span></span></p><div align="center">  <img src="/2021/05/05/cs224w/google.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>我们虽然从随机游走的角度来理解算法，但运行过程中并不实际进行游走，而是假定它游走了无限长时间。</p><h4 id="2-personalized-pagerankppr-random-walk-with-restarts"><a class="markdownIt-Anchor" href="#2-personalized-pagerankppr-random-walk-with-restarts"></a> 2. Personalized PageRank（PPR）&amp; Random walk with restarts</h4><p>PPR：在传送的时候不像PageRank那样概率传送到图中每个节点而是只取<strong>一个节点子集S</strong>。这个子集由之前的随机游走记录而得，每个节点的概率由访问次数决定。</p><div align="center">  <img src="/2021/05/05/cs224w/ppr.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>Random walk with restarts：将传送节点子集S缩小至单个节点，即总是传送回起始节点。</p><p>优势在于这类方法考虑了：1）一对节点间多种连接；2）多条路径；3）连接是否有向；4）路径中节点的度。</p><h4 id="3-矩阵分解与节点表征的关系"><a class="markdownIt-Anchor" href="#3-矩阵分解与节点表征的关系"></a> 3. 矩阵分解与节点表征的关系</h4><p>以“存在边连接”定义节点相似度的内积形式的解码器与邻接矩阵A的矩阵分解等价。</p><p>DeepWalk、node2vec等方法具有更为复杂的节点相似度定义（基于随机游走），它们等同于形式更为复杂的矩阵的矩阵分解。</p><p>下图为DeepWalk对应的矩阵形式：</p><div align="center">  <img src="/2021/05/05/cs224w/DeepWalk.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><h4 id="4方法局限性"><a class="markdownIt-Anchor" href="#4方法局限性"></a> 4.方法局限性</h4><p>这类基于矩阵分解/随机游走的节点表征方法，如DeepWalk、node2vec等（PageRank也可以视为一维嵌入）有以下几点局限：</p><ul><li><p>无法得到新加入（未在训练集中出现过）的节点的表征向量；</p></li><li><p>无法捕获结构相似性（structurally similar），比如下图中节点1和节点11会有很不同的表征。如果使用匿名随机游走也许有提升；</p></li><li><p>无法利用节点、边或整个图的特征信息</p></li></ul><div align="center">  <img src="/2021/05/05/cs224w/limit.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>针对以上这些缺陷的解决方法是：Deep Representation Learning和Graph Neural Networks。</p><h2 id="课程作业借鉴"><a class="markdownIt-Anchor" href="#课程作业借鉴"></a> 课程作业借鉴</h2><h4 id="1可视化函数"><a class="markdownIt-Anchor" href="#1可视化函数"></a> 1.可视化函数</h4><pre><code class="hljs python"><span class="hljs-comment"># Helper function for visualization.</span>%matplotlib inline<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># Visualization function for NX graph or PyTorch tensor</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">visualize</span><span class="hljs-params">(h, color, epoch=None, loss=None)</span>:</span>    plt.figure(figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">7</span>))    plt.xticks([])    plt.yticks([])    <span class="hljs-keyword">if</span> torch.is_tensor(h):        h = h.detach().cpu().numpy()        plt.scatter(h[:, <span class="hljs-number">0</span>], h[:, <span class="hljs-number">1</span>], s=<span class="hljs-number">140</span>, c=color, cmap=<span class="hljs-string">"Set2"</span>)        <span class="hljs-keyword">if</span> epoch <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> loss <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            plt.xlabel(<span class="hljs-string">f'Epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.4</span>f&#125;</span>'</span>, fontsize=<span class="hljs-number">16</span>)    <span class="hljs-keyword">else</span>:        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=<span class="hljs-number">42</span>), with_labels=<span class="hljs-literal">False</span>,                         node_color=color, cmap=<span class="hljs-string">"Set2"</span>)    plt.show()</code></pre>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络的局限</title>
    <link href="/2020/12/22/limit-graph/"/>
    <url>/2020/12/22/limit-graph/</url>
    
    <content type="html"><![CDATA[<p>后面计划进行有关图模型攻击方面的研究，学习斯坦福<a href="http://web.stanford.edu/class/cs224w/" target="_blank" rel="noopener">CS224W《图机器学习》</a>a&gt;和大佬Stephan Günnemann教授<a href="https://www.in.tum.de/daml/teaching/mlgs/ " target="_blank" rel="noopener">MLGS课程</a>中“Limitations of GNN”部分，记录如下。</p><p>关键点：</p><ul><li>图同构判断问题：单射，max/mean/sum pooling，WL Test</li><li>对抗攻击：Nettack，离散数据（无法直接梯度下降优化）、双层优化问题、如何对抗（certification）</li><li>Robutness and certification部分</li></ul><h2 id="mlgs"><a class="markdownIt-Anchor" href="#mlgs"></a> MLGS</h2><div align="center">  <img src="/2020/12/22/limit-graph/summary.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="一-表达能力"><a class="markdownIt-Anchor" href="#一-表达能力"></a> 一. 表达能力</h3><h4 id="1-图同构问题"><a class="markdownIt-Anchor" href="#1-图同构问题"></a> 1. 图同构问题</h4><p>如何判断两个图是否在结构上相同？此问题最优解最差时间复杂度呈指数形式。</p><p><strong>WL test</strong>（Weisfeiler-Lehman Test），只能得出“两个图同构或可能同构”的结论。</p><div align="center">  <img src="/2020/12/22/limit-graph/wl.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>在这个问题上GNN无法做到比WL test更好，尤其是它使用了非单射的聚合操作的时候更是无法区分图同构问题。</p><div align="center">  <img src="/2020/12/22/limit-graph/increase.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2-过平滑问题"><a class="markdownIt-Anchor" href="#2-过平滑问题"></a> 2. 过平滑问题</h4><p>随着层数增加GNN的预测结果过于平滑。无穷多层的GNN会导致所有的</p><p>节点得到同样的表征向量，这个向量表达了整个图的结构信息（和PageRank类似）而无法区分局部信息。</p><div align="center">  <img src="/2020/12/22/limit-graph/limit.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>关注一下<strong>PageRank</strong>。在PageRank里我们使用teleport vector进行信息局部化（关注邻居），同理可以应用到GCN场景中，相关工作为<strong>PPNP</strong>（Personalized Propagation of Neural Predictions，2018，建议阅读原文）。将转换与传播操作分开，并加入personalized teleportation，最终将迭代公式修改为：</p><div align="center">  <img src="/2020/12/22/limit-graph/shizi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>PPNP在防止过平滑、计算效率、扩展性等方面有如下优势：</p><div align="center">  <img src="/2020/12/22/limit-graph/ppnp.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-鲁棒性"><a class="markdownIt-Anchor" href="#二-鲁棒性"></a> 二. 鲁棒性</h3><p>有关图数据的对抗可以发生在<strong>节点属性</strong>和图<strong>结构信息</strong>两方面（后者在现实世界中更普遍），进行针对某些节点的<strong>有目标攻击</strong>或进行针对整个图的<strong>全局攻击</strong>。</p><p>图对抗攻击的<strong>挑战</strong>：</p><ul><li>针对离散变量的优化问题；通过非凸的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">L_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>范数计算扰动；</li><li>样本节点间相互关联，不可以单独计算；</li><li>如何定义“难以察觉”的扰动？</li><li>现实中多抽象为投毒攻击（影响训练数据集），抽象为一个双层优化问题。</li></ul><div align="center">  <img src="/2020/12/22/limit-graph/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div>最早的攻击为Nettack‘2018，目标是影响single node's prediction。关键操作在于首先将分类器线性化（为简化模型去掉了激活函数ReLU），之后通过贪心算法迭代找到最优扰动。<p>如何提升鲁棒性：</p><p>1）启发式防御方法：adjacency low-rank approximaition via truncated Singular Value Decomposition （Entezari 2020）; filtering of malicious edges via attribute similarity（Wu 2019）等，但这些方法在CNN领域已被证明无法应对最差情况的扰动。</p><p>2）鲁棒的训练方法，如 via Projected Gradient Descent（Xu et al，2019，但目前这种通过生成其它图样本的方法效果不是特别好）或者propose with a certification technique（low up bound，这个方面教授发表了很多论文）</p><ul><li>《Certifiable Robustness and Robust Training for Graph Convolutional Networks》</li><li>《[Certifiable robustness of graph convolutional networks under structure perturbations](javascript:void(0))》</li><li>《Certifiable Robustness to Graph Perturbations》</li></ul><p>3）随机平滑（randomized smoothing），如何在离散的图结构信息上加入高斯噪声？将邻接矩阵上的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>个边视为伯努利随机变量，但由于实际中的网络大多比较稀疏，很难找到一个合适的概率参数p。所以我们需要进行sparsity-aware random sampling。（这部分需要更详细得看一下）《Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more》‘ICML 2020</p><p>这方面的问题依然大有可为！（GNN robustness/certification is a highly active research area）</p><h3 id="三-扩展性"><a class="markdownIt-Anchor" href="#三-扩展性"></a> 三. 扩展性</h3><p>消息传递机制下需要同时处理整个网络，节点数据非独立同分布，动态增删节点/边会造成较大影响。</p><h2 id="cs-224w"><a class="markdownIt-Anchor" href="#cs-224w"></a> CS 224W</h2><h3 id="一-capture-graph-structure"><a class="markdownIt-Anchor" href="#一-capture-graph-structure"></a> 一. Capture graph structure</h3><p>Graph Isomorphism（图同构问题），邻居节点聚合函数（mean，max）并不单射。提出GIN（Graph Isomorphism Network），使用sum pooling。GIN可以更好地把握图结构信息，对于图分类问题表现更优秀，尤其是当网络中没有节点属性信息时。</p><p>GIN的思想与WL测试法近似。WL可以解决实际中的绝大多数图同构判断问题，但有一些例外，比如下面的例子：</p><div align="center">  <img src="/2020/12/22/limit-graph/except.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h3 id="二-vulnerability-of-gnns-to-noise-in-graph-data"><a class="markdownIt-Anchor" href="#二-vulnerability-of-gnns-to-noise-in-graph-data"></a> 二. Vulnerability of GNNs to noise in graph data</h3><p>以图上半监督节点分类问题为例，重点介绍了KDD18上Stephan Günnemann的工作，第一次提出该问题的数学模型并解答。解如下优化问题有两个难点：1）离散数据难以使用梯度下降；2）该问题为双层优化问题，如果使用迭代求解，每一步重新训练GNN非常耗时。作者为了保证高效，使用了很多启发式近似方法，比如贪心地一步步进行图修改，删除GCN中的ReLU激活函数进行简化等。（更多细节可以直接看论文，Adversarial Attacks on Neural Networks for Graph Data，PPT也做的很赞）。</p><div align="center">  <img src="/2020/12/22/limit-graph/attack.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><div align="center">  <img src="/2020/12/22/limit-graph/math.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><h3 id="三-challenges-and-future"><a class="markdownIt-Anchor" href="#三-challenges-and-future"></a> 三. Challenges and Future</h3><p>带标签数据集不容易获得（这是整个ML领域的问题），数据集不足又比较容易出现过拟合问题。为解决这个问题，提出Pre-training GNNs [Hu+ 2019]，先在某些相关数据集上训练之后，遇到真实任务再进行finetune。</p><p>如何防御上述类型对抗攻击？</p><p>攻击过程中如何在离散数据上找到最优解？</p><p>如何在准确性和鲁棒性之间找到最佳平衡？</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图模型</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【小故事】无法消散</title>
    <link href="/2020/07/07/story-1/"/>
    <url>/2020/07/07/story-1/</url>
    
    <content type="html"><![CDATA[<p>下午四点半，9号楼的小洛死了。</p><p>一个女孩就这么从楼上摔下去，“肝脑涂地”，围观的人们在惋惜中带着阵阵恶心。</p><p>警队来了，120无聊地待在一边。</p><p>小洛家在十一楼，电梯坏了，郝队爬得气喘吁吁，“大夏天的，整这出？”</p><p>小洛和父母同住，郝队进门，家里一尘不染，物件规整，门口立着扫帚，卫生间里的拖布还没晾干。阳台的花鸟，墙上的字画，尤其沙发、床头随处可见的书籍，显示出这家人对文化生活的习惯，不是那种张扬在外的追捧。</p><p>“没有打斗的痕迹，看来是失足坠楼。”</p><p>小洛摔下去的地方正对着家里小阳台的窗户，窗页大敞着，郝队看了看，有齐腰高，如果不是故意爬上去，应该没可能发生意外。奇怪的是，周围窗台也一尘不染，连在附近活动过的痕迹都没有。如果是失足坠落，这窗户就显得过分冷静了。</p><p>“真会给我出难题。”</p><p>在屋内转了一圈，没获得什么实质性线索，郝队转过头来询问家人亲友。据了解，小洛是个成绩不错的学生，正在读博士，平时安安静静，不太可能涉及校园贷、勒索威胁等杂七杂八的事情。小洛有近期和同学出游的计划，前一天洛爸还听见她在电话上兴奋地讨论行程安排。听同学说，小洛最近有一篇论文在写，她还报名了半个月之后的一场线上比赛。哦，还有在小区附近的美容院里预约了两天后的祛痘清洁服务。</p><p>“你家孩子近期遇到什么事情了么？”，  “没有啊，疫情在家，每天平平淡淡的，哪有什么大事。” 洛妈已经哭晕了，都是洛爸在撑着回答。</p><p>“孩子人际交往怎么样？”</p><p>“性格有些内向，打小害羞，爱自己一个人玩，但长大就好很多，上学也结识了几个挺不错的朋友，这几天晚上还经常一起打游戏聊天呢。”</p><p>“那她平时生活状态怎么样？我看她是博士生，是不是课业压力挺大的？”</p><p>“刚读博的时候确实是，她老觉得毕业没希望，打电话回家也都挺沮丧的，后来发了两篇论文，就好很多了。我孩子不可能自杀，她最近也作息规律，偶尔锻炼，跟我俩聊天，都很好的，警官您可得仔细给查查啊，我们都配合，都配合。”</p><p>“不像啊”，郝队心想，“这感觉过得挺好。” 忽然，郝队看到垃圾桶里有个弯了的勺子，是平时做饭用的不锈钢厨具。要不是强外力，勺子不可能拧成这样。刚刚在书房里发现的塑料碎片，应该就是这勺子把手上掉下来的。郝队一阵激动，“这案子应该另有隐情。”</p><p>其实，这就是一场简单的自杀。</p><p>小洛是个理解力远超表达力几个维度的人，这样的人是孤独的，她在期盼与失落中交替，觉得没劲，就走了。</p><p>临走的几个小时前，又一次失落后，小洛很愤怒，刷碗时猛地把勺子砸向地板。看着弯折的勺子和四散的勺柄，她觉得挺好笑的。小洛有很多年，或者甚至说是从小，就不会生气，她好像总能站过去理解对面的逻辑，然后承认现实。但承认现实，安慰不了自己。因为无法被理解，所以时常失落，又因为能理解，所以她的失落没有焦点。</p><p>不如算了吧，通过模仿别人而产生的烟火气，总也不能落地，搞得大家都麻烦。后来她想到，家里人都爱干净，就一一收拾好，还彻底打扫了卫生。以往这种时候，洛妈回来都会表扬几句，小洛听着，觉得“你开心就好”。</p><p>窗台上，小洛一边擦去最后的痕迹，一边记起初中时，在思想政治书上背过，“热爱生活，珍惜生命，回报父母，贡献社会”。真没办法，对不起当时考出的98分。下坠时，她记起，之前和同学讨论起生命的意义、自杀等问题，同学说，“死不死得无所谓，走之前可以把角膜啥的这些器官，捐献给那些想活着的人”。</p><p>我走了，就任你们处置了，最好可以尽快消散掉。不过，好像没大可能，而且又要为当代青年抹黑了，真是不好意思。</p><p>“现在这些年轻人啊，生活条件那么好了，蜜罐里泡大，心理素质就是差。”</p>]]></content>
    
    
    <categories>
      
      <category>瞎写</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人文社科</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>这个博客的搭建过程</title>
    <link href="/2020/06/01/build-blog/"/>
    <url>/2020/06/01/build-blog/</url>
    
    <content type="html"><![CDATA[<p>本文重点介绍基于Hexo+Github搭建个人网站流程，最初基本源自<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>（如何使用Github从零开始搭建一个博客），作者把步骤已经介绍得非常详细完善了。我只是补充踩到的一些坑….</p><p>后来，我背弃了极简审美，开始使用<strong>Fluid</strong>主题。</p><h3 id="一-基础搭建"><a class="markdownIt-Anchor" href="#一-基础搭建"></a> 一. 基础搭建</h3><p>搭建过程使用的两个最基本、最重要的东西是Hexo和Github，其中前者是一个轻量级博客框架，支持将Markdown编写的文章直接编译为静态网页文件并发布，省去了数据库问题。Github则用来解决域名问题，其Github Pages允许每个用户创建一个名为{username}.github.io的仓库，发布博客网页。当然也可以自己申请域名，使用CNAME跳转。</p><h4 id="1-github创建仓库"><a class="markdownIt-Anchor" href="#1-github创建仓库"></a> 1. Github创建仓库</h4><p>在Github上创建一个名为{username}.github.io的仓库，注意必须是github.io结尾。比如我的github账户为“DeepDeer”，创建仓库为“<a href="http://deepdeer.github.io" target="_blank" rel="noopener">deepdeer.github.io</a>”。另外，申请对应仓库时不要弄成private的，否则开放博客Github要收费哈。</p><h4 id="2-安装环境"><a class="markdownIt-Anchor" href="#2-安装环境"></a> 2. 安装环境</h4><p>首先在自己电脑上安装Node.js，确保环境变量配置好，可以使用npm命令；</p><p>其次使用npm命令安装Hexo，安装后确保可以使用<code>hexo</code>命令。</p><pre><code class="hljs bash">npm install -g hexo-cli</code></pre><h4 id="3-初始化项目"><a class="markdownIt-Anchor" href="#3-初始化项目"></a> 3. 初始化项目</h4><p>选定存储博客文件的位置，在此文件夹中使用如下命令创建项目及对应文件夹：</p><pre><code class="hljs bash">hexo init &#123;name&#125;</code></pre><p>命令下产生的文件夹包括themes、source等文件夹，调用如下命令，则在public文件夹中生成js、css、font等内容。</p><pre><code class="hljs verilog">hexo <span class="hljs-keyword">generate</span></code></pre><p>使用server命令在本地运行博客，可以看到类似结果：</p><pre><code class="hljs routeros">hexo<span class="hljs-built_in"> server </span> #或简写为 hexo s</code></pre><div align="center">  <img src="/2020/06/01/build-blog/hello.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/hexo.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h4 id="4-部署至github"><a class="markdownIt-Anchor" href="#4-部署至github"></a> 4. 部署至Github</h4><p>安装一个支持Git的部署插件</p><pre><code class="hljs sql">npm <span class="hljs-keyword">install</span> hexo-deployer-git <span class="hljs-comment">--save</span></code></pre><p>修改Hexo的配置文件_config.yml，找到Deployment部分，修改为如下内容：</p><pre><code class="hljs bash"><span class="hljs-comment"># Deployment</span><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span>deploy:  <span class="hljs-built_in">type</span>: git  repo: git@github.com:DeepDeer/deepdeer.github.io <span class="hljs-comment">#你自己的Github仓库地址</span>  branch: master</code></pre><p>使用deploy命令部署后，可通过域名deepdeer.github.io访问，Github上传代码如下：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">hexo deploy</span></code></pre><div align="center">  <img src="/2020/06/01/build-blog/github.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p><a href="http://xn--deploy-hp7ik1vdf32vsxrqigxw5bw73eklg.sh" target="_blank" rel="noopener">可编写如下内容脚本deploy.sh</a>，此后每当有内容更新时，<code>.deploy.sh</code>运行脚本即可。</p><pre><code class="hljs bash">hexo cleanhexo generatehexo deploy</code></pre><h3 id="二-加入主题"><a class="markdownIt-Anchor" href="#二-加入主题"></a> 二.  加入主题</h3><p>目前我加入的是Fluid主题（因为看上了颜值），之前用过一段时间Next主题，也很推荐。</p><h4 id="1-next主题"><a class="markdownIt-Anchor" href="#1-next主题"></a> 1. Next主题</h4><p>有关Next主题的配置及各种插件，在<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog" target="_blank" rel="noopener">这篇文章</a>中介绍地非常详细，这里只补充有关1）添加Gitalk插件和2）修改字体部分。</p><h5 id="1-gitalk插件"><a class="markdownIt-Anchor" href="#1-gitalk插件"></a> 1&gt; Gitalk插件</h5><p>申请Gitalk就在Github个人账户的settings——&gt; Developer settings ——&gt; OAuth Apps，点击 New OAuth App，出现申请界面。其中应用名称随便写就行，Hompage URL和Authorization callback URL写博客链接。如果有自己的域名可以更改Authorization callback URL。点击注册，生成Client ID和Client Secret。</p><p>注意，如果自己配置了域名，这个callback URL要改成自定义域名</p><div align="center">  <img src="/2020/06/01/build-blog/oauth.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><p>在配置了_config.yml文件后，第一次进入界面会出现下图效果。如果点击Github登录后跳转到了404界面，那么, 就说明配错了。我当时是在写_config.yml忘了把client_id, client_secret字段带的{ }去掉。这给我一顿google啊…</p><div align="center">  <img src="/2020/06/01/build-blog/begin.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>最后效果就像这个博客里一样，相关评论会显示在对应仓库的issues里，记得在仓库的settings里把features—&gt;issues勾选上（貌似默认就是开启的）</p><div align="center">  <img src="/2020/06/01/build-blog/comment.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/issues.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><h5 id="2-修改字体"><a class="markdownIt-Anchor" href="#2-修改字体"></a> 2&gt; 修改字体</h5><p>Next主题默认的博文正文字体大小有点大了，可以在配置文件里改一下。相关配置在hexo\themes\next\source\css\variables路径下的base.styl文件里的Font Size部分。这里面每个变量控制某一部分的字体大小，我是挨个试出来的font-size-large是正文字体（简单粗暴，真开心…)</p><div align="center">  <img src="/2020/06/01/build-blog/font.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><p>另外，在Hexo配置文件和Next主题配置文件中，都有一些有关网站信息的配置选项，最终使用Next主题搭建出的网站效果如下：</p><div align="center">  <img src="/2020/06/01/build-blog/next.jpg" srcset="/img/loading.gif" width="70%" height="50%" alt="oauth"></div><h4 id="2fluid主题"><a class="markdownIt-Anchor" href="#2fluid主题"></a> 2.Fluid主题</h4><p>其实这个主题有非常好的<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide/" target="_blank" rel="noopener">配置指南</a>，其配置文件_config.yml的注释也很清晰，可以从头摸索。有几点经验包括1）图片插入；2）评论插件。</p><h5 id="1-图片插入"><a class="markdownIt-Anchor" href="#1-图片插入"></a> 1&gt; 图片插入</h5><p>需要注意的一点是，在Fluid主题下有文章背景图等存在于框架中的图片，这些图片一律存放在<code>./themes/fluid/source/img</code>文件夹下。即使是某个文章的缩略图也是这样。</p><div align="center">  <img src="/2020/06/01/build-blog/pic.jpg" srcset="/img/loading.gif" width="30%" height="30%" alt="oauth"></div><p>其他存在于文章中的图片，可用如下形式添加。</p><p>首先，把_config.yml文件里的post_asset_folder选项设置为true。</p><p>其次安装一个插件，据说原有插件有一些bug，下面是修改过的插件，亲测有效，感谢<a href="https://www.jianshu.com/p/3db6a61d3782" target="_blank" rel="noopener">这篇博客</a></p><pre><code class="hljs vim">npm install http<span class="hljs-variable">s:</span>//github.<span class="hljs-keyword">com</span>/<span class="hljs-number">7</span>ym0n/hexo-asset-image --<span class="hljs-keyword">sa</span></code></pre><p>有了这些配置后，再运行hexo new xxx，在/source/_posts/路径下，除了可以生成新文章xxx.md之外，还生成一个同名文件夹。插入图片时放到这个文件夹里即可，在markdown里用如下语句：</p><pre><code class="hljs routeros">&lt;img <span class="hljs-attribute">src</span>=<span class="hljs-string">"xxx/图片名称.png"</span> <span class="hljs-attribute">alt</span>=<span class="hljs-string">"图片标识"</span> <span class="hljs-attribute">style</span>=<span class="hljs-string">"zoom:30%;"</span> /&gt;</code></pre><p>但是，在Fluid主题下，这些图片并没有默认居中，可以采用如下HTML代码控制位置和大小：</p><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">"build-blog/pic.jpg"</span> <span class="hljs-attr">width</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">height</span>=<span class="hljs-string">"30%"</span> <span class="hljs-attr">alt</span>=<span class="hljs-string">"oauth"</span>  /&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></code></pre><h5 id="2评论插件"><a class="markdownIt-Anchor" href="#2评论插件"></a> 2&gt;评论插件</h5><p>Fluid推荐的utteranc.es插件，经常会有加载比较慢的问题，乍一看以为不让评论…</p><p>这个配置过程也很简单。</p><p>首先在github创建一个公开的仓库，比如命名为’deepdeer_comments’。</p><p>点击<a href="https://github.com/apps/utterances" target="_blank" rel="noopener">这个链接</a>安装应用，选择“only select repositories”选项，找到刚刚建立好的仓库，点击install。</p><p>在配置中填写repo名，格式为“用户名/仓库名”，如“DeepDeer/deepdeer_comments”。Issue的命名方式建议选择第一个“Issue title contains page pathname”。</p><p>根据个人喜好选择主题之后，最后一栏会自动生成配置信息，复制这些信息。</p><p>在fluid主题的配置文件中，找到<code>comments</code>部分，将enable设置为true，并将type写成utterances。</p><p>在后面的comments具体配置部分，改成之前自动生成的配置。</p><div align="center">  <img src="/2020/06/01/build-blog/utter.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>这个博客目前用的是Gitalk插件，配置与Next主题中提到的大致相同。Fluid代码中该插件配置有问题，评论无法分页显示，感谢<a href="https://juejin.im/post/5ed177e36fb9a047923a39fe" target="_blank" rel="noopener">这篇文章</a>。即更改fluid主题下的<code>layout/_partial/comments/gitalk.ejs</code>文件内容中的’id’一栏部分</p><pre><code class="hljs python"><span class="hljs-comment">#原有的</span>id: <span class="hljs-string">'&lt;%- md5(theme.gitalk.id) %&gt;'</span>,<span class="hljs-comment">#改正后</span>id: &lt;%- theme.gitalk.id %&gt;,</code></pre><h3 id="三-自定义域名"><a class="markdownIt-Anchor" href="#三-自定义域名"></a> 三. 自定义域名</h3><p>本博客使用了阿里云上购买的域名。</p><p>在<a href="https://wanwang.aliyun.com/domain/searchresult/?keyword=skylasun&suffix=.cn#/?keyword=skylasun&suffix=cn" target="_blank" rel="noopener">这里</a>点击“控制台”，登录后，找到边栏中的“域名”。选择“域名注册”</p><div align="center">  <img src="/2020/06/01/build-blog/domain.jpg" srcset="/img/loading.gif" width="40%" height="40%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/reg.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><p>进入后，查询你喜欢的关键字相关的域名的注册情况，选择中意的域名就可以交钱了。最终付款之前还需要一些身份认证。</p><div align="center">  <img src="/2020/06/01/build-blog/buy.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>购买成功，认证通过后，在已有域名那里，点击“解析”，添加解析规则。这里添加的IP地址是之前deepdeer.github.io的解析情况，可以通过各类IP或域名查询网站找到，比如<a href="https://site.ip138.com/" target="_blank" rel="noopener">这里</a>。注意下面要加上一条CNMA规则。</p><div align="center">  <img src="/2020/06/01/build-blog/map.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><div align="center">  <img src="/2020/06/01/build-blog/ip.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>另外，在Github仓库的Settings里，需要加上“Custom domain”，保存配置后，会自动生成名为CNAME的文件，内容如下。但需要注意的是，每次我们重新部署时，使用deploy clean再generate后，会清除掉这个CNAME文件。为解决这个问题，可以把CNAME文件放到博客的“source”文件夹中。</p><div align="center">  <img src="/2020/06/01/build-blog/cname.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><h3 id="四-其它小经验"><a class="markdownIt-Anchor" href="#四-其它小经验"></a> 四. 其它小经验</h3><h4 id="1markdown编辑器推荐"><a class="markdownIt-Anchor" href="#1markdown编辑器推荐"></a> 1.Markdown编辑器推荐</h4><p>这些博客都是用<a href="https://www.typora.io/" target="_blank" rel="noopener">Typora</a>写的。该软件界面简洁，即时效果，很好用，推荐~</p><p>Typora除了支持公式块之外，还支持行内公式，在偏好设置中勾选”内联公式“即可。</p><div align="center">  <img src="/2020/06/01/build-blog/gongshi.jpg" srcset="/img/loading.gif" width="50%" height="50%" alt="oauth"></div><h4 id="2markdown中内容折叠"><a class="markdownIt-Anchor" href="#2markdown中内容折叠"></a> 2.Markdown中内容折叠</h4><p>有时文章内容过多不便于显示，可以使用如下语法进行折叠</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">details</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">summary</span>&gt;</span>可显示的标题<span class="hljs-tag">&lt;/<span class="hljs-name">summary</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">code</span>&gt;</span>   折叠内容  <span class="hljs-tag">&lt;/<span class="hljs-name">code</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">pre</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">details</span>&gt;</span></code></pre><p>比如</p><div align="center">  <img src="/2020/06/01/build-blog/zhedie.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>效果如下，点击后显示黑色部分</p><div align="center">  <img src="/2020/06/01/build-blog/xiaoguo.jpg" srcset="/img/loading.gif" width="100%" height="100%" alt="oauth"></div><p>如果有其它的坑，欢迎大家评论补充，谢谢！</p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工程技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>THU研究生国际会议出行准备流程</title>
    <link href="/2020/05/30/baoxiao/"/>
    <url>/2020/05/30/baoxiao/</url>
    
    <content type="html"><![CDATA[<p>下文仅限清华大学网络科学与网络空间研究院研究生同学使用，包含护照、签证、报销等</p><blockquote><p>提示：出国手续涉及部门较多，请尽早准备提前办理。如遇假期会有所调整，要关注邮件通知~</p></blockquote><h3 id="首先"><a class="markdownIt-Anchor" href="#首先"></a> 首先</h3><p>​你要有个<strong>护照</strong>，如果没有，办理的时候把发票留好，可以报销。</p><h3 id="前期准备"><a class="markdownIt-Anchor" href="#前期准备"></a> 前期准备</h3><p>注册会议并拿到<strong>邀请函</strong></p><p>预定<strong>机票和酒店</strong>（办签证使用）。目前携程等网站貌似不再支持付款前先打印行程单。</p><ol><li>机票时间，一般可定在会议安排往前往后各一天。有特殊情况，赶在自己文章汇报前到达即可。</li><li>酒店一般订会议推荐的，预定前注意一下学校给的当地住宿报销额度。有些会议官网会贴出提前订酒店有优惠的通知，发邮件过去即可。</li></ol><h3 id="学校审批"><a class="markdownIt-Anchor" href="#学校审批"></a> 学校审批</h3><h4 id="1申请出国批件"><a class="markdownIt-Anchor" href="#1申请出国批件"></a> 1&gt;申请出国批件</h4><ol><li>首先在info上进行申请，找“出国出境申报”——&gt;“因公出国（境）申报系统（新）</li></ol><div align="center">  <img src="/2020/05/30/baoxiao/apply.jpg" srcset="/img/loading.gif" width="70%" height="70%" alt="oauth"></div><ol start="2"><li>进入系统后，选择”新申请”，点击“我已阅读”，在因公出境申请表上填写信息，其中会<strong>比较犹豫的几个字段</strong>有：</li></ol><ul><li>出访基本信息：出入境时间大概在会议日程往前往后各一天，离境、入境时间是否需要过境等如实填写即可</li><li>出访类别：单位公派，会议</li><li>出访经费：费用来源一般选择“全部校内支付”，“纵向科研经费”，校内支付，人民币（大致写一个费用即可）</li><li>日程计划：简单填写就行，比如出发，抵达，开会，回程等等（可适当扩展）</li></ul><ol start="3"><li><p>提交之后会有一个预算表，大概可以看到给当地的住宿、日常消费额度等。这类信息也可以在边栏中“预算、外汇与报销”的“政策与标准”的表格中看到。</p><div align="center">  <img src="/2020/05/30/baoxiao/biaozhun.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div></li><li><p>“报批材料”里上传会议的邀请函和论文录用证明。</p></li><li><p>点击提交，打印申请表，这个表需要自己和导师签字。</p></li><li><p>提交完成后，返回主界面会显示出当前进度，完成后圆圈会变绿。大概两周左右“单位审核”会变绿。等到“学校审批”通过，显示批件下达之后，可以下载电子版。</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/jindu.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><h4 id="2批件领取"><a class="markdownIt-Anchor" href="#2批件领取"></a> 2&gt;批件领取</h4><p>去国际处，在李兆基4楼（可以进楼的门有点多，但失之毫厘谬以千里，所以可以问下保安…）</p><p>去之前先准备一份**“派出证明”<strong>。还是在刚刚的出入境申请系统的边栏里面。点击进去下载对应模板，注意老师们已经用最直接醒目的方法标示出的</strong>注意事项**。</p><div align="center">  <img src="/2020/05/30/baoxiao/chat.jpg" srcset="/img/loading.gif" alt="oauth" style="zoom:40%;"></div><div align="center">  <img src="/2020/05/30/baoxiao/chats.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/attention.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><p>在国际处主要有以下<strong>几个事情</strong>：</p><ul><li>拿批件</li><li>拿外汇预算单</li><li>派出证明需要盖章</li><li>如果办签证时需要单位法人证明一类的材料，需要<strong>主动</strong>和老师提及</li></ul><h3 id="签证办理"><a class="markdownIt-Anchor" href="#签证办理"></a> 签证办理</h3><p>这个就要看去哪个国家了，我以希腊为例，需要申根签，可以先在官网上填写表格申请，然后按照里面写的去依次准备材料。去使馆办事处。一定要注意时间，选最最最是工作时间的时段过去。我第一去的时候好像是下午3点左右到的，说是刚刚停止办理…</p><p>其它细节事项：</p><ol><li><p>保险可以直接在淘宝上买，搜“申根保险”就可以，看清楚额度是否符合要求；</p></li><li><p>户口页，如果户口在学校的话，直接在info上申请，”集体户口卡借阅”，里面包括“借阅预约”和“首页打印”。预约之后直接去地图里圈出的小房子（保卫处）里拿就好了；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/hukou.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="3"><li><p>银行对账单可以直接在C楼打印；</p></li><li><p>在读证明，在info上预约然后直接与三教打印（貌似改到了六教？，反正C楼应该都是万能的）；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/zaidu.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="5"><li>办理签证最后需要交纳现金。留好发票，这个在报销范围内。</li></ol><h3 id="出行及报销"><a class="markdownIt-Anchor" href="#出行及报销"></a> 出行及报销</h3><ol><li><p>行程中尽量保存好<strong>所有票据</strong>，回来整理<strong>报销</strong>。（我都是先垫付再报销，据说还可以先去学校<strong>借款</strong>）</p><p>各类车票，登机牌，行程单，机票购买记录及发票（让网站寄过来），酒店账单/发票，会议注册费发票等</p></li><li><p>去首都机场的话，清华科技园那里有大巴，车费是30块？这种貌似属于城建交通，也可以报销，不行的话，也有日常杂费可以cover掉。</p></li><li><p>回来后的报销主要是填写一个报销表格，还是在刚刚的出入境申报系统的边栏上的“表格下载”里，选择**”报销表格下载“**，表格如下图，里面也标明了一些报销流程和注意事项；</p></li></ol><div align="center">  <img src="/2020/05/30/baoxiao/baoxiaochat.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><div align="center">  <img src="/2020/05/30/baoxiao/items.jpg" srcset="/img/loading.gif" width="60%" height="60%" alt="oauth"></div><ol start="4"><li><p>其它细节事项</p><ul><li>大额机票需要发票验真，通过官网或发票自带的网站都可以，截图打印</li><li>打印护照的出入境记录页</li><li>提供交易记录截图（微信通知，短信账单，订单等均可）</li><li>在发票上签字需要用<strong>油笔</strong></li></ul><p>我们组的报销可以去对门实验室请教<strong>乔老师</strong>，老师会给予很多帮助，在此表示感谢~</p></li></ol><p>本文凭借对半年前的回忆整理，如有疏漏，欢迎大家评论指正！</p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>办公事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
